This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
src/
  pedalboard_pluginary/
    resources/
      default_ignores.json
    scanners/
      __init__.py
      au_scanner.py
      vst3_scanner.py
    __init__.py
    __main__.py
    async_scanner.py
    base_scanner.py
    constants.py
    core.py
    data.py
    exceptions.py
    models.py
    progress.py
    protocols.py
    retry.py
    scanner.py
    serialization.py
    timeout.py
    types.py
    utils.py
  pedalboard-stubs/
    __init__.pyi
tests/
  scanners/
    __init__.py
    test_au_scanner.py
    test_vst3_scanner.py
  test_cli.py
  test_data.py
  test_utils.py
.coveragerc
.gitignore
.isort.cfg
.pre-commit-config.yaml
AUTHORS.md
build.sh
CHANGELOG.md
LICENSE.txt
PLAN.md
pyproject.toml
README.md
TODO.md
tox.ini
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/pedalboard_pluginary/resources/default_ignores.json">
[
    "aufx/ANIMATE",
    "aufx/AudioDSP",
    "aufx/CoreAudio",
    "aufx/Dynamics",
    "aufx/iZNectar4Auto-LevelAUHook",
    "aufx/iZNeutron4AUHook",
    "aufx/iZNeutron4CompressorAUHook",
    "aufx/iZNeutron4EqualizerAUHook",
    "aufx/iZNeutron4ExciterAUHook",
    "aufx/iZNeutron4GateAUHook",
    "aufx/iZNeutron4SculptorAUHook",
    "aufx/iZNeutron4TransientShaperAUHook",
    "aufx/iZNeutron4UnmaskAUHook",
    "aufx/iZRX10ConnectAUHook",
    "aufx/iZRelayAUHook",
    "aufx/smartEQ3",
    "aufx/smartcomp2",
    "aufx/smartgate",
    "aufx/unknown URL",
    "vst3/RX 10 Connect",
    "vst3/RX 10 Repair Assistant",
    "vst3/smartEQ3"
]
</file>

<file path="src/pedalboard_pluginary/scanners/__init__.py">
# pedalboard_pluginary/scanners/__init__.py

# This file makes Python treat the `scanners` directory as a package.

# Optionally, you can import specific classes or functions here to make them
# available at the package level, e.g.:
# from .au_scanner import AUScanner
# from .vst3_scanner import VST3Scanner

# For now, it will be kept empty.
</file>

<file path="src/pedalboard_pluginary/async_scanner.py">
"""
Async scanner implementation for concurrent plugin scanning.
"""

import asyncio
import logging
from pathlib import Path
from typing import AsyncIterator, List, Optional, TYPE_CHECKING

from .constants import PLUGIN_LOAD_TIMEOUT
from .models import PluginInfo
from .protocols import ProgressReporter
from .timeout import TimeoutError

if TYPE_CHECKING:
    from .protocols import PluginScanner

logger = logging.getLogger(__name__)


class AsyncScannerMixin:
    """Mixin to add async capabilities to scanners.
    
    This mixin expects to be mixed with a class that implements the PluginScanner protocol.
    """
    
    async def scan_plugin_async(self, path: Path) -> Optional[PluginInfo]:
        """Async wrapper for plugin scanning with timeout.
        
        Args:
            path: Path to the plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        try:
            # Use asyncio.to_thread for CPU-bound plugin loading
            loop = asyncio.get_event_loop()
            return await asyncio.wait_for(
                loop.run_in_executor(None, self.scan_plugin, path),  # type: ignore[attr-defined]
                timeout=PLUGIN_LOAD_TIMEOUT
            )
        except asyncio.TimeoutError:
            logger.warning(f"Plugin {path} timed out during async scan")
            return None
        except Exception as e:
            logger.error(f"Error in async plugin scan for {path}: {e}")
            return None
    
    async def scan_plugins_batch(
        self, 
        paths: List[Path], 
        max_concurrent: int = 10,
        progress_reporter: Optional[ProgressReporter] = None
    ) -> AsyncIterator[PluginInfo]:
        """Scan multiple plugins concurrently with backpressure control.
        
        Args:
            paths: List of plugin paths to scan.
            max_concurrent: Maximum number of concurrent scans.
            progress_reporter: Optional progress reporter.
            
        Yields:
            PluginInfo objects for successfully scanned plugins.
        """
        if not paths:
            return
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def scan_with_semaphore(path: Path) -> Optional[PluginInfo]:
            async with semaphore:
                return await self.scan_plugin_async(path)
        
        # Create tasks for all paths
        tasks = [scan_with_semaphore(path) for path in paths]
        
        # Start progress tracking
        if progress_reporter:
            progress_reporter.start(len(tasks), f"Scanning {len(tasks)} plugins")
        
        completed = 0
        successful = 0
        
        # Process tasks as they complete
        for coro in asyncio.as_completed(tasks):
            result = await coro
            completed += 1
            
            if result:
                successful += 1
                yield result
            
            # Update progress
            if progress_reporter:
                message = f"Completed {completed}/{len(tasks)} ({successful} successful)"
                progress_reporter.update(1, message)
        
        # Finish progress tracking
        if progress_reporter:
            progress_reporter.finish(f"Scan completed: {successful}/{len(tasks)} plugins")
    
    async def scan_directory_async(
        self, 
        directory: Path,
        max_concurrent: int = 10,
        progress_reporter: Optional[ProgressReporter] = None
    ) -> List[PluginInfo]:
        """Async scan of an entire directory.
        
        Args:
            directory: Directory to scan for plugins.
            max_concurrent: Maximum number of concurrent scans.
            progress_reporter: Optional progress reporter.
            
        Returns:
            List of successfully scanned plugins.
        """
        # Find plugin files
        plugin_files = self.find_plugin_files([directory])  # type: ignore[attr-defined]
        
        # Scan plugins concurrently
        plugins = []
        async for plugin in self.scan_plugins_batch(
            plugin_files, 
            max_concurrent=max_concurrent,
            progress_reporter=progress_reporter
        ):
            plugins.append(plugin)
        
        return plugins


class AsyncVST3Scanner(AsyncScannerMixin):
    """VST3 scanner with async capabilities.
    
    This is a placeholder for the async VST3 scanner that will inherit
    from both VST3Scanner and AsyncScannerMixin.
    """
    pass


class AsyncAUScanner(AsyncScannerMixin):
    """AU scanner with async capabilities.
    
    This is a placeholder for the async AU scanner that will inherit
    from both AUScanner and AsyncScannerMixin.
    """
    pass
</file>

<file path="src/pedalboard_pluginary/exceptions.py">
"""
Custom exception hierarchy for pedalboard_pluginary.
"""

from typing import Optional


class PluginaryError(Exception):
    """Base exception for all Pluginary errors."""
    
    def __init__(self, message: str, details: Optional[str] = None):
        """Initialize the exception with a message and optional details.
        
        Args:
            message: Main error message.
            details: Optional additional details about the error.
        """
        super().__init__(message)
        self.message = message
        self.details = details
    
    def __str__(self) -> str:
        """Return string representation of the error."""
        if self.details:
            return f"{self.message} - {self.details}"
        return self.message


class ScannerError(PluginaryError):
    """Base exception for scanner-related errors."""
    pass


class PluginLoadError(ScannerError):
    """Raised when a plugin fails to load."""
    
    def __init__(self, plugin_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            plugin_path: Path to the plugin that failed to load.
            reason: Optional reason for the failure.
        """
        message = f"Failed to load plugin: {plugin_path}"
        super().__init__(message, reason)
        self.plugin_path = plugin_path


class PluginScanError(ScannerError):
    """Raised when scanning a plugin fails."""
    
    def __init__(self, plugin_path: str, scanner_type: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            plugin_path: Path to the plugin that failed to scan.
            scanner_type: Type of scanner that failed (e.g., 'vst3', 'aufx').
            reason: Optional reason for the failure.
        """
        message = f"Failed to scan {scanner_type} plugin: {plugin_path}"
        super().__init__(message, reason)
        self.plugin_path = plugin_path
        self.scanner_type = scanner_type


class CacheError(PluginaryError):
    """Base exception for cache-related errors."""
    pass


class CacheCorruptedError(CacheError):
    """Raised when cache file is corrupted."""
    
    def __init__(self, cache_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            cache_path: Path to the corrupted cache file.
            reason: Optional reason or details about the corruption.
        """
        message = f"Cache file is corrupted: {cache_path}"
        super().__init__(message, reason)
        self.cache_path = cache_path


class CacheVersionError(CacheError):
    """Raised when cache version is incompatible."""
    
    def __init__(self, expected: str, actual: str, cache_path: str):
        """Initialize the exception.
        
        Args:
            expected: Expected cache version.
            actual: Actual cache version found.
            cache_path: Path to the cache file.
        """
        message = f"Cache version mismatch: expected {expected}, got {actual}"
        details = f"Cache file: {cache_path}"
        super().__init__(message, details)
        self.expected_version = expected
        self.actual_version = actual
        self.cache_path = cache_path


class CacheWriteError(CacheError):
    """Raised when writing to cache fails."""
    
    def __init__(self, cache_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            cache_path: Path to the cache file.
            reason: Optional reason for the write failure.
        """
        message = f"Failed to write cache: {cache_path}"
        super().__init__(message, reason)
        self.cache_path = cache_path


class ConfigError(PluginaryError):
    """Base exception for configuration-related errors."""
    pass


class InvalidConfigError(ConfigError):
    """Raised when configuration is invalid."""
    
    def __init__(self, config_key: str, invalid_value: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            config_key: Configuration key that has invalid value.
            invalid_value: The invalid value.
            reason: Optional reason why the value is invalid.
        """
        message = f"Invalid configuration value for '{config_key}': {invalid_value}"
        super().__init__(message, reason)
        self.config_key = config_key
        self.invalid_value = invalid_value


class PlatformError(PluginaryError):
    """Raised when an operation is not supported on the current platform."""
    
    def __init__(self, operation: str, platform: str, supported_platforms: Optional[list[str]] = None):
        """Initialize the exception.
        
        Args:
            operation: Operation that is not supported.
            platform: Current platform.
            supported_platforms: Optional list of supported platforms.
        """
        message = f"Operation '{operation}' is not supported on {platform}"
        if supported_platforms:
            details = f"Supported platforms: {', '.join(supported_platforms)}"
        else:
            details = None
        super().__init__(message, details)
        self.operation = operation
        self.platform = platform
        self.supported_platforms = supported_platforms or []
</file>

<file path="src/pedalboard_pluginary/protocols.py">
"""
Protocol definitions for plugin scanner implementations.
"""

from typing import Protocol, List, Optional, Dict, runtime_checkable
from pathlib import Path

from .models import PluginInfo


@runtime_checkable
class PluginScanner(Protocol):
    """Protocol defining the interface for plugin scanners."""
    
    plugin_type: str
    supported_extensions: List[str]
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all plugin files of this scanner's type.
        
        Args:
            paths: Optional list of specific paths to check. If None, searches default locations.
            
        Returns:
            List of paths to plugin files found.
        """
        ...
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a single plugin file and return its information.
        
        Args:
            path: Path to the plugin file to scan.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        ...
    
    def validate_plugin_path(self, path: Path) -> bool:
        """Validate if a path is a valid plugin for this scanner.
        
        Args:
            path: Path to validate.
            
        Returns:
            True if the path is a valid plugin file, False otherwise.
        """
        ...


@runtime_checkable
class ProgressReporter(Protocol):
    """Protocol for progress reporting implementations."""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        ...
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        ...
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        ...


@runtime_checkable
class CacheBackend(Protocol):
    """Protocol for cache backend implementations."""
    
    def load(self) -> Dict[str, PluginInfo]:
        """Load all cached plugins.
        
        Returns:
            Dictionary mapping plugin IDs to PluginInfo objects.
        """
        ...
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Save plugins to cache.
        
        Args:
            plugins: Dictionary mapping plugin IDs to PluginInfo objects.
        """
        ...
    
    def update(self, plugin_id: str, plugin: PluginInfo) -> None:
        """Update a single plugin in cache.
        
        Args:
            plugin_id: ID of the plugin to update.
            plugin: Updated PluginInfo object.
        """
        ...
    
    def delete(self, plugin_id: str) -> None:
        """Remove a plugin from cache.
        
        Args:
            plugin_id: ID of the plugin to remove.
        """
        ...
    
    def clear(self) -> None:
        """Clear entire cache."""
        ...
    
    def exists(self) -> bool:
        """Check if cache exists.
        
        Returns:
            True if cache exists, False otherwise.
        """
        ...
</file>

<file path="src/pedalboard_pluginary/retry.py">
"""
Retry logic for handling transient failures.
"""

import functools
import logging
import time
from typing import Any, Callable, Optional, Tuple, Type, TypeVar, Union

from .constants import MAX_SCAN_RETRIES, SCAN_RETRY_DELAY

logger = logging.getLogger(__name__)

F = TypeVar('F', bound=Callable[..., Any])


def with_retry(
    exceptions: Union[Type[Exception], Tuple[Type[Exception], ...]],
    max_attempts: int = MAX_SCAN_RETRIES,
    delay: float = SCAN_RETRY_DELAY,
    backoff_factor: float = 2.0,
    max_delay: float = 60.0,
) -> Callable[[F], F]:
    """Decorator that retries a function on specified exceptions.
    
    Args:
        exceptions: Exception or tuple of exceptions to catch and retry on.
        max_attempts: Maximum number of attempts (including the first).
        delay: Initial delay between retries in seconds.
        backoff_factor: Factor to multiply delay by after each failure.
        max_delay: Maximum delay between retries in seconds.
        
    Returns:
        Decorated function that will retry on failure.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            current_delay = delay
            last_exception: Optional[Exception] = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_attempts - 1:
                        # Last attempt, re-raise
                        logger.warning(
                            f"{func.__name__} failed after {max_attempts} attempts: {e}"
                        )
                        raise
                    
                    logger.info(
                        f"{func.__name__} failed (attempt {attempt + 1}/{max_attempts}): {e}. "
                        f"Retrying in {current_delay:.1f}s..."
                    )
                    time.sleep(current_delay)
                    
                    # Exponential backoff with max delay
                    current_delay = min(current_delay * backoff_factor, max_delay)
            
            # This should never be reached, but just in case
            if last_exception:
                raise last_exception
            else:
                raise RuntimeError(f"{func.__name__} failed with unknown error")
        
        return wrapper  # type: ignore[return-value]
    
    return decorator


def with_timeout(timeout: float) -> Callable[[F], F]:
    """Decorator that adds a timeout to a function.
    
    Note: This is a placeholder for future implementation.
    Proper timeout handling requires different approaches for
    synchronous vs asynchronous functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            # TODO: Implement proper timeout handling
            # For now, just pass through
            return func(*args, **kwargs)
        
        return wrapper  # type: ignore[return-value]
    
    return decorator
</file>

<file path="src/pedalboard_pluginary/timeout.py">
"""
Timeout handling for plugin operations.
"""

import asyncio
import concurrent.futures
import functools
import logging
from typing import Any, Awaitable, Callable, TypeVar, Union

from .constants import PLUGIN_LOAD_TIMEOUT

logger = logging.getLogger(__name__)

T = TypeVar('T')
F = TypeVar('F', bound=Callable[..., Any])


class TimeoutError(Exception):
    """Raised when an operation times out."""
    
    def __init__(self, message: str, timeout: float):
        super().__init__(message)
        self.timeout = timeout


def sync_timeout(func: Callable[..., T], timeout: float, *args: Any, **kwargs: Any) -> T:
    """Execute synchronous function with timeout using ThreadPoolExecutor.
    
    Args:
        func: Function to execute.
        timeout: Timeout in seconds.
        *args: Positional arguments for the function.
        **kwargs: Keyword arguments for the function.
        
    Returns:
        Function result.
        
    Raises:
        TimeoutError: If function execution exceeds timeout.
        Exception: Any exception raised by the function.
    """
    logger.debug(f"Executing {func.__name__} with {timeout}s timeout")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(func, *args, **kwargs)
        try:
            result = future.result(timeout=timeout)
            logger.debug(f"{func.__name__} completed successfully")
            return result
        except concurrent.futures.TimeoutError:
            logger.warning(f"{func.__name__} timed out after {timeout}s")
            # Cancel the future to prevent resource leaks
            future.cancel()
            raise TimeoutError(f"{func.__name__} timed out after {timeout}s", timeout)
        except Exception as e:
            logger.error(f"{func.__name__} failed: {e}")
            raise


async def async_timeout(coro_func: Callable[..., Awaitable[T]], timeout: float, *args: Any, **kwargs: Any) -> T:
    """Execute coroutine function with timeout.
    
    Args:
        coro_func: Coroutine function to execute.
        timeout: Timeout in seconds.
        *args: Positional arguments for the function.
        **kwargs: Keyword arguments for the function.
        
    Returns:
        Function result.
        
    Raises:
        TimeoutError: If function execution exceeds timeout.
        Exception: Any exception raised by the function.
    """
    logger.debug(f"Executing {coro_func.__name__} with {timeout}s timeout")
    
    try:
        result: T = await asyncio.wait_for(coro_func(*args, **kwargs), timeout=timeout)
        logger.debug(f"{coro_func.__name__} completed successfully")
        return result
    except asyncio.TimeoutError:
        logger.warning(f"{coro_func.__name__} timed out after {timeout}s")
        raise TimeoutError(f"{coro_func.__name__} timed out after {timeout}s", timeout)
    except Exception as e:
        logger.error(f"{coro_func.__name__} failed: {e}")
        raise


def with_sync_timeout(timeout: float = PLUGIN_LOAD_TIMEOUT) -> Callable[[F], F]:
    """Decorator that adds timeout to synchronous functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            return sync_timeout(func, timeout, *args, **kwargs)
        return wrapper  # type: ignore[return-value]
    return decorator


def with_async_timeout(timeout: float = PLUGIN_LOAD_TIMEOUT) -> Callable[[F], F]:
    """Decorator that adds timeout to async functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        async def async_wrapper(*args: Any, **kwargs: Any) -> Any:
            return await async_timeout(func, timeout, *args, **kwargs)
        return async_wrapper  # type: ignore[return-value]
    return decorator
</file>

<file path="src/pedalboard-stubs/__init__.pyi">
"""Type stubs for pedalboard library."""

from typing import Dict, Union, Any, Optional, TypeVar
from pathlib import Path

# Parameter value types that pedalboard can return
ParameterValue = Union[float, int, bool, str]

class Plugin:
    """Base plugin class."""
    
    # Core attributes that all plugins have
    parameters: Dict[str, ParameterValue]
    name: str
    manufacturer: Optional[str]
    
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...

class AudioUnitPlugin(Plugin):
    """Audio Unit plugin class."""
    pass

class VST3Plugin(Plugin):
    """VST3 plugin class."""
    pass

# Plugin loading function
def load_plugin(
    path_or_name: Union[str, Path], 
    plugin_name: Optional[str] = None,
    disable_caching: bool = False,
    **kwargs: Any
) -> Plugin: ...

# Re-export common types
__all__ = [
    "Plugin",
    "AudioUnitPlugin", 
    "VST3Plugin",
    "load_plugin",
    "ParameterValue",
]
</file>

<file path="tests/scanners/__init__.py">
# tests/scanners/__init__.py
# This file makes Python treat the `tests/scanners` directory as a package.
</file>

<file path="tests/scanners/test_au_scanner.py">
# tests/scanners/test_au_scanner.py
import os
import platform
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock

from pedalboard_pluginary.scanners.au_scanner import AUScanner

# Sample auval output
AUVAL_OUTPUT_VALID = """
 Westwood AU Test
--------------------------------------------------
AUVALTOOL Discount AU
--------------------------------------------------
PLAYER version 2.0.13 (build 17)
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'dely' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: AUDelay
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/AUDelay.component
Component AU Path: /Library/Audio/Plug-Ins/Components/AUDelay.component/Contents/MacOS/AUDelay

* * PASS
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'mcmp' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: AUMultibandCompressor
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component
Component AU Path: /Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component/Contents/MacOS/AUMultibandCompressor

* * PASS
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aumf' - 'dls ' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: DLSMusicDevice
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/DLSMusicDevice.component
Component AU Path: /Library/Audio/Plug-Ins/Components/DLSMusicDevice.component/Contents/MacOS/DLSMusicDevice

* * PASS
--------------------------------------------------
TESTING OPEN TIMES:
COLD:
Time to open AudioUnit:      21.112 ms
WARM:
Time to open AudioUnit:      0.042  ms
This AudioUnit is a version 3 implementation.
FIRST TIME:
FATAL ERROR: Initialize: result: -50


--------------------------------------------------
AU VALIDATION SUCCEEDED.
--------------------------------------------------
"""

AUVAL_OUTPUT_GARBAGE_URL = """
 Westwood AU Test
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'xxxx' - 'test'
--------------------------------------------------
Manufacturer String: Test Inc
AudioUnit Name: BadURLPlugin
Component Version: 1.0.0
Component Bundle Path: /path/to/plugin with spaces.component
Component AU Path: (null)

* * PASS
--------------------------------------------------
AU VALIDATION SUCCEEDED.
--------------------------------------------------
"""


@pytest.fixture
def au_scanner_instance():
    return AUScanner(ignores=set())

@pytest.fixture
def au_scanner_with_ignores_instance():
    return AUScanner(ignores={"aufx/DLSMusicDevice"}) # Key is type/stem

@patch('platform.system', return_value='Darwin') # Assume macOS for these tests
class TestAUScanner:

    @patch('subprocess.run')
    def test_list_aufx_plugins_raw_success(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        lines = au_scanner_instance._list_aufx_plugins_raw()
        assert len(lines) > 0
        assert "AUDelay" in AUVAL_OUTPUT_VALID
        mock_subprocess_run.assert_called_once_with(
            ["auval", "-l"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )

    @patch('subprocess.run', side_effect=FileNotFoundError("auval not found"))
    def test_list_aufx_plugins_raw_auval_not_found(self, mock_subprocess_run, au_scanner_instance):
        lines = au_scanner_instance._list_aufx_plugins_raw()
        assert lines == []

    @patch('subprocess.run')
    def test_find_plugin_files_valid_output(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        # Mock Path.resolve() and Path.exists() for paths found by auval
        with patch.object(Path, 'resolve') as mock_resolve, \
             patch.object(Path, 'exists', return_value=True) as mock_exists:

            # Make resolve return a Path object that can be further manipulated if needed
            # and also has an 'exists' method.
            def side_effect_resolve(*args, **kwargs):
                # The input to resolve is the path string from auval output
                # e.g., Path("/Library/Audio/Plug-Ins/Components/AUDelay.component")
                # We want it to return itself basically, but as a mock if needed for exists()
                p = Path(*args) # Reconstruct the original path
                # Mock the exists for this specific path if needed, though global mock_exists might cover it
                # For bundle path logic, ensure suffix is checked on original path.
                return p
            mock_resolve.side_effect = side_effect_resolve

            plugin_files = au_scanner_instance.find_plugin_files()

            assert len(plugin_files) == 3 # AUDelay, AUMultibandCompressor, DLSMusicDevice
            expected_paths = [
                Path("/Library/Audio/Plug-Ins/Components/AUDelay.component"),
                Path("/Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component"),
                Path("/Library/Audio/Plug-Ins/Components/DLSMusicDevice.component"),
            ]
            for p in expected_paths:
                assert p.resolve() in plugin_files # Comparing resolved paths

    @patch('subprocess.run')
    def test_find_plugin_files_with_ignores(self, mock_subprocess_run, au_scanner_with_ignores_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_with_ignores_instance.find_plugin_files()

            # DLSMusicDevice should be ignored
            assert len(plugin_files) == 2
            ignored_path = Path("/Library/Audio/Plug-Ins/Components/DLSMusicDevice.component").resolve()
            assert ignored_path not in plugin_files
            delay_path = Path("/Library/Audio/Plug-Ins/Components/AUDelay.component").resolve()
            assert delay_path in plugin_files


    @patch('subprocess.run')
    def test_find_plugin_files_garbage_url(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_GARBAGE_URL # Contains (null) URL
        mock_subprocess_run.return_value = mock_process

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p) if p else None), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_instance.find_plugin_files()
            assert len(plugin_files) == 0 # Should skip the one with (null) URL

    @patch('platform.system', return_value='Linux') # Test non-Darwin platform
    def test_scanner_on_non_macos(self, mock_platform_system_linux, au_scanner_instance):
        assert au_scanner_instance._list_aufx_plugins_raw() == []
        assert au_scanner_instance.find_plugin_files() == []

    @patch('subprocess.run')
    def test_find_plugin_files_with_specific_paths_filter(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        # User wants to check only AUDelay
        specific_paths_to_check = [Path("/Library/Audio/Plug-Ins/Components/AUDelay.component")]

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_instance.find_plugin_files(plugin_paths=specific_paths_to_check)

            assert len(plugin_files) == 1
            assert Path("/Library/Audio/Plug-Ins/Components/AUDelay.component").resolve() in plugin_files

    # Test for bundle path resolution logic
    # This requires more intricate mocking of Path objects if we don't want to rely on filesystem
    @patch('subprocess.run')
    def test_bundle_path_resolution(self, mock_subprocess_run, au_scanner_instance):
        # Simulate auval output where path is deep inside the bundle
        deep_path_auval_output = """
        VALIDATING AUDIO UNIT: 'aufx' - 'test' - 'tstc'
        --------------------------------------------------
        Manufacturer String: TestCompany
        AudioUnit Name: DeepTestPlugin
        Component Version: 1.0.0
        Component Bundle Path: /Some/Path/DeepTestPlugin.component/Contents/MacOS/DeepTestPlugin
        Component AU Path: /Some/Path/DeepTestPlugin.component/Contents/MacOS/DeepTestPlugin
        * * PASS
        --------------------------------------------------
        AU VALIDATION SUCCEEDED.
        --------------------------------------------------
        """
        mock_process = MagicMock()
        mock_process.stdout = deep_path_auval_output
        mock_subprocess_run.return_value = mock_process

        # We need to mock Path behavior for suffix and parent
        # The Path object created from the string should behave as expected.
        # No complex mocking needed if Path objects work as standard for these attributes.
        # We only need to ensure Path.resolve and Path.exists are controlled.

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):

            plugin_files = au_scanner_instance.find_plugin_files()

            assert len(plugin_files) == 1
            # The scanner should correctly identify the .component bundle path
            expected_bundle_path = Path("/Some/Path/DeepTestPlugin.component").resolve()
            assert expected_bundle_path in plugin_files

# TODO: Add tests for error conditions in auval (e.g., CalledProcessError)
# TODO: Add tests for when Path.resolve() or other Path operations raise exceptions
# (though these are less likely for valid path strings)
</file>

<file path="tests/test_cli.py">
# tests/test_cli.py
import os
import subprocess
import json
import yaml
from pathlib import Path
from unittest.mock import patch, MagicMock
import pytest

from pedalboard_pluginary.data import APP_NAME, PLUGINS_CACHE_FILENAME_BASE, get_cache_path

# Helper to get the cache file path for plugins
def get_plugins_cache_file():
    return get_cache_path(PLUGINS_CACHE_FILENAME_BASE)

@pytest.fixture(autouse=True)
def manage_plugin_cache():
    """Fixture to ensure plugin cache is handled before and after tests."""
    cache_file = get_plugins_cache_file()
    original_content = None

    if cache_file.exists():
        original_content = cache_file.read_text()
        cache_file.unlink() # Remove before test

    yield # Test runs here

    # Cleanup: remove cache file created by test, or restore original
    if cache_file.exists():
        cache_file.unlink()
    if original_content:
        # Ensure parent directory exists before writing back
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        cache_file.write_text(original_content)


# Mocked data for PedalboardScanner.scan_all_plugins and load_json_file
# This data will be "written" by the mocked scan and "read" by list/json/yaml
MOCK_PLUGIN_DATA = {
    "vst3/MockSynth": {
        "id": "vst3/MockSynth",
        "name": "MockSynth",
        "path": "/fake/path/to/MockSynth.vst3",
        "filename": "MockSynth.vst3",
        "plugin_type": "vst3",
        "parameters": {
            "Volume": {"name": "Volume", "value": 0.75},
            "Pan": {"name": "Pan", "value": 0.0}
        },
        "manufacturer": "FakePlugins",
        "name_in_file": "MockSynth"
    },
    "aufx/MockEffect": {
        "id": "aufx/MockEffect",
        "name": "MockEffect",
        "path": "/fake/path/to/MockEffect.component",
        "filename": "MockEffect.component",
        "plugin_type": "aufx",
        "parameters": {
            "Wet/Dry": {"name": "Wet/Dry", "value": 0.5}
        },
        "manufacturer": "FakeAudio",
        "name_in_file": "MockEffect"
    }
}

# This mock will replace the actual PedalboardScanner instance or its methods
@patch('pedalboard_pluginary.scanner.PedalboardScanner.scan_all_plugins')
@patch('pedalboard_pluginary.scanner.PedalboardScanner.update_scan') # Also mock update_scan
@patch('pedalboard_pluginary.scanner.PedalboardScanner.save_plugins') # Mock save_plugins
@patch('pedalboard_pluginary.core.PedalboardPluginary.load_data') # Mock load_data in core
def run_cli_command(
    cli_args_list,
    mock_core_load_data,
    mock_scanner_save_plugins,
    mock_scanner_update_scan,
    mock_scanner_scan_all,
    expected_exit_code=0
):
    """Helper to run CLI commands and capture output."""

    # If scan or update is called, make them "create" the mock cache file
    def side_effect_scan_or_update(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4)
        # The actual scan methods in PedalboardScanner don't return anything.
        # They modify self.plugins and then call self.save_plugins.
        # We've mocked save_plugins separately.

    mock_scanner_scan_all.side_effect = side_effect_scan_or_update
    mock_scanner_update_scan.side_effect = side_effect_scan_or_update

    # Mock load_data to set the plugins attribute on an instance if needed,
    # or simply prevent it from trying to load a real file during list commands
    # if scan hasn't run.
    # For 'list', 'json', 'yaml', the PedalboardPluginary instance will try to load.
    # We can patch load_json_file used by PedalboardPluginary.load_data

    base_command = ["pbpluginary"]
    full_command = base_command + cli_args_list

    try:
        result = subprocess.run(full_command, capture_output=True, text=True, check=False)
        if result.returncode != expected_exit_code:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        assert result.returncode == expected_exit_code
        return result
    except FileNotFoundError:
        pytest.fail("pbpluginary command not found. Ensure it's installed and in PATH for testing.")


# Test 'scan' command
# Patching at the source of where PedalboardScanner is instantiated or used by CLI
@patch('pedalboard_pluginary.__main__.PedalboardScanner')
def test_cli_scan(MockScannerConstructor):
    # Mock the instance methods that would be called
    mock_scanner_instance = MockScannerConstructor.return_value
    mock_scanner_instance.rescan.return_value = None # rescan calls full_scan which calls scan_all_plugins

    # We need rescan (which is an alias for full_scan) to effectively create the cache
    # by having its underlying scan_all_plugins call write the MOCK_PLUGIN_DATA
    def mock_rescan_writes_cache(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4)
    mock_scanner_instance.rescan.side_effect = mock_rescan_writes_cache

    result = run_cli_command(["scan"]) # Uses the patches from run_cli_command's decorators

    # Check that the cache file was created with mock data
    cache_file = get_plugins_cache_file()
    assert cache_file.exists()
    with open(cache_file, 'r') as f:
        data_from_cache = json.load(f)
    assert data_from_cache == MOCK_PLUGIN_DATA

    # Check if scan method on the instance was called
    mock_scanner_instance.rescan.assert_called_once()


# Test 'list' command (implicitly tests JSON output)
# For list, we need to ensure that the cache exists or that PedalboardPluginary can load it.
# The manage_plugin_cache fixture helps here.
# We also need to control what PedalboardPluginary.load_data does.
@patch('pedalboard_pluginary.data.load_json_file') # Patch where load_json_file is defined
def test_cli_list(mock_load_json, capsys):
    # Setup: Ensure a cache file with MOCK_PLUGIN_DATA exists for 'list' to read
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    # Configure the mock for load_json_file used by PedalboardPluginary
    # It should return the MOCK_PLUGIN_DATA when the specific plugins cache path is requested
    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            # Return raw dict, PedalboardPluginary.load_data will handle reconstruction
            return MOCK_PLUGIN_DATA
        return {} # Default for other calls
    mock_load_json.side_effect = side_effect_load_json

    # Run the 'list' command
    # Using direct function call to avoid subprocess complexity with stdout/stderr and fire's display hook
    from pedalboard_pluginary.__main__ import list_json_cli

    # Fire's Display hook is tricky to test with subprocess.run, so call directly.
    # list_json_cli returns a string.
    # We need to ensure that when `pbpluginary list` is run, this function is called
    # and its output (which is JSON string) is printed.
    # For simplicity here, just test the function that `fire` would call.

    # To test the actual CLI output, we need to let pbpluginary run as subprocess
    # and capture stdout. This means not mocking PedalboardPluginary or its load_data directly here
    # but ensuring the underlying data.load_json_file behaves as expected due to the patch.

    result = subprocess.run(["pbpluginary", "list"], capture_output=True, text=True, check=True)

    # The output should be the MOCK_PLUGIN_DATA formatted as JSON
    # Fire wraps output, so it might not be exact JSON string if printed line-by-line.
    # The default 'list' command in __main__.py calls bdict().to_json() and fire prints it.
    # Let's parse the stdout.
    try:
        output_data = json.loads(result.stdout)
        assert output_data == MOCK_PLUGIN_DATA
    except json.JSONDecodeError:
        pytest.fail(f"CLI output was not valid JSON. Output:\n{result.stdout}")


# Test 'json' command (should be identical to 'list')
@patch('pedalboard_pluginary.data.load_json_file')
def test_cli_json_output(mock_load_json_for_json_cmd, capsys):
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            return MOCK_PLUGIN_DATA
        return {}
    mock_load_json_for_json_cmd.side_effect = side_effect_load_json

    result = subprocess.run(["pbpluginary", "json"], capture_output=True, text=True, check=True)
    try:
        output_data = json.loads(result.stdout)
        assert output_data == MOCK_PLUGIN_DATA
    except json.JSONDecodeError:
        pytest.fail(f"CLI output for 'json' was not valid JSON. Output:\n{result.stdout}")


# Test 'yaml' command
@patch('pedalboard_pluginary.data.load_json_file')
def test_cli_yaml_output(mock_load_json_for_yaml_cmd, capsys):
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            return MOCK_PLUGIN_DATA
        return {}
    mock_load_json_for_yaml_cmd.side_effect = side_effect_load_json

    result = subprocess.run(["pbpluginary", "yaml"], capture_output=True, text=True, check=True)
    try:
        # python-benedict's to_yaml output might have specific formatting.
        # For robustness, parse it back and compare with original data.
        output_data = yaml.safe_load(result.stdout)
        # YAML load might produce slightly different types (e.g. list for dict items sometimes)
        # A direct comparison MOCK_PLUGIN_DATA might be tricky if numbers are float vs int.
        # For now, let's assume benedict produces standard YAML that converts back cleanly.
        assert json.dumps(output_data, sort_keys=True) == json.dumps(MOCK_PLUGIN_DATA, sort_keys=True)
    except yaml.YAMLError:
        pytest.fail(f"CLI output for 'yaml' was not valid YAML. Output:\n{result.stdout}")
    except Exception as e:
        pytest.fail(f"Error comparing YAML output: {e}. Output:\n{result.stdout}")


# Test 'update' command
@patch('pedalboard_pluginary.__main__.PedalboardScanner')
def test_cli_update(MockScannerConstructorForUpdate):
    mock_scanner_instance = MockScannerConstructorForUpdate.return_value

    # Simulate that update_scan effectively writes the cache
    def mock_update_scan_writes_cache(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        # Update might add to existing data or rescan if no cache.
        # For this test, assume it behaves like scan if no cache.
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4) # For simplicity, same as scan
    mock_scanner_instance.update_scan.side_effect = mock_update_scan_writes_cache

    result = run_cli_command(["update"]) # Uses patches from run_cli_command

    cache_file = get_plugins_cache_file()
    assert cache_file.exists()
    with open(cache_file, 'r') as f:
        data_from_cache = json.load(f)
    assert data_from_cache == MOCK_PLUGIN_DATA # Assuming update wrote this

    mock_scanner_instance.update_scan.assert_called_once()


# TODO: Test for verbose logging options (--verbose=1, --verbose=2)
# TODO: Test for --extra-folders option with scan and update
# TODO: Test scan/update when cache already exists (for update's diff logic, though that's scanner internal)
# TODO: Test error conditions (e.g., unparseable cache, permissions issues - harder to mock)

# Note: The run_cli_command helper and its patches are quite broad.
# For more targeted tests, especially for 'list', 'json', 'yaml',
# it might be better to directly call the CLI functions from __main__.py
# and mock their dependencies (like PedalboardPluginary instance) instead of using subprocess.
# However, subprocess tests the actual command-line invocation.
# The current `test_cli_list` and `test_cli_json_output`, `test_cli_yaml_output`
# have been changed to use subprocess.run directly.
</file>

<file path="tests/test_utils.py">
import pytest
from pedalboard_pluginary.utils import ensure_folder
from pathlib import Path

def test_ensure_folder(tmp_path):
    test_folder = tmp_path / "test_folder"
    ensure_folder(test_folder)
    assert test_folder.exists()
</file>

<file path=".coveragerc">
# .coveragerc to control coverage.py
[run]
branch = True
source = pedalboard_pluginary
# omit = bad_file.py

[paths]
source =
    src/
    */site-packages/

[report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
</file>

<file path=".isort.cfg">
[settings]
profile = black
known_first_party = pedalboard_pluginary
</file>

<file path="build.sh">
#!/usr/bin/env bash
# this_file: build.sh

set -e # Exit on error

echo "🧹 Cleaning up previous builds..."
rm -rf build/ dist/ *.egg-info .eggs/ .pytest_cache/ .coverage .tox/ .mypy_cache/

echo "🔍 Running type checks with mypy..."
python -m mypy src/pedalboard_pluginary

echo "�� Running tests..."
PYTHONPATH=src pytest tests/ -p no:flake8 -p no:briefcase

echo "📦 Building package..."
python -m build

echo "🚀 Installing locally..."
pip install -e .

echo "✨ Build and installation complete!"
</file>

<file path="LICENSE.txt">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="src/pedalboard_pluginary/base_scanner.py">
"""
Base scanner class implementing common functionality for all plugin scanners.
"""

import logging
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Optional

from .models import PluginInfo
from .protocols import PluginScanner

logger = logging.getLogger(__name__)


class BaseScanner(ABC):
    """Base class for plugin scanner implementations."""
    
    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
    ):
        """Initialize the scanner with optional ignore paths and specific paths.
        
        Args:
            ignore_paths: List of regex patterns for paths to ignore.
            specific_paths: List of specific paths to scan (if provided, only these are scanned).
        """
        self.ignore_paths = ignore_paths or []
        self.specific_paths = specific_paths or []
        self._compiled_ignore_patterns = [re.compile(pattern) for pattern in self.ignore_paths]
    
    @property
    @abstractmethod
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles (e.g., 'vst3', 'aufx')."""
        ...
    
    @property
    @abstractmethod
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        ...
    
    @abstractmethod
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all plugin files of this scanner's type.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to plugin files found.
        """
        ...
    
    @abstractmethod
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a single plugin file and return its information.
        
        Args:
            path: Path to the plugin file to scan.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        ...
    
    def validate_plugin_path(self, path: Path) -> bool:
        """Validate if a path is a valid plugin for this scanner.
        
        Args:
            path: Path to validate.
            
        Returns:
            True if the path is a valid plugin file, False otherwise.
        """
        if not path.exists():
            return False
        
        # Check extension
        if path.suffix not in self.supported_extensions:
            return False
        
        # Check against ignore patterns
        if self._should_ignore_path(path):
            return False
        
        # Check if specific paths are set and this path is in them
        if self.specific_paths and str(path) not in self.specific_paths:
            return False
        
        return True
    
    def _should_ignore_path(self, path: Path) -> bool:
        """Check if a path should be ignored based on ignore patterns.
        
        Args:
            path: Path to check.
            
        Returns:
            True if the path should be ignored, False otherwise.
        """
        path_str = str(path)
        for pattern in self._compiled_ignore_patterns:
            if pattern.search(path_str):
                logger.debug(f"Ignoring path {path} due to pattern {pattern.pattern}")
                return True
        return False
    
    def _filter_plugin_paths(self, paths: List[Path]) -> List[Path]:
        """Filter plugin paths based on validation criteria.
        
        Args:
            paths: List of paths to filter.
            
        Returns:
            Filtered list of valid plugin paths.
        """
        valid_paths = []
        for path in paths:
            if self.validate_plugin_path(path):
                valid_paths.append(path)
            else:
                logger.debug(f"Filtered out invalid plugin path: {path}")
        
        return valid_paths
    
    def _create_plugin_id(self, path: Path) -> str:
        """Create a unique plugin ID from its path.
        
        Args:
            path: Path to the plugin.
            
        Returns:
            Unique plugin ID string.
        """
        return f"{self.plugin_type}/{path.stem}"
</file>

<file path="src/pedalboard_pluginary/constants.py">
"""
Constants and configuration values for pedalboard_pluginary.
"""

from typing import Final

# Application metadata
APP_NAME: Final[str] = "com.twardoch.pedalboard-pluginary"
APP_VERSION: Final[str] = "0.1.0"  # TODO: Get from package metadata

# Cache configuration
CACHE_VERSION: Final[str] = "2.0.0"
PLUGINS_CACHE_FILENAME: Final[str] = "plugins"
IGNORES_CACHE_FILENAME: Final[str] = "ignores"

# Scanner configuration
DEFAULT_SCAN_TIMEOUT: Final[int] = 10  # seconds
PLUGIN_LOAD_TIMEOUT: Final[float] = 10.0  # seconds for individual plugin loading
MAX_SCAN_RETRIES: Final[int] = 3
SCAN_RETRY_DELAY: Final[float] = 1.0  # seconds

# Async scanning configuration
DEFAULT_MAX_CONCURRENT: Final[int] = 10  # concurrent async scans
MIN_CONCURRENT_SCANS: Final[int] = 1
MAX_CONCURRENT_SCANS: Final[int] = 50

# Plugin types
PLUGIN_TYPE_VST3: Final[str] = "vst3"
PLUGIN_TYPE_AU: Final[str] = "aufx"
SUPPORTED_PLUGIN_TYPES: Final[list[str]] = [PLUGIN_TYPE_VST3, PLUGIN_TYPE_AU]

# File extensions
VST3_EXTENSION: Final[str] = ".vst3"
AU_EXTENSION: Final[str] = ".component"

# Platform names
PLATFORM_WINDOWS: Final[str] = "Windows"
PLATFORM_MACOS: Final[str] = "Darwin"
PLATFORM_LINUX: Final[str] = "Linux"

# Progress reporting
DEFAULT_PROGRESS_BAR_WIDTH: Final[int] = 80
PROGRESS_UPDATE_INTERVAL: Final[float] = 0.1  # seconds

# Logging configuration
LOG_FORMAT: Final[str] = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_DATE_FORMAT: Final[str] = "%Y-%m-%d %H:%M:%S"

# CLI configuration
DEFAULT_OUTPUT_FORMAT: Final[str] = "json"
SUPPORTED_OUTPUT_FORMATS: Final[list[str]] = ["json", "yaml", "table", "csv"]

# Resource paths
RESOURCES_PACKAGE: Final[str] = "pedalboard_pluginary.resources"
DEFAULT_IGNORES_FILENAME: Final[str] = "default_ignores.json"
</file>

<file path="src/pedalboard_pluginary/models.py">
# pedalboard_pluginary/models.py
"""
Dataclasses for representing plugin information.
"""
from dataclasses import dataclass, field
from pathlib import Path
from typing import Union, Dict, Optional, Any

# ParameterValue is what we store (after conversion from pedalboard's raw param value)
ParameterValue = Union[float, bool, str]

@dataclass
class PluginParameter:
    """Represents a single parameter of a plugin."""
    name: str
    value: ParameterValue
    # Optional: Add other attributes like min_value, max_value, string_value, etc.
    # if Pedalboard consistently provides them and they are useful to store.
    # For now, keeping it simple with just name and current (default) value.
    # raw_pedalboard_value: Any # Could store the original pedalboard value if needed for debugging

@dataclass
class PluginInfo:
    """Represents a scanned audio plugin."""
    # Unique key for this plugin, e.g., "vst3/FabFilter Pro-Q 3" or "aufx/ChannelEQ"
    # This key might be different from `name` if a file contains multiple plugins or
    # if the user-facing name has characters not suitable for a key.
    # This will be the key in the main dictionary of plugins.
    id: str

    name: str # The display name of the plugin
    path: str # Path to the plugin file or bundle (as string for JSON serialization)
    filename: str # Filename of the plugin (e.g., "FabFilter Pro-Q 3.vst3")
    plugin_type: str # "vst3" or "aufx"

    # Parameters: dict where key is param name, value is PluginParameter object
    parameters: Dict[str, PluginParameter] = field(default_factory=dict)

    manufacturer: Optional[str] = None # Optional: Plugin manufacturer name

    # Optional: If a plugin file (e.g. VST3) can contain multiple uniquely identifiable
    # plugins, this field could store the specific name used to load this plugin
    # from the file, if different from the main `name`.
    # E.g. `pedalboard.load_plugin(path, plugin_name=name_in_file)`
    name_in_file: Optional[str] = None

    def __post_init__(self) -> None:
        # Ensure path is stored as a string for easier JSON serialization
        # Note: self.path is already typed as str, so this check is defensive
        pass

    # Consider adding methods for to_dict/from_dict if needed for complex serialization,
    # though dataclasses.asdict and direct instantiation usually suffice.

# Example usage:
# if __name__ == "__main__":
#     eq_param = PluginParameter(name="Frequency", value=1000.0)
#     gain_param = PluginParameter(name="Gain", value=0.0)
#     bypass_param = PluginParameter(name="Bypass", value=False)

#     example_plugin = PluginInfo(
#         id="vst3/AwesomeEQ",
#         name="Awesome EQ",
#         path="/path/to/AwesomeEQ.vst3",
#         filename="AwesomeEQ.vst3",
#         plugin_type="vst3",
#         parameters={
#             "Frequency": eq_param,
#             "Gain": gain_param,
#             "Bypass": bypass_param
#         },
#         manufacturer="MyPluginCompany"
#     )
#     import json
#     from dataclasses import asdict
#     print(json.dumps(asdict(example_plugin), indent=2))
</file>

<file path="src/pedalboard_pluginary/progress.py">
"""
Progress reporting implementations.
"""

import logging
from typing import Any, Callable, Optional

from tqdm import tqdm

from .protocols import ProgressReporter

logger = logging.getLogger(__name__)


class TqdmProgress(ProgressReporter):
    """Progress reporter using tqdm progress bars."""
    
    def __init__(self) -> None:
        """Initialize the progress reporter."""
        self._pbar: Optional[tqdm[Any]] = None
        self._total: int = 0
        self._current: int = 0
        self._description: str = ""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        self._description = description
        self._pbar = tqdm(total=total, desc=description)
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        if self._pbar is None:
            return
        
        self._current += amount
        self._pbar.update(amount)
        
        if message and hasattr(self._pbar, 'set_description'):
            # Update description with message
            self._pbar.set_description(f"{self._description} - {message}")
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        if self._pbar is None:
            return
        
        # Ensure we reach 100%
        if self._current < self._total:
            self._pbar.update(self._total - self._current)
        
        if message and hasattr(self._pbar, 'set_description'):
            # Update description with message
            self._pbar.set_description(f"{self._description} - {message}")
        
        self._pbar.close()
        self._pbar = None


class NoOpProgress(ProgressReporter):
    """No-operation progress reporter for quiet mode."""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking (no-op)."""
        pass
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress (no-op)."""
        pass
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking (no-op)."""
        pass


class LogProgress(ProgressReporter):
    """Progress reporter that logs to standard logging."""
    
    def __init__(self, log_level: int = logging.INFO):
        """Initialize the progress reporter.
        
        Args:
            log_level: Logging level to use for progress messages.
        """
        self._log_level = log_level
        self._total: int = 0
        self._current: int = 0
        self._description: str = ""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        self._description = description
        
        logger.log(
            self._log_level,
            f"Starting: {description} (0/{total})"
        )
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        self._current += amount
        
        progress_pct = (self._current / self._total * 100) if self._total > 0 else 0
        status = f"{self._description}: {self._current}/{self._total} ({progress_pct:.1f}%)"
        
        if message:
            status += f" - {message}"
        
        logger.log(self._log_level, status)
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        status = f"Completed: {self._description}"
        if message:
            status += f" - {message}"
        
        logger.log(self._log_level, status)


class CallbackProgress(ProgressReporter):
    """Progress reporter that calls user-provided callbacks."""
    
    def __init__(
        self,
        on_start: Optional[Callable[[int, str], None]] = None,
        on_update: Optional[Callable[[int, int, Optional[str]], None]] = None,
        on_finish: Optional[Callable[[Optional[str]], None]] = None,
    ):
        """Initialize the progress reporter with callbacks.
        
        Args:
            on_start: Callback for start(total, description).
            on_update: Callback for update(current, total, message).
            on_finish: Callback for finish(message).
        """
        self._on_start = on_start
        self._on_update = on_update
        self._on_finish = on_finish
        self._total: int = 0
        self._current: int = 0
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        
        if self._on_start:
            self._on_start(total, description)
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        self._current += amount
        
        if self._on_update:
            self._on_update(self._current, self._total, message)
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        if self._on_finish:
            self._on_finish(message)
</file>

<file path="tests/scanners/test_vst3_scanner.py">
# tests/scanners/test_vst3_scanner.py
import os
import platform
from pathlib import Path
from unittest.mock import MagicMock, mock_open, patch

import pytest

from pedalboard_pluginary.scanners.vst3_scanner import VST3Scanner


# Helper to create dummy VST3 files and folders
def create_dummy_vst3_structure(tmp_path, structure):
    """
    Creates a dummy VST3 plugin directory structure.
    structure is a dict like:
    {
        "folder_name": ["plugin1.vst3", "plugin2.vst3", {"subfolder": ["plugin3.vst3"]}]
    }
    """
    for name, contents in structure.items():
        current_path = tmp_path / name
        current_path.mkdir(parents=True, exist_ok=True)
        for item in contents:
            if isinstance(item, str):  # It's a file
                (current_path / item).touch()
            elif isinstance(item, dict):  # It's a sub-structure
                create_dummy_vst3_structure(current_path, item)


@pytest.fixture
def vst3_scanner_instance():
    return VST3Scanner()


@pytest.fixture
def vst3_scanner_with_ignores_instance():
    return VST3Scanner(ignore_paths=["vst3/IgnoredPlugin"])


class TestVST3Scanner:
    @patch("platform.system", return_value="Windows")
    @patch.dict(
        os.environ,
        {
            "ProgramFiles": "C:\\Program Files",
            "ProgramFiles(x86)": "C:\\Program Files (x86)",
        },
    )
    def test_get_default_vst3_folders_windows(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        # Create dummy common VST3 folders for Windows
        win_vst3_path1 = tmp_path / "Program Files" / "Common Files" / "VST3"
        win_vst3_path1.mkdir(parents=True, exist_ok=True)
        win_vst3_path2 = tmp_path / "Program Files (x86)" / "Common Files" / "VST3"
        win_vst3_path2.mkdir(parents=True, exist_ok=True)

        # Patch os.getenv to return mocked ProgramFiles paths relative to tmp_path
        def mock_getenv_windows(var_name, default=None):
            if var_name == "ProgramFiles":
                return str(tmp_path / "Program Files")
            if var_name == "ProgramFiles(x86)":
                return str(tmp_path / "Program Files (x86)")
            return default

        with patch("os.getenv", side_effect=mock_getenv_windows):
            folders = vst3_scanner_instance._get_default_vst3_folders()
            assert Path(win_vst3_path1).resolve() in folders
            assert Path(win_vst3_path2).resolve() in folders

    @patch("platform.system", return_value="Darwin")
    def test_get_default_vst3_folders_macos(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        mac_vst3_path1 = tmp_path / "Library" / "Audio" / "Plug-Ins" / "VST3"  # System
        mac_vst3_path1.mkdir(parents=True, exist_ok=True)
        # User path needs to be mocked for expanduser
        user_home_vst3_path = (
            tmp_path / "Users" / "testuser" / "Library" / "Audio" / "Plug-Ins" / "VST3"
        )
        user_home_vst3_path.mkdir(parents=True, exist_ok=True)

        with (
            patch("pathlib.Path.home", return_value=tmp_path / "Users" / "testuser"),
            patch(
                "pathlib.Path.expanduser",
                side_effect=lambda p: p
                if not str(p).startswith("~")
                else user_home_vst3_path,
            ),
        ):
            # Mock /Library path to point to our tmp_path version
            original_path_init = Path.__init__

            def mocked_path_init(self, *args, **kwargs):
                if args and args[0] == "/Library/Audio/Plug-Ins/VST3":
                    args = (str(mac_vst3_path1),) + args[1:]
                original_path_init(self, *args, **kwargs)

            with patch("pathlib.Path.__init__", mocked_path_init):
                folders = vst3_scanner_instance._get_default_vst3_folders()
                assert user_home_vst3_path.resolve() in folders
                assert mac_vst3_path1.resolve() in folders

    @patch("platform.system", return_value="Linux")
    def test_get_default_vst3_folders_linux(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        linux_vst3_path1 = tmp_path / ".vst3"  # User
        linux_vst3_path1.mkdir(parents=True, exist_ok=True)
        linux_vst3_path2 = tmp_path / "usr" / "lib" / "vst3"  # System
        linux_vst3_path2.mkdir(parents=True, exist_ok=True)

        with (
            patch("pathlib.Path.home", return_value=tmp_path),
            patch(
                "pathlib.Path.expanduser",
                side_effect=lambda p: p
                if not str(p).startswith("~")
                else linux_vst3_path1,
            ),
        ):
            # Mock /usr/lib/vst3 to point to our tmp_path version
            original_path_init = Path.__init__

            def mocked_path_init(self, *args, **kwargs):
                if args and args[0] == "/usr/lib/vst3":
                    args = (str(linux_vst3_path2),) + args[1:]
                elif (
                    args and args[0] == "/usr/local/lib/vst3"
                ):  # Also mock this common path
                    args = (str(tmp_path / "usr" / "local" / "lib" / "vst3"),) + args[
                        1:
                    ]
                    (tmp_path / "usr" / "local" / "lib" / "vst3").mkdir(
                        parents=True, exist_ok=True
                    )

                original_path_init(self, *args, **kwargs)

            with patch("pathlib.Path.__init__", mocked_path_init):
                folders = vst3_scanner_instance._get_default_vst3_folders()
                assert linux_vst3_path1.resolve() in folders
                assert linux_vst3_path2.resolve() in folders

    def test_find_plugin_files_discovery(self, vst3_scanner_instance, tmp_path):
        # Create a dummy default folder and put some plugins in it
        default_folder = tmp_path / "DefaultVST3s"
        default_folder.mkdir()
        (default_folder / "PluginA.vst3").touch()
        (default_folder / "PluginB.vst3").touch()

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 2
            assert default_folder / "PluginA.vst3" in found_plugins
            assert default_folder / "PluginB.vst3" in found_plugins

    def test_find_plugin_files_with_extra_folders(
        self, vst3_scanner_instance, tmp_path
    ):
        extra_folder1 = tmp_path / "ExtraVST3s1"
        extra_folder1.mkdir()
        (extra_folder1 / "PluginC.vst3").touch()

        extra_folder2 = tmp_path / "ExtraVST3s2"  # Non-existent

        # Mock default folders to be empty to isolate test to extra_folders
        with patch.object(VST3Scanner, "_get_default_vst3_folders", return_value=[]):
            found_plugins = vst3_scanner_instance.find_plugin_files(
                extra_folders=[str(extra_folder1), str(extra_folder2)]
            )
            assert len(found_plugins) == 1
            assert extra_folder1 / "PluginC.vst3" in found_plugins

    def test_find_plugin_files_with_specific_paths(
        self, vst3_scanner_instance, tmp_path
    ):
        plugin_path1 = tmp_path / "SpecificPlugin1.vst3"
        plugin_path1.touch()
        plugin_path2 = tmp_path / "SpecificPlugin2.vst3"  # Non-existent for this call

        found_plugins = vst3_scanner_instance.find_plugin_files(
            plugin_paths=[plugin_path1, plugin_path2]
        )
        assert len(found_plugins) == 1
        assert (
            plugin_path1 in found_plugins
        )  # plugin_path2 should not be found as it doesn't exist yet

    def test_find_plugin_files_with_ignores(
        self, vst3_scanner_with_ignores_instance, tmp_path
    ):
        default_folder = tmp_path / "VST3WithIgnores"
        default_folder.mkdir()
        (default_folder / "NormalPlugin.vst3").touch()
        (
            default_folder / "IgnoredPlugin.vst3"
        ).touch()  # This one has stem "IgnoredPlugin"

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_with_ignores_instance.find_plugin_files()
            assert len(found_plugins) == 1
            assert default_folder / "NormalPlugin.vst3" in found_plugins
            assert default_folder / "IgnoredPlugin.vst3" not in found_plugins

    def test_find_plugin_files_no_folders_exist(self, vst3_scanner_instance):
        with patch.object(VST3Scanner, "_get_default_vst3_folders", return_value=[]):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 0

    def test_find_plugin_files_skips_directories_with_vst3_suffix(
        self, vst3_scanner_instance, tmp_path
    ):
        default_folder = tmp_path / "VST3WithDirs"
        default_folder.mkdir()
        (default_folder / "RealPlugin.vst3").touch()
        (default_folder / "FakePlugin.vst3").mkdir()  # A directory named like a plugin

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 1
            assert default_folder / "RealPlugin.vst3" in found_plugins
            assert default_folder / "FakePlugin.vst3" not in found_plugins


# TODO: Test case where a plugin_path provided to find_plugin_files is a directory (should be ignored)
# TODO: Test case with symlinks if relevant (Path.resolve() should handle them, but good to be aware)
# TODO: Test case for duplicate plugin paths from overlapping folder definitions (should be unique)
#       (find_plugin_files uses a set internally for discovery before sorting, so this should be handled)
</file>

<file path="tests/test_data.py">
import os
from pedalboard_pluginary.data import get_cache_path
from unittest.mock import patch

def test_get_cache_path_windows():
    with patch.dict(os.environ, {"APPDATA": "C:\\Users\\TestUser\\AppData"}):
        path = get_cache_path("test_cache")
        assert str(path) == "C:\\Users\\TestUser\\AppData\\com.twardoch.pedalboard-pluginary\\test_cache.json"

@patch('platform.system', return_value='Darwin')
def test_get_cache_path_macos(mock_platform_system):
    # Test for macOS when APPDATA is not set (should not be used)
    # and XDG_CACHE_HOME is not set (should not be used)
    with patch.dict(os.environ, {}, clear=True):
        path = get_cache_path("test_cache")
        home = os.path.expanduser("~")
        expected_path = f"{home}/Library/Application Support/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path

@patch('platform.system', return_value='Linux')
def test_get_cache_path_linux_xdg_set(mock_platform_system):
    xdg_cache_dir = "/custom/xdg/cache"
    with patch.dict(os.environ, {"XDG_CACHE_HOME": xdg_cache_dir}, clear=True):
        path = get_cache_path("test_cache")
        expected_path = f"{xdg_cache_dir}/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path

@patch('platform.system', return_value='Linux')
def test_get_cache_path_linux_xdg_not_set(mock_platform_system):
    # Test when XDG_CACHE_HOME is not set
    with patch.dict(os.environ, {}, clear=True): # Ensure XDG_CACHE_HOME is not set
        path = get_cache_path("test_cache")
        home = os.path.expanduser("~")
        expected_path = f"{home}/.cache/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path
</file>

<file path=".gitignore">
temp/

# Temporary and binary files
*~
*.py[cod]
*.so
*.cfg
!.isort.cfg
!setup.cfg
*.orig
*.log
*.pot
__pycache__/*
.cache/*
.*.swp
*/.ipynb_checkpoints/*
.DS_Store

# Project files
.ropeproject
.project
.pydevproject
.settings
.idea
.vscode
tags

# Package files
*.egg
*.eggs/
.installed.cfg
*.egg-info

# Unittest and coverage
htmlcov/*
.coverage
.coverage.*
.tox
junit*.xml
coverage.xml
.pytest_cache/

# Build and docs folder/files
build/*
dist/*
sdist/*
docs/api/*
docs/_rst/*
docs/_build/*
cover/*
MANIFEST

# Per-project virtualenvs
.venv*/
.conda*/
.python-version
</file>

<file path=".pre-commit-config.yaml">
exclude: '^docs/conf.py'

repos:
- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v4.5.0
  hooks:
  - id: trailing-whitespace
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
  - id: debug-statements
  - id: end-of-file-fixer
  - id: requirements-txt-fixer
  - id: mixed-line-ending
    args: ['--fix=auto']  # replace 'auto' with 'lf' to enforce Linux/Mac line endings or 'crlf' for Windows

## If you want to automatically "modernize" your Python code:
# - repo: https://github.com/asottile/pyupgrade
#   rev: v3.7.0
#   hooks:
#   - id: pyupgrade
#     args: ['--py37-plus']

## If you want to avoid flake8 errors due to unused vars or imports:
# - repo: https://github.com/PyCQA/autoflake
#   rev: v2.1.1
#   hooks:
#   - id: autoflake
#     args: [
#       --in-place,
#       --remove-all-unused-imports,
#       --remove-unused-variables,
#     ]

- repo: https://github.com/PyCQA/isort
  rev: 5.12.0
  hooks:
  - id: isort

- repo: https://github.com/psf/black
  rev: 23.11.0
  hooks:
  - id: black
    language_version: python3

## If like to embrace black styles even in the docs:
# - repo: https://github.com/asottile/blacken-docs
#   rev: v1.13.0
#   hooks:
#   - id: blacken-docs
#     additional_dependencies: [black]

- repo: https://github.com/PyCQA/flake8
  rev: 6.1.0
  hooks:
  - id: flake8
  ## You can add flake8 plugins via `additional_dependencies`:
  #  additional_dependencies: [flake8-bugbear]

- repo: https://github.com/pre-commit/mirrors-mypy
  rev: v1.7.0 # Or choose the latest version
  hooks:
  - id: mypy
    # You might need to specify `additional_dependencies` for mypy to find your project's dependencies
    # e.g., additional_dependencies: [types-setuptools, types-requests]
    # For this project:
    additional_dependencies: [
      types-setuptools, # For pkg_resources, etc.
      # Add stubs for other dependencies if mypy complains and they exist
      # types-fire, types-tqdm, types-python-benedict might not exist or be mature.
      # For now, we'll rely on inline # type: ignore for problematic libs
      # and the mypy config in pyproject.toml for global settings.
      "pedalboard", # To make mypy aware of pedalboard, even if it has no stubs
      "fire",
      "tqdm",
      "python-benedict"
    ]
    # It's good practice to also configure mypy via pyproject.toml or mypy.ini
    # For example, to specify the Python version, follow imports, etc.
    args: [--config-file=pyproject.toml] # Point to pyproject.toml for config

## Check for misspells in documentation files:
# - repo: https://github.com/codespell-project/codespell
#   rev: v2.2.5
#   hooks:
#   - id: codespell
</file>

<file path="AUTHORS.md">
# Contributors

* Adam Twardoch <adam+github@twardoch.com>
</file>

<file path="tox.ini">
# Tox configuration file
# Read more under https://tox.wiki/
# THIS SCRIPT IS SUPPOSED TO BE AN EXAMPLE. MODIFY IT ACCORDING TO YOUR NEEDS!

[tox]
minversion = 3.24
envlist = default
isolated_build = True

[testenv]
description = Invoke pytest to run automated tests
setenv =
    TOXINIDIR = {toxinidir}
passenv =
    HOME
    SETUPTOOLS_*
extras =
    testing
commands =
    pytest {posargs}

# To run `tox -e lint` you need to make sure you have a
# `.pre-commit-config.yaml` file. See https://pre-commit.com
# [testenv:lint]
# description = Perform static analysis and style checks
# skip_install = True
# deps = pre-commit
# passenv =
#     HOMEPATH
#     PROGRAMDATA
#     SETUPTOOLS_*
# commands =
#     pre-commit run --all-files {posargs:--show-diff-on-failure}

[testenv:{build,clean}]
description =
    build: Build the package in isolation according to PEP517, see https://github.com/pypa/build
    clean: Remove old distribution files and temporary build artifacts (./build and ./dist)
skip_install = True
changedir = {toxinidir}
deps =
    build: build[virtualenv]
passenv =
    SETUPTOOLS_*
commands =
    clean: python -c 'import shutil; [shutil.rmtree(p, True) for p in ("build", "dist")]'
    clean: python -c 'import pathlib, shutil; [shutil.rmtree(p, True) for p in pathlib.Path("src").glob("*.egg-info")]'
    build: python -m build {posargs}

[testenv:publish]
description =
    Publish the package you have been developing to a package index server.
    By default, it uses testpypi. If you really want to publish your package
    to be publicly accessible in PyPI, use the `-- --repository pypi` option.
skip_install = True
changedir = {toxinidir}
passenv =
    TWINE_USERNAME
    TWINE_PASSWORD
    TWINE_REPOSITORY
    TWINE_REPOSITORY_URL
deps = twine
commands =
    python -m twine check dist/*
    python -m twine upload {posargs:--repository {env:TWINE_REPOSITORY:testpypi}} dist/*
</file>

<file path=".github/workflows/ci.yml">
name: Python package CI

on:
  push:
    branches: [main]
    tags: ['v*']
  pull_request:
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install fire pytest pytest-cov # Added pytest-cov for coverage
    - name: Run tests with coverage
      run: |
        python -m pip install -e .
        # Pytest is configured in pyproject.toml to run with --cov
        # and output to term-missing. It also creates .coverage file.
        pytest
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        # token: ${{ secrets.CODECOV_TOKEN }} # Only if needed for private repos or specific cases
        fail_ci_if_error: true # Optional: fail CI if coverage upload fails

  publish:
    needs: build-and-test
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    - name: Build and publish
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
      run: |
        python -m pip install --upgrade build twine
        python -m build
        twine upload dist/*
</file>

<file path="src/pedalboard_pluginary/__init__.py">
from importlib.metadata import PackageNotFoundError, version

try:
    # Change here if project is renamed and does not equal the package name
    dist_name = __name__
    __version__ = version(dist_name)
except PackageNotFoundError:  # pragma: no cover
    __version__ = "unknown"
finally:
    del version, PackageNotFoundError

from .core import PedalboardPluginary
</file>

<file path="src/pedalboard_pluginary/serialization.py">
"""
Unified serialization module for plugin data.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from .constants import APP_VERSION, CACHE_VERSION
from .exceptions import CacheCorruptedError, CacheVersionError, CacheWriteError
from .models import PluginInfo, PluginParameter
from .types import (
    CacheData,
    CacheMetadata,
    SerializedParameter,
    SerializedPlugin,
    is_serialized_parameter,
    is_serialized_plugin,
)
from .utils import ensure_folder

logger = logging.getLogger(__name__)


class PluginSerializer:
    """Handles serialization and deserialization of plugin data."""
    
    @staticmethod
    def parameter_to_dict(param: PluginParameter) -> SerializedParameter:
        """Convert PluginParameter to serializable dictionary.
        
        Args:
            param: PluginParameter object to serialize.
            
        Returns:
            SerializedParameter dictionary.
        """
        return {
            "name": param.name,
            "value": param.value,
        }
    
    @staticmethod
    def dict_to_parameter(data: Dict[str, Any]) -> Optional[PluginParameter]:
        """Convert dictionary to PluginParameter with validation.
        
        Args:
            data: Dictionary containing parameter data.
            
        Returns:
            PluginParameter object if valid, None otherwise.
        """
        if not is_serialized_parameter(data):
            logger.warning(f"Invalid parameter data: {data}")
            return None
        
        return PluginParameter(
            name=data["name"],
            value=data["value"],
        )
    
    @staticmethod
    def plugin_to_dict(plugin: PluginInfo) -> SerializedPlugin:
        """Convert PluginInfo to serializable dictionary.
        
        Args:
            plugin: PluginInfo object to serialize.
            
        Returns:
            SerializedPlugin dictionary.
        """
        # Convert parameters
        params_dict: Dict[str, SerializedParameter] = {}
        for param_name, param in plugin.parameters.items():
            params_dict[param_name] = PluginSerializer.parameter_to_dict(param)
        
        result: SerializedPlugin = {
            "id": plugin.id,
            "name": plugin.name,
            "path": plugin.path,
            "filename": plugin.filename,
            "plugin_type": plugin.plugin_type,
            "parameters": params_dict,
            "manufacturer": plugin.manufacturer,
            "name_in_file": plugin.name_in_file,
        }
        
        return result
    
    @staticmethod
    def dict_to_plugin(data: Dict[str, Any]) -> Optional[PluginInfo]:
        """Convert dictionary to PluginInfo with validation.
        
        Args:
            data: Dictionary containing plugin data.
            
        Returns:
            PluginInfo object if valid, None otherwise.
        """
        if not is_serialized_plugin(data):
            logger.warning(f"Invalid plugin data for ID: {data.get('id', 'unknown')}")
            return None
        
        # Convert parameters
        params: Dict[str, PluginParameter] = {}
        for param_name, param_data in data.get("parameters", {}).items():
            param = PluginSerializer.dict_to_parameter(param_data)
            if param:
                params[param_name] = param
        
        return PluginInfo(
            id=data["id"],
            name=data["name"],
            path=data["path"],
            filename=data["filename"],
            plugin_type=data["plugin_type"],
            parameters=params,
            manufacturer=data.get("manufacturer"),
            name_in_file=data.get("name_in_file"),
        )
    
    @classmethod
    def create_cache_metadata(cls, plugin_count: int) -> CacheMetadata:
        """Create cache metadata.
        
        Args:
            plugin_count: Number of plugins in the cache.
            
        Returns:
            CacheMetadata dictionary.
        """
        now = datetime.utcnow().isoformat()
        return {
            "version": CACHE_VERSION,
            "created_at": now,
            "updated_at": now,
            "plugin_count": plugin_count,
            "scanner_version": APP_VERSION,
        }
    
    @classmethod
    def save_plugins(cls, plugins: Dict[str, PluginInfo], path: Path) -> None:
        """Save plugins to JSON file with metadata and error handling.
        
        Args:
            plugins: Dictionary mapping plugin IDs to PluginInfo objects.
            path: Path to the cache file.
        """
        ensure_folder(path.parent)
        
        # Convert plugins to serializable format
        plugins_dict: Dict[str, SerializedPlugin] = {}
        for plugin_id, plugin in plugins.items():
            try:
                plugins_dict[plugin_id] = cls.plugin_to_dict(plugin)
            except Exception as e:
                logger.error(f"Failed to serialize plugin {plugin_id}: {e}")
                continue
        
        # Create cache data with metadata
        cache_data: CacheData = {
            "metadata": cls.create_cache_metadata(len(plugins_dict)),
            "plugins": plugins_dict,
        }
        
        # Write to file with error handling
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2)
            logger.info(f"Saved {len(plugins_dict)} plugins to {path}")
        except Exception as e:
            logger.error(f"Failed to save plugins to {path}: {e}")
            raise CacheWriteError(str(path), str(e))
    
    @classmethod
    def load_plugins(cls, path: Path) -> Dict[str, PluginInfo]:
        """Load plugins from JSON file with validation.
        
        Args:
            path: Path to the cache file.
            
        Returns:
            Dictionary mapping plugin IDs to PluginInfo objects.
        """
        if not path.exists():
            logger.info(f"Cache file not found: {path}")
            return {}
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in cache file {path}: {e}")
            raise CacheCorruptedError(str(path), f"JSON decode error: {e}")
        except Exception as e:
            logger.error(f"Failed to read cache file {path}: {e}")
            raise CacheCorruptedError(str(path), str(e))
        
        # Handle both old format (direct plugin dict) and new format (with metadata)
        if isinstance(data, dict) and "metadata" in data and "plugins" in data:
            # New format with metadata
            metadata = data.get("metadata", {})
            cache_version = metadata.get("version", "1.0.0")
            
            if cache_version != CACHE_VERSION:
                logger.warning(f"Cache version mismatch: expected {CACHE_VERSION}, got {cache_version}")
                raise CacheVersionError(CACHE_VERSION, cache_version, str(path))
            
            plugins_data = data.get("plugins", {})
        else:
            # Old format - direct plugin dictionary
            logger.info("Loading cache in legacy format")
            plugins_data = data
        
        # Convert to PluginInfo objects
        plugins: Dict[str, PluginInfo] = {}
        for plugin_id, plugin_data in plugins_data.items():
            if not isinstance(plugin_data, dict):
                logger.warning(f"Invalid plugin data for ID {plugin_id}")
                continue
            
            plugin = cls.dict_to_plugin(plugin_data)
            if plugin:
                plugins[plugin_id] = plugin
            else:
                logger.warning(f"Failed to deserialize plugin {plugin_id}")
        
        logger.info(f"Loaded {len(plugins)} plugins from {path}")
        return plugins
    
    @classmethod
    def migrate_cache(cls, old_data: Dict[str, Any], old_version: str, new_version: str) -> Dict[str, Any]:
        """Migrate cache data from old version to new version.
        
        Args:
            old_data: Cache data in old format.
            old_version: Version of the old cache format.
            new_version: Target version to migrate to.
            
        Returns:
            Migrated cache data.
        """
        # TODO: Implement cache migration logic as needed
        logger.info(f"Migrating cache from version {old_version} to {new_version}")
        return old_data
</file>

<file path="src/pedalboard_pluginary/types.py">
"""
Type definitions and aliases for the pedalboard_pluginary package.
"""

from typing import Union, Dict, Any, TypedDict, Optional
import sys

if sys.version_info >= (3, 11):
    from typing import NotRequired
else:
    from typing_extensions import NotRequired

# Basic type aliases
ParameterValue = Union[float, bool, str]
PluginID = str
PluginType = str  # "vst3" or "aufx"
PluginPath = str  # String representation of path for JSON serialization


class SerializedParameter(TypedDict):
    """TypedDict for serialized plugin parameter."""
    name: str
    value: ParameterValue


class SerializedPlugin(TypedDict):
    """TypedDict for serialized plugin data."""
    id: str
    name: str
    path: str
    filename: str
    plugin_type: str
    parameters: Dict[str, SerializedParameter]
    manufacturer: NotRequired[Optional[str]]
    name_in_file: NotRequired[Optional[str]]


class CacheMetadata(TypedDict):
    """TypedDict for cache metadata."""
    version: str
    created_at: str
    updated_at: str
    plugin_count: int
    scanner_version: str


class CacheData(TypedDict):
    """TypedDict for complete cache data structure."""
    metadata: CacheMetadata
    plugins: Dict[str, SerializedPlugin]


# Type guards
def is_parameter_value(value: Any) -> bool:
    """Check if a value is a valid ParameterValue."""
    return isinstance(value, (float, bool, str))


def is_serialized_parameter(data: Any) -> bool:
    """Check if data is a valid SerializedParameter."""
    return (
        isinstance(data, dict) and
        "name" in data and
        "value" in data and
        isinstance(data["name"], str) and
        is_parameter_value(data["value"])
    )


def is_serialized_plugin(data: Any) -> bool:
    """Check if data is a valid SerializedPlugin."""
    if not isinstance(data, dict):
        return False
    
    required_fields = ["id", "name", "path", "filename", "plugin_type", "parameters"]
    for field in required_fields:
        if field not in data:
            return False
    
    # Check types of required fields
    if not all(isinstance(data[field], str) for field in ["id", "name", "path", "filename", "plugin_type"]):
        return False
    
    # Check parameters
    if not isinstance(data["parameters"], dict):
        return False
    
    for param_name, param_data in data["parameters"].items():
        if not isinstance(param_name, str) or not is_serialized_parameter(param_data):
            return False
    
    # Check optional fields
    if "manufacturer" in data and data["manufacturer"] is not None:
        if not isinstance(data["manufacturer"], str):
            return False
    
    if "name_in_file" in data and data["name_in_file"] is not None:
        if not isinstance(data["name_in_file"], str):
            return False
    
    return True
</file>

<file path="src/pedalboard_pluginary/utils.py">
from pathlib import Path
from typing import Any, Union

def ensure_folder(path: Path) -> None:
    """ Ensure that a folder exists. """
    path.parent.mkdir(parents=True, exist_ok=True)

def from_pb_param(data: Any) -> Union[float, bool, str]:
    """
    Converts a pedalboard parameter value to a Python native type.
    Pedalboard parameter values can be string representations of floats, booleans, or just strings.
    """
    drep = str(data)
    try:
        return float(drep)
    except ValueError:
        pass
    if drep.lower() == "true":
        return True
    if drep.lower() == "false":
        return False
    return drep
</file>

<file path="PLAN.md">
# Pedalboard Pluginary - Production Readiness Roadmap

## Executive Summary

Based on comprehensive codebase analysis, Pedalboard Pluginary has excellent architectural foundations but requires significant performance and user experience improvements to become production-ready. The current implementation suffers from sequential scanning bottlenecks and lacks modern CLI features, but the protocol-based design makes these improvements straightforward to implement.

## Current State Assessment

### ✅ Completed Strengths
- **Type-safe architecture** with comprehensive TypedDict and protocol definitions
- **Modular scanner design** supporting VST3 and AU plugins with extensible BaseScanner
- **Robust error handling** with custom exception hierarchy and timeout protection
- **Unified serialization layer** with validation and type safety
- **Cross-platform support** with OS-specific path handling
- **Zero mypy errors** in strict mode - excellent type safety foundation

### ❌ Critical Performance Bottlenecks
- **Sequential scanning** - 1-2 plugins/second (should be 10-20 with async)
- **Memory inefficiency** - O(n) memory usage, loads entire cache
- **No concurrency** - Single-threaded design wastes modern hardware
- **Basic caching** - JSON files don't scale beyond 100s of plugins

### ❌ User Experience Gaps  
- **Outdated CLI** using Fire framework with poor error handling
- **No search/filtering** capabilities for large plugin libraries
- **Limited output formats** - missing tables, rich formatting
- **No configuration management** system

## Phase 1: Async Performance Revolution (Week 1-2)

### 1.1 Async Scanner Architecture (High Impact)

**Problem**: Sequential plugin scanning is the #1 performance bottleneck.

**Solution**: Implement fully async scanner with configurable concurrency.

```python
# src/pedalboard_pluginary/async_scanner.py
import asyncio
from typing import AsyncIterator, List, Optional
from .protocols import PluginScanner
from .models import PluginInfo

class AsyncScannerMixin:
    """Mixin to add async capabilities to scanners."""
    
    async def scan_plugin_async(self, path: Path) -> Optional[PluginInfo]:
        """Async wrapper for plugin scanning with timeout."""
        loop = asyncio.get_event_loop()
        try:
            # Use asyncio.to_thread for CPU-bound plugin loading
            return await asyncio.wait_for(
                asyncio.to_thread(self.scan_plugin, path),
                timeout=PLUGIN_LOAD_TIMEOUT
            )
        except asyncio.TimeoutError:
            logger.warning(f"Plugin {path} timed out during async scan")
            return None
    
    async def scan_plugins_batch(
        self, 
        paths: List[Path], 
        max_concurrent: int = 10,
        progress_reporter: Optional[ProgressReporter] = None
    ) -> AsyncIterator[PluginInfo]:
        """Scan multiple plugins concurrently with backpressure control."""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def scan_with_semaphore(path: Path) -> Optional[PluginInfo]:
            async with semaphore:
                return await self.scan_plugin_async(path)
        
        # Create tasks for all paths
        tasks = [scan_with_semaphore(path) for path in paths]
        
        # Process with progress tracking
        if progress_reporter:
            progress_reporter.start(len(tasks), f"Scanning {len(tasks)} plugins")
        
        completed = 0
        for coro in asyncio.as_completed(tasks):
            result = await coro
            completed += 1
            
            if progress_reporter:
                progress_reporter.update(1, f"Completed {completed}/{len(tasks)}")
            
            if result:
                yield result
        
        if progress_reporter:
            progress_reporter.finish("Scan completed")

class AsyncVST3Scanner(VST3Scanner, AsyncScannerMixin):
    """VST3 scanner with async capabilities."""
    pass

class AsyncAUScanner(AUScanner, AsyncScannerMixin):  
    """AU scanner with async capabilities."""
    pass
```

**Implementation Steps**:
1. Create AsyncScannerMixin with concurrent plugin loading
2. Implement AsyncVST3Scanner and AsyncAUScanner
3. Add configurable concurrency limits in constants
4. Update PedalboardScanner to support async mode
5. Benchmark performance improvements (target: 5-10x speedup)

**Expected Impact**: 
- **Performance**: 10-20 plugins/second (vs 1-2 current)
- **Resource Utilization**: Better CPU and I/O usage
- **User Experience**: Responsive progress reporting

### 1.2 SQLite Cache Backend (Scalability)

**Problem**: JSON file cache doesn't scale beyond 100s of plugins.

**Solution**: Implement SQLite backend with indexing and lazy loading.

```python
# src/pedalboard_pluginary/cache/sqlite_backend.py
import sqlite3
import json
from typing import Dict, Optional, Iterator, List
from pathlib import Path
from ..models import PluginInfo
from ..protocols import CacheBackend

class SQLiteCacheBackend(CacheBackend):
    """SQLite-based cache with indexed access and lazy loading."""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self) -> None:
        """Initialize database schema with indexes."""
        with self._connect() as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS plugins (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    path TEXT NOT NULL,
                    plugin_type TEXT NOT NULL,
                    manufacturer TEXT,
                    data TEXT NOT NULL,
                    last_modified REAL NOT NULL,
                    created_at REAL NOT NULL
                );
                
                -- Indexes for fast searching
                CREATE INDEX IF NOT EXISTS idx_plugins_name ON plugins(name);
                CREATE INDEX IF NOT EXISTS idx_plugins_type ON plugins(plugin_type);
                CREATE INDEX IF NOT EXISTS idx_plugins_manufacturer ON plugins(manufacturer);
                CREATE INDEX IF NOT EXISTS idx_plugins_path ON plugins(path);
                CREATE INDEX IF NOT EXISTS idx_plugins_modified ON plugins(last_modified);
                
                -- Full-text search support
                CREATE VIRTUAL TABLE IF NOT EXISTS plugins_fts USING fts5(
                    id, name, manufacturer, content=plugins
                );
            """)
    
    def get(self, plugin_id: str) -> Optional[PluginInfo]:
        """Get single plugin without loading entire cache."""
        with self._connect() as conn:
            row = conn.execute(
                "SELECT data FROM plugins WHERE id = ?", 
                (plugin_id,)
            ).fetchone()
            
            if row:
                data = json.loads(row[0])
                return PluginSerializer.dict_to_plugin(data)
            return None
    
    def search(
        self, 
        query: Optional[str] = None,
        plugin_type: Optional[str] = None,
        manufacturer: Optional[str] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Iterator[PluginInfo]:
        """Search plugins with filters and pagination."""
        conditions = []
        params = []
        
        if query:
            # Use FTS for text search
            conditions.append("id IN (SELECT id FROM plugins_fts WHERE plugins_fts MATCH ?)")
            params.append(query)
        
        if plugin_type:
            conditions.append("plugin_type = ?")
            params.append(plugin_type)
        
        if manufacturer:
            conditions.append("manufacturer LIKE ?")
            params.append(f"%{manufacturer}%")
        
        where_clause = " AND ".join(conditions) if conditions else "1"
        
        sql = f"""
            SELECT data FROM plugins 
            WHERE {where_clause}
            ORDER BY name
            LIMIT ? OFFSET ?
        """
        params.extend([limit, offset])
        
        with self._connect() as conn:
            for row in conn.execute(sql, params):
                data = json.loads(row[0])
                plugin = PluginSerializer.dict_to_plugin(data)
                if plugin:
                    yield plugin
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Save plugins to database with transaction."""
        with self._connect() as conn:
            for plugin_id, plugin in plugins.items():
                data = PluginSerializer.plugin_to_dict(plugin)
                conn.execute("""
                    INSERT OR REPLACE INTO plugins 
                    (id, name, path, plugin_type, manufacturer, data, last_modified, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    plugin_id,
                    plugin.name,
                    plugin.path,
                    plugin.plugin_type,
                    plugin.manufacturer,
                    json.dumps(data),
                    Path(plugin.path).stat().st_mtime,
                    time.time()
                ))
            
            # Update FTS index
            conn.execute("INSERT INTO plugins_fts(plugins_fts) VALUES('rebuild')")
    
    def update(self, plugin_id: str, plugin: PluginInfo) -> None:
        """Update single plugin efficiently."""
        self.save({plugin_id: plugin})
    
    def delete(self, plugin_id: str) -> None:
        """Delete plugin from cache."""
        with self._connect() as conn:
            conn.execute("DELETE FROM plugins WHERE id = ?", (plugin_id,))
    
    def clear(self) -> None:
        """Clear all plugins from cache."""
        with self._connect() as conn:
            conn.execute("DELETE FROM plugins")
    
    def exists(self) -> bool:
        """Check if cache exists and has plugins."""
        if not self.db_path.exists():
            return False
        
        with self._connect() as conn:
            count = conn.execute("SELECT COUNT(*) FROM plugins").fetchone()[0]
            return count > 0
    
    def get_stats(self) -> Dict[str, int]:
        """Get cache statistics."""
        with self._connect() as conn:
            total = conn.execute("SELECT COUNT(*) FROM plugins").fetchone()[0]
            
            type_counts = {}
            for row in conn.execute("SELECT plugin_type, COUNT(*) FROM plugins GROUP BY plugin_type"):
                type_counts[row[0]] = row[1]
            
            return {
                "total_plugins": total,
                "by_type": type_counts
            }
    
    def _connect(self) -> sqlite3.Connection:
        """Create database connection with optimizations."""
        conn = sqlite3.connect(self.db_path)
        conn.execute("PRAGMA journal_mode=WAL")  # Better concurrency
        conn.execute("PRAGMA synchronous=NORMAL")  # Performance vs safety balance
        conn.execute("PRAGMA cache_size=-64000")  # 64MB cache
        return conn
```

**Implementation Steps**:
1. Create cache package with SQLiteCacheBackend
2. Implement indexed search with full-text capabilities  
3. Add lazy loading for large datasets
4. Create migration from JSON to SQLite
5. Add cache statistics and management commands

**Expected Impact**:
- **Search Performance**: O(log n) vs O(n) current
- **Memory Usage**: Constant vs linear current
- **Scalability**: Handle 10,000+ plugins efficiently

### 1.3 Smart Cache Invalidation

**Problem**: No incremental updates - full rescans required.

**Solution**: File-based change detection with intelligent updates.

```python
# src/pedalboard_pluginary/cache/change_detection.py
import time
from pathlib import Path
from typing import Dict, List, Set
from ..models import PluginInfo

class ChangeDetector:
    """Detects changes in plugin files for incremental updates."""
    
    def __init__(self, cache_backend: CacheBackend):
        self.cache = cache_backend
        
    def detect_changes(self, scan_paths: List[Path]) -> Dict[str, List[Path]]:
        """Detect changed, added, and removed plugins."""
        current_files = self._discover_plugin_files(scan_paths)
        cached_plugins = self._get_cached_plugins()
        
        # Detect changes by comparing modification times
        changes = {
            "added": [],
            "modified": [],
            "removed": []
        }
        
        # Check for new and modified files
        for file_path in current_files:
            plugin_id = self._path_to_id(file_path)
            file_mtime = file_path.stat().st_mtime
            
            if plugin_id in cached_plugins:
                cached_mtime = cached_plugins[plugin_id].get("last_modified", 0)
                if file_mtime > cached_mtime:
                    changes["modified"].append(file_path)
            else:
                changes["added"].append(file_path)
        
        # Check for removed files
        cached_paths = {Path(p["path"]) for p in cached_plugins.values()}
        current_paths = set(current_files)
        removed_paths = cached_paths - current_paths
        changes["removed"].extend(removed_paths)
        
        return changes
    
    def incremental_scan(
        self, 
        scan_paths: List[Path], 
        scanners: List[PluginScanner],
        progress_reporter: Optional[ProgressReporter] = None
    ) -> Dict[str, PluginInfo]:
        """Perform incremental scan of only changed plugins."""
        changes = self.detect_changes(scan_paths)
        total_changes = len(changes["added"]) + len(changes["modified"])
        
        if progress_reporter:
            progress_reporter.start(total_changes, "Incremental scan")
        
        updated_plugins = {}
        
        # Scan new and modified plugins
        for file_path in changes["added"] + changes["modified"]:
            scanner = self._find_scanner_for_path(file_path, scanners)
            if scanner:
                plugin_info = scanner.scan_plugin(file_path)
                if plugin_info:
                    updated_plugins[plugin_info.id] = plugin_info
                
                if progress_reporter:
                    progress_reporter.update(1)
        
        # Remove deleted plugins from cache
        for removed_path in changes["removed"]:
            plugin_id = self._path_to_id(removed_path)
            self.cache.delete(plugin_id)
        
        # Update cache with new/modified plugins
        if updated_plugins:
            self.cache.save(updated_plugins)
        
        if progress_reporter:
            progress_reporter.finish(f"Updated {len(updated_plugins)} plugins")
        
        return updated_plugins
```

## Phase 2: Modern CLI Revolution (Week 3)

### 2.1 Migrate to Click + Rich

**Problem**: Fire framework is outdated and provides poor user experience.

**Solution**: Modern CLI with Click framework and Rich formatting.

```python
# src/pedalboard_pluginary/cli.py
import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress
from rich.panel import Panel
from typing import Optional

console = Console()

@click.group()
@click.version_option()
@click.option('--config-file', type=click.Path(), help='Configuration file path')
@click.option('--verbose', '-v', count=True, help='Increase verbosity')
def cli(config_file: Optional[str], verbose: int):
    """Pedalboard Pluginary - Audio plugin scanner and manager.
    
    A fast, reliable tool for discovering and managing audio plugins
    on your system. Supports VST3 and Audio Unit formats.
    """
    setup_logging(verbose)
    if config_file:
        load_config(config_file)

@cli.command()
@click.option('--async/--sync', default=True, help='Use async scanning for better performance')
@click.option('--concurrency', default=10, help='Max concurrent plugin scans (async mode)')
@click.option('--timeout', default=10.0, help='Plugin load timeout in seconds')
@click.option('--folders', help='Additional folders to scan (comma-separated)')
@click.option('--force', is_flag=True, help='Force full rescan (ignore cache)')
def scan(async_: bool, concurrency: int, timeout: float, folders: Optional[str], force: bool):
    """Scan system for audio plugins.
    
    Discovers VST3 and Audio Unit plugins in standard locations
    and any additional folders specified. Results are cached for
    faster subsequent operations.
    """
    extra_folders = folders.split(',') if folders else []
    
    with console.status("Initializing scanner...") as status:
        scanner = PedalboardPluginary(
            async_mode=async_,
            max_concurrent=concurrency,
            timeout=timeout
        )
        
        if force:
            status.update("Clearing cache...")
            scanner.clear_cache()
        
        status.update("Scanning plugins...")
        
        # Use Rich progress bar
        with Progress() as progress:
            task = progress.add_task("Scanning...", total=None)
            
            def progress_callback(current: int, total: int, message: str = ""):
                progress.update(task, completed=current, total=total, description=message)
            
            plugins = scanner.scan(
                extra_folders=extra_folders,
                progress_callback=progress_callback
            )
    
    # Display results summary
    _display_scan_summary(plugins)

@cli.command()
@click.option('--format', type=click.Choice(['table', 'json', 'yaml', 'csv']), 
              default='table', help='Output format')
@click.option('--filter', 'filter_text', help='Filter plugins by name or manufacturer')
@click.option('--type', 'plugin_type', type=click.Choice(['vst3', 'au']), 
              help='Filter by plugin type')
@click.option('--manufacturer', help='Filter by manufacturer')
@click.option('--limit', default=50, help='Limit number of results')
def list(format: str, filter_text: Optional[str], plugin_type: Optional[str], 
         manufacturer: Optional[str], limit: int):
    """List discovered plugins with filtering options.
    
    Display plugins in various formats with powerful filtering
    capabilities. Use --filter for text search across names
    and manufacturers.
    """
    scanner = PedalboardPluginary()
    
    # Apply filters
    plugins = scanner.search_plugins(
        query=filter_text,
        plugin_type=plugin_type,
        manufacturer=manufacturer,
        limit=limit
    )
    
    _output_plugins(plugins, format)

@cli.command()
@click.argument('plugin_id')
@click.option('--parameters/--no-parameters', default=True, help='Show plugin parameters')
@click.option('--test', is_flag=True, help='Test plugin loading')
def info(plugin_id: str, parameters: bool, test: bool):
    """Show detailed information about a specific plugin.
    
    PLUGIN_ID should be the full plugin identifier,
    e.g., 'vst3/FabFilter Pro-Q 3'
    """
    scanner = PedalboardPluginary()
    plugin = scanner.get_plugin(plugin_id)
    
    if not plugin:
        console.print(f"[red]Plugin '{plugin_id}' not found[/red]")
        raise click.Abort()
    
    _display_plugin_info(plugin, parameters, test)

@cli.command()
@click.argument('query')
@click.option('--limit', default=20, help='Limit number of results')
def search(query: str, limit: int):
    """Search plugins by name, manufacturer, or type.
    
    Performs full-text search across plugin metadata.
    Supports wildcards and boolean operators.
    """
    scanner = PedalboardPluginary()
    plugins = scanner.search_plugins(query=query, limit=limit)
    
    if not plugins:
        console.print(f"[yellow]No plugins found matching '{query}'[/yellow]")
        return
    
    _output_plugins(plugins, 'table')

@cli.group()
def cache():
    """Cache management commands."""
    pass

@cache.command()
def clear():
    """Clear the plugin cache."""
    scanner = PedalboardPluginary()
    scanner.clear_cache()
    console.print("[green]Cache cleared successfully[/green]")

@cache.command()
def stats():
    """Show cache statistics."""
    scanner = PedalboardPluginary()
    stats = scanner.get_cache_stats()
    _display_cache_stats(stats)

@cache.command()
def repair():
    """Repair corrupted cache."""
    scanner = PedalboardPluginary()
    
    with console.status("Repairing cache..."):
        repaired = scanner.repair_cache()
    
    if repaired:
        console.print("[green]Cache repaired successfully[/green]")
    else:
        console.print("[yellow]No cache corruption detected[/yellow]")

def _display_scan_summary(plugins: Dict[str, PluginInfo]) -> None:
    """Display scan results summary with Rich formatting."""
    table = Table(title="Scan Results Summary")
    table.add_column("Plugin Type", style="cyan")
    table.add_column("Count", justify="right", style="magenta")
    
    type_counts = {}
    for plugin in plugins.values():
        type_counts[plugin.plugin_type] = type_counts.get(plugin.plugin_type, 0) + 1
    
    for plugin_type, count in sorted(type_counts.items()):
        table.add_row(plugin_type.upper(), str(count))
    
    table.add_row("", "")  # Separator
    table.add_row("Total", str(len(plugins)), style="bold green")
    
    console.print(table)

def _output_plugins(plugins: List[PluginInfo], format: str) -> None:
    """Output plugins in specified format."""
    if format == 'table':
        _output_table(plugins)
    elif format == 'json':
        _output_json(plugins)
    elif format == 'yaml':
        _output_yaml(plugins)
    elif format == 'csv':
        _output_csv(plugins)

def _output_table(plugins: List[PluginInfo]) -> None:
    """Display plugins in a rich table."""
    table = Table()
    table.add_column("Name", style="cyan", no_wrap=True)
    table.add_column("Type", style="magenta")
    table.add_column("Manufacturer", style="green")
    table.add_column("Path", style="blue", overflow="ellipsis")
    
    for plugin in plugins:
        table.add_row(
            plugin.name,
            plugin.plugin_type.upper(),
            plugin.manufacturer or "Unknown",
            plugin.path
        )
    
    console.print(table)

def _display_plugin_info(plugin: PluginInfo, show_parameters: bool, test_loading: bool) -> None:
    """Display detailed plugin information."""
    # Main info panel
    info_text = f"""
[bold]Name:[/bold] {plugin.name}
[bold]Type:[/bold] {plugin.plugin_type.upper()}
[bold]Manufacturer:[/bold] {plugin.manufacturer or 'Unknown'}
[bold]Path:[/bold] {plugin.path}
[bold]Parameters:[/bold] {len(plugin.parameters)}
    """
    
    panel = Panel(info_text.strip(), title=f"Plugin: {plugin.name}", border_style="blue")
    console.print(panel)
    
    # Parameters table
    if show_parameters and plugin.parameters:
        param_table = Table(title="Parameters")
        param_table.add_column("Parameter", style="cyan")
        param_table.add_column("Value", style="magenta")
        param_table.add_column("Type", style="green")
        
        for param in plugin.parameters.values():
            param_table.add_row(
                param.name,
                str(param.value),
                type(param.value).__name__
            )
        
        console.print(param_table)
    
    # Test loading
    if test_loading:
        with console.status("Testing plugin loading..."):
            try:
                # Test load the plugin
                scanner = PedalboardPluginary()
                success = scanner.test_plugin(plugin.id)
                
                if success:
                    console.print("[green]✓ Plugin loads successfully[/green]")
                else:
                    console.print("[red]✗ Plugin failed to load[/red]")
            except Exception as e:
                console.print(f"[red]✗ Error testing plugin: {e}[/red]")
```

**Implementation Steps**:
1. Replace Fire with Click for command structure
2. Add Rich for tables, progress bars, and formatting
3. Implement comprehensive help system
4. Add plugin search and filtering commands
5. Create cache management subcommands

**Expected Impact**:
- **User Experience**: Professional CLI with rich formatting
- **Discoverability**: Comprehensive help and examples
- **Functionality**: Search, filtering, and management features

### 2.2 Advanced Search and Filtering

**Problem**: No way to find specific plugins in large libraries.

**Solution**: Full-text search with multiple filter options.

```python
# src/pedalboard_pluginary/search.py
import re
from typing import List, Optional, Dict, Any, Callable
from .models import PluginInfo

class PluginSearchEngine:
    """Advanced search engine for plugin discovery."""
    
    def __init__(self, cache_backend: CacheBackend):
        self.cache = cache_backend
    
    def search(
        self,
        query: Optional[str] = None,
        plugin_type: Optional[str] = None,
        manufacturer: Optional[str] = None,
        has_parameters: Optional[List[str]] = None,
        parameter_count: Optional[tuple] = None,  # (min, max)
        sort_by: str = "name",
        sort_desc: bool = False,
        limit: int = 100,
        offset: int = 0
    ) -> List[PluginInfo]:
        """Comprehensive plugin search with multiple filters."""
        
        # Start with cache backend search for basic filters
        plugins = list(self.cache.search(
            query=query,
            plugin_type=plugin_type,
            manufacturer=manufacturer,
            limit=limit * 2,  # Get more for additional filtering
            offset=offset
        ))
        
        # Apply additional filters
        if has_parameters:
            plugins = self._filter_by_parameters(plugins, has_parameters)
        
        if parameter_count:
            min_params, max_params = parameter_count
            plugins = [p for p in plugins 
                      if min_params <= len(p.parameters) <= max_params]
        
        # Apply sorting
        plugins = self._sort_plugins(plugins, sort_by, sort_desc)
        
        # Apply final limit
        return plugins[:limit]
    
    def fuzzy_search(self, query: str, threshold: float = 0.6) -> List[PluginInfo]:
        """Fuzzy search using similarity matching."""
        import difflib
        
        all_plugins = list(self.cache.search(limit=10000))
        matches = []
        
        for plugin in all_plugins:
            # Check similarity against name and manufacturer
            name_similarity = difflib.SequenceMatcher(
                None, query.lower(), plugin.name.lower()
            ).ratio()
            
            manufacturer_similarity = 0
            if plugin.manufacturer:
                manufacturer_similarity = difflib.SequenceMatcher(
                    None, query.lower(), plugin.manufacturer.lower()
                ).ratio()
            
            max_similarity = max(name_similarity, manufacturer_similarity)
            
            if max_similarity >= threshold:
                matches.append((plugin, max_similarity))
        
        # Sort by similarity score
        matches.sort(key=lambda x: x[1], reverse=True)
        return [plugin for plugin, _ in matches]
    
    def suggest_similar(self, plugin_id: str, limit: int = 5) -> List[PluginInfo]:
        """Find plugins similar to the given plugin."""
        plugin = self.cache.get(plugin_id)
        if not plugin:
            return []
        
        # Find plugins with similar characteristics
        similar = []
        
        # Same manufacturer
        if plugin.manufacturer:
            manufacturer_plugins = list(self.cache.search(
                manufacturer=plugin.manufacturer,
                limit=50
            ))
            similar.extend([p for p in manufacturer_plugins if p.id != plugin_id])
        
        # Similar parameter count
        param_count = len(plugin.parameters)
        param_range_plugins = list(self.cache.search(limit=1000))
        param_similar = [
            p for p in param_range_plugins
            if abs(len(p.parameters) - param_count) <= 2 and p.id != plugin_id
        ]
        similar.extend(param_similar)
        
        # Remove duplicates and limit
        seen = set()
        unique_similar = []
        for p in similar:
            if p.id not in seen:
                seen.add(p.id)
                unique_similar.append(p)
                if len(unique_similar) >= limit:
                    break
        
        return unique_similar
    
    def _filter_by_parameters(
        self, 
        plugins: List[PluginInfo], 
        required_params: List[str]
    ) -> List[PluginInfo]:
        """Filter plugins that have specific parameters."""
        filtered = []
        
        for plugin in plugins:
            plugin_params = {p.name.lower() for p in plugin.parameters.values()}
            required_lower = {p.lower() for p in required_params}
            
            if required_lower.issubset(plugin_params):
                filtered.append(plugin)
        
        return filtered
    
    def _sort_plugins(
        self, 
        plugins: List[PluginInfo], 
        sort_by: str, 
        desc: bool
    ) -> List[PluginInfo]:
        """Sort plugins by specified criteria."""
        sort_functions = {
            "name": lambda p: p.name.lower(),
            "manufacturer": lambda p: (p.manufacturer or "").lower(),
            "type": lambda p: p.plugin_type,
            "parameters": lambda p: len(p.parameters),
            "path": lambda p: p.path
        }
        
        if sort_by not in sort_functions:
            sort_by = "name"
        
        return sorted(plugins, key=sort_functions[sort_by], reverse=desc)
```

## Phase 3: Production Hardening (Week 4)

### 3.1 Comprehensive Testing Strategy

**Problem**: Current test coverage is ~40% with gaps in critical areas.

**Solution**: Achieve 90%+ coverage with focus on reliability.

```python
# tests/integration/test_full_workflow.py
import pytest
import asyncio
from pathlib import Path
from unittest.mock import patch, MagicMock

from pedalboard_pluginary import PedalboardPluginary
from pedalboard_pluginary.models import PluginInfo

class TestFullWorkflow:
    """Integration tests for complete scanning workflows."""
    
    @pytest.fixture
    def test_plugins_dir(self, tmp_path):
        """Create directory with mock plugin files."""
        plugins_dir = tmp_path / "plugins"
        plugins_dir.mkdir()
        
        # Create mock VST3 files
        vst3_dir = plugins_dir / "vst3"
        vst3_dir.mkdir()
        (vst3_dir / "TestSynth.vst3").touch()
        (vst3_dir / "TestEffect.vst3").touch()
        
        # Create mock AU files
        au_dir = plugins_dir / "au"
        au_dir.mkdir()
        (au_dir / "TestAU.component").touch()
        
        return plugins_dir
    
    @pytest.mark.asyncio
    async def test_async_scanning_performance(self, test_plugins_dir, benchmark):
        """Test async scanning is faster than sync."""
        scanner = PedalboardPluginary(async_mode=True, max_concurrent=4)
        
        # Mock plugin loading to be fast but measurable
        with patch('pedalboard.load_plugin') as mock_load:
            mock_plugin = MagicMock()
            mock_plugin.name = "Test Plugin"
            mock_plugin.manufacturer = "Test Company"
            mock_plugin.parameters = {"gain": 0.5, "freq": 1000.0}
            mock_load.return_value = mock_plugin
            
            async def scan_async():
                return await scanner.scan_directory_async(test_plugins_dir)
            
            result = benchmark(asyncio.run, scan_async)
            assert len(result) > 0
    
    def test_cache_persistence(self, test_plugins_dir, tmp_path):
        """Test that cache persists between sessions."""
        cache_dir = tmp_path / "cache"
        scanner1 = PedalboardPluginary(cache_dir=cache_dir)
        
        with patch('pedalboard.load_plugin') as mock_load:
            mock_plugin = MagicMock()
            mock_plugin.name = "Persistent Plugin"
            mock_plugin.parameters = {}
            mock_load.return_value = mock_plugin
            
            # First scan
            plugins1 = scanner1.scan([test_plugins_dir])
            assert len(plugins1) > 0
        
        # Second scanner instance should load from cache
        scanner2 = PedalboardPluginary(cache_dir=cache_dir)
        plugins2 = scanner2.load_plugins()
        
        assert len(plugins2) == len(plugins1)
        assert list(plugins2.keys()) == list(plugins1.keys())
    
    def test_incremental_updates(self, test_plugins_dir):
        """Test incremental cache updates work correctly."""
        scanner = PedalboardPluginary()
        
        with patch('pedalboard.load_plugin') as mock_load:
            mock_plugin = MagicMock()
            mock_plugin.name = "Original Plugin"
            mock_load.return_value = mock_plugin
            
            # Initial scan
            plugins1 = scanner.scan([test_plugins_dir])
            initial_count = len(plugins1)
            
            # Add new plugin file
            new_plugin = test_plugins_dir / "vst3" / "NewPlugin.vst3"
            new_plugin.touch()
            
            # Update scan should detect new plugin
            plugins2 = scanner.update_scan([test_plugins_dir])
            assert len(plugins2) == initial_count + 1
    
    def test_error_recovery(self, test_plugins_dir):
        """Test graceful handling of plugin loading errors."""
        scanner = PedalboardPluginary()
        
        def mock_load_plugin(path):
            if "TestSynth" in str(path):
                raise Exception("Plugin corrupted")
            mock_plugin = MagicMock()
            mock_plugin.name = "Working Plugin"
            mock_plugin.parameters = {}
            return mock_plugin
        
        with patch('pedalboard.load_plugin', side_effect=mock_load_plugin):
            plugins = scanner.scan([test_plugins_dir])
            
            # Should have some plugins despite errors
            assert len(plugins) > 0
            
            # Error should be logged but not crash
            working_plugins = [p for p in plugins.values() 
                             if p.name == "Working Plugin"]
            assert len(working_plugins) > 0

# tests/performance/test_benchmarks.py
import pytest
import time
from pathlib import Path
from unittest.mock import patch, MagicMock

class TestPerformanceBenchmarks:
    """Performance regression tests."""
    
    def test_scan_performance_regression(self, benchmark):
        """Ensure scan performance doesn't regress."""
        # Create mock plugins that take realistic time to load
        def slow_load_plugin(path):
            time.sleep(0.01)  # 10ms per plugin
            mock_plugin = MagicMock()
            mock_plugin.name = f"Plugin {Path(path).stem}"
            mock_plugin.parameters = {"param1": 0.5}
            return mock_plugin
        
        scanner = PedalboardPluginary()
        mock_paths = [Path(f"/fake/plugin{i}.vst3") for i in range(10)]
        
        with patch('pedalboard.load_plugin', side_effect=slow_load_plugin), \
             patch.object(scanner, 'find_plugin_files', return_value=mock_paths):
            
            result = benchmark(scanner.scan, [])
            assert len(result) == 10
            
            # Should complete 10 plugins in under 1 second with async
            # (much faster than 10 * 0.01 = 0.1s sequential minimum)
    
    def test_memory_usage_scaling(self):
        """Test memory usage scales properly with plugin count."""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        
        # Measure baseline memory
        baseline_memory = process.memory_info().rss
        
        # Create scanner with large mock dataset
        scanner = PedalboardPluginary()
        
        # Create 1000 mock plugins
        large_dataset = {}
        for i in range(1000):
            plugin = PluginInfo(
                id=f"plugin_{i}",
                name=f"Plugin {i}",
                path=f"/fake/plugin{i}.vst3",
                filename=f"plugin{i}.vst3",
                plugin_type="vst3",
                parameters={}
            )
            large_dataset[plugin.id] = plugin
        
        scanner.plugins = large_dataset
        
        # Memory should not grow excessively
        current_memory = process.memory_info().rss
        memory_growth = current_memory - baseline_memory
        
        # Should use less than 50MB for 1000 plugins
        assert memory_growth < 50 * 1024 * 1024

# tests/reliability/test_error_scenarios.py
class TestErrorScenarios:
    """Test error handling and recovery scenarios."""
    
    def test_corrupted_cache_recovery(self, tmp_path):
        """Test recovery from corrupted cache file."""
        cache_file = tmp_path / "plugins.json"
        cache_file.write_text("invalid json content")
        
        scanner = PedalboardPluginary(cache_dir=tmp_path)
        
        # Should handle corrupted cache gracefully
        plugins = scanner.load_plugins()
        assert isinstance(plugins, dict)
        assert len(plugins) == 0
    
    def test_permission_denied_handling(self, tmp_path):
        """Test handling of permission denied errors."""
        restricted_dir = tmp_path / "restricted"
        restricted_dir.mkdir(mode=0o000)  # No permissions
        
        scanner = PedalboardPluginary()
        
        try:
            # Should not crash on permission errors
            plugins = scanner.scan([restricted_dir])
            assert isinstance(plugins, dict)
        finally:
            restricted_dir.chmod(0o755)  # Restore permissions for cleanup
    
    def test_network_drive_timeouts(self):
        """Test handling of slow network drives."""
        # Mock slow file system operations
        def slow_stat():
            time.sleep(2)  # Simulate slow network
            raise OSError("Network timeout")
        
        scanner = PedalboardPluginary(timeout=1.0)
        
        with patch.object(Path, 'stat', side_effect=slow_stat):
            # Should timeout gracefully
            plugins = scanner.scan([Path("/fake/network/path")])
            assert isinstance(plugins, dict)
```

**Implementation Steps**:
1. Create comprehensive integration test suite
2. Add performance benchmarks and regression tests
3. Test error scenarios and edge cases
4. Add cross-platform compatibility tests
5. Implement continuous integration with multiple Python versions

**Expected Impact**:
- **Reliability**: Catch regressions before deployment
- **Performance**: Prevent performance degradation
- **Confidence**: Thorough testing enables rapid development

### 3.2 Error Recovery and Resilience

**Problem**: Limited error recovery capabilities.

**Solution**: Comprehensive error handling with automatic recovery.

```python
# src/pedalboard_pluginary/resilience.py
import logging
import time
from typing import Dict, List, Optional, Callable, Any
from pathlib import Path

class ResilienceManager:
    """Manages error recovery and system resilience."""
    
    def __init__(self, config: 'Settings'):
        self.config = config
        self.error_counts: Dict[str, int] = {}
        self.blacklisted_plugins: Set[str] = set()
        self.logger = logging.getLogger(__name__)
    
    def safe_plugin_scan(
        self, 
        scanner: PluginScanner, 
        plugin_path: Path,
        max_retries: int = 3
    ) -> Optional[PluginInfo]:
        """Safely scan a plugin with error recovery."""
        plugin_key = str(plugin_path)
        
        # Check if plugin is blacklisted
        if plugin_key in self.blacklisted_plugins:
            self.logger.debug(f"Skipping blacklisted plugin: {plugin_path}")
            return None
        
        for attempt in range(max_retries):
            try:
                return scanner.scan_plugin(plugin_path)
                
            except TimeoutError as e:
                self.logger.warning(f"Plugin {plugin_path} timed out (attempt {attempt + 1})")
                self._record_error(plugin_key, "timeout")
                
                if attempt == max_retries - 1:
                    self._maybe_blacklist_plugin(plugin_key)
                    
            except PluginLoadError as e:
                self.logger.error(f"Plugin {plugin_path} failed to load: {e}")
                self._record_error(plugin_key, "load_error")
                
                # Don't retry load errors - they're likely permanent
                self._maybe_blacklist_plugin(plugin_key)
                break
                
            except Exception as e:
                self.logger.error(f"Unexpected error scanning {plugin_path}: {e}")
                self._record_error(plugin_key, "unexpected")
                
                if attempt == max_retries - 1:
                    self._maybe_blacklist_plugin(plugin_key)
        
        return None
    
    def safe_cache_operation(
        self, 
        operation: Callable[[], Any], 
        operation_name: str,
        fallback: Optional[Callable[[], Any]] = None
    ) -> Any:
        """Safely perform cache operation with fallback."""
        try:
            return operation()
            
        except (IOError, OSError) as e:
            self.logger.error(f"Cache {operation_name} failed: {e}")
            
            if fallback:
                self.logger.info(f"Attempting {operation_name} fallback")
                try:
                    return fallback()
                except Exception as fallback_error:
                    self.logger.error(f"Fallback also failed: {fallback_error}")
            
            # Return safe default
            return {} if "load" in operation_name else None
            
        except Exception as e:
            self.logger.error(f"Unexpected error in cache {operation_name}: {e}")
            return {} if "load" in operation_name else None
    
    def repair_cache(self, cache_backend: CacheBackend) -> bool:
        """Attempt to repair corrupted cache."""
        try:
            # Test basic cache operations
            cache_backend.exists()
            test_plugins = list(cache_backend.search(limit=1))
            
            self.logger.info("Cache appears healthy")
            return False
            
        except Exception as e:
            self.logger.warning(f"Cache corruption detected: {e}")
            
            try:
                # Attempt repair by clearing and rebuilding
                self.logger.info("Attempting cache repair...")
                cache_backend.clear()
                
                # Cache will be rebuilt on next scan
                self.logger.info("Cache cleared for rebuild")
                return True
                
            except Exception as repair_error:
                self.logger.error(f"Cache repair failed: {repair_error}")
                return False
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get system health status."""
        total_errors = sum(self.error_counts.values())
        
        return {
            "total_errors": total_errors,
            "blacklisted_plugins": len(self.blacklisted_plugins),
            "error_breakdown": dict(self.error_counts),
            "health_score": self._calculate_health_score()
        }
    
    def _record_error(self, plugin_key: str, error_type: str) -> None:
        """Record error for tracking."""
        error_key = f"{plugin_key}:{error_type}"
        self.error_counts[error_key] = self.error_counts.get(error_key, 0) + 1
    
    def _maybe_blacklist_plugin(self, plugin_key: str) -> None:
        """Blacklist plugin if it has too many errors."""
        plugin_errors = sum(
            count for key, count in self.error_counts.items()
            if key.startswith(plugin_key)
        )
        
        if plugin_errors >= self.config.max_plugin_errors:
            self.blacklisted_plugins.add(plugin_key)
            self.logger.warning(f"Blacklisted problematic plugin: {plugin_key}")
    
    def _calculate_health_score(self) -> float:
        """Calculate system health score (0-100)."""
        total_errors = sum(self.error_counts.values())
        blacklisted_count = len(self.blacklisted_plugins)
        
        # Simple scoring algorithm
        error_penalty = min(total_errors * 2, 50)  # Max 50 point penalty
        blacklist_penalty = min(blacklisted_count * 5, 30)  # Max 30 point penalty
        
        return max(0, 100 - error_penalty - blacklist_penalty)
```

## Phase 4: Advanced Features (Week 5-6)

### 4.1 Configuration Management System

**Problem**: No user configuration system.

**Solution**: Pydantic-based settings with environment variable support.

```python
# src/pedalboard_pluginary/config.py
from pydantic import BaseSettings, validator
from typing import List, Optional
from pathlib import Path

class Settings(BaseSettings):
    """Application configuration with environment variable support."""
    
    # Performance settings
    async_mode: bool = True
    max_concurrent_scans: int = 10
    plugin_load_timeout: float = 10.0
    
    # Cache settings
    cache_backend: str = "sqlite"  # "json" or "sqlite"
    cache_directory: Optional[Path] = None
    max_cache_size_mb: int = 100
    
    # Scanning settings
    scan_default_locations: bool = True
    extra_scan_paths: List[str] = []
    ignore_patterns: List[str] = []
    
    # Error handling
    max_plugin_errors: int = 3
    enable_error_recovery: bool = True
    
    # Output settings
    default_output_format: str = "table"
    enable_colors: bool = True
    
    class Config:
        env_prefix = "PLUGINARY_"
        env_file = ".env"
        case_sensitive = False
    
    @validator('cache_directory')
    def validate_cache_directory(cls, v):
        if v is None:
            return Path.home() / ".cache" / "pedalboard-pluginary"
        return Path(v)
    
    @validator('max_concurrent_scans')
    def validate_concurrency(cls, v):
        if v < 1:
            raise ValueError("max_concurrent_scans must be at least 1")
        if v > 50:
            raise ValueError("max_concurrent_scans should not exceed 50")
        return v
```

### 4.2 Plugin Categorization System

**Problem**: No way to organize plugins by function.

**Solution**: Intelligent categorization based on names and parameters.

```python
# src/pedalboard_pluginary/categorizer.py
from enum import Enum
from typing import List, Set, Dict
import re

class PluginCategory(Enum):
    EQUALIZER = "equalizer"
    COMPRESSOR = "compressor"
    REVERB = "reverb"
    DELAY = "delay"
    MODULATION = "modulation"
    DISTORTION = "distortion"
    SYNTHESIZER = "synthesizer"
    UTILITY = "utility"
    UNKNOWN = "unknown"

class PluginCategorizer:
    """Automatically categorize plugins based on name and parameters."""
    
    CATEGORY_RULES = {
        PluginCategory.EQUALIZER: {
            "name_keywords": ["eq", "equalizer", "filter", "shelf", "bell"],
            "parameter_keywords": ["frequency", "freq", "gain", "q", "bandwidth"],
            "parameter_patterns": [r".*freq.*", r".*hz.*", r".*khz.*"]
        },
        PluginCategory.COMPRESSOR: {
            "name_keywords": ["comp", "compressor", "limiter", "gate"],
            "parameter_keywords": ["threshold", "ratio", "attack", "release", "knee"],
            "parameter_patterns": [r".*thresh.*", r".*ratio.*"]
        },
        PluginCategory.REVERB: {
            "name_keywords": ["reverb", "verb", "hall", "room", "plate", "spring"],
            "parameter_keywords": ["decay", "size", "damping", "predelay"],
            "parameter_patterns": [r".*room.*", r".*hall.*", r".*decay.*"]
        },
        PluginCategory.DELAY: {
            "name_keywords": ["delay", "echo", "ping", "pong"],
            "parameter_keywords": ["delay", "feedback", "time", "sync"],
            "parameter_patterns": [r".*delay.*", r".*time.*", r".*sync.*"]
        },
        PluginCategory.MODULATION: {
            "name_keywords": ["chorus", "flanger", "phaser", "tremolo", "vibrato", "mod"],
            "parameter_keywords": ["rate", "depth", "lfo", "speed"],
            "parameter_patterns": [r".*rate.*", r".*depth.*", r".*lfo.*"]
        },
        PluginCategory.DISTORTION: {
            "name_keywords": ["dist", "overdrive", "fuzz", "saturator", "drive"],
            "parameter_keywords": ["drive", "gain", "distortion", "saturation"],
            "parameter_patterns": [r".*drive.*", r".*dist.*", r".*sat.*"]
        },
        PluginCategory.SYNTHESIZER: {
            "name_keywords": ["synth", "osc", "generator", "bass", "lead", "pad"],
            "parameter_keywords": ["oscillator", "envelope", "filter", "cutoff"],
            "parameter_patterns": [r".*osc.*", r".*env.*", r".*adsr.*"]
        }
    }
    
    def categorize(self, plugin: PluginInfo) -> List[PluginCategory]:
        """Categorize a plugin based on its characteristics."""
        categories = []
        
        for category, rules in self.CATEGORY_RULES.items():
            if self._matches_category(plugin, rules):
                categories.append(category)
        
        return categories if categories else [PluginCategory.UNKNOWN]
    
    def _matches_category(self, plugin: PluginInfo, rules: Dict) -> bool:
        """Check if plugin matches category rules."""
        score = 0
        
        # Check name keywords
        name_lower = plugin.name.lower()
        name_matches = sum(1 for keyword in rules["name_keywords"] 
                          if keyword in name_lower)
        score += name_matches * 3  # Name matches are weighted highly
        
        # Check parameter keywords and patterns
        param_names = [p.name.lower() for p in plugin.parameters.values()]
        
        param_keyword_matches = 0
        for param_name in param_names:
            for keyword in rules["parameter_keywords"]:
                if keyword in param_name:
                    param_keyword_matches += 1
        
        param_pattern_matches = 0
        for param_name in param_names:
            for pattern in rules["parameter_patterns"]:
                if re.search(pattern, param_name):
                    param_pattern_matches += 1
        
        score += param_keyword_matches * 2
        score += param_pattern_matches * 1
        
        # Threshold for categorization
        return score >= 2
```

## Implementation Priorities

### Immediate Next Steps (Week 1)
1. **Async Scanner Implementation** - Highest impact performance improvement
2. **SQLite Cache Backend** - Essential for scalability
3. **Click CLI Migration** - Better user experience

### Quality Gates
- **Phase 1**: Async scanning 5x faster than sync, zero mypy errors
- **Phase 2**: Modern CLI with rich output, search functionality working  
- **Phase 3**: 90%+ test coverage, comprehensive error handling
- **Phase 4**: Production-ready with advanced features

### Success Metrics
- **Performance**: 10-20 plugins/second scan speed
- **Scalability**: Handle 10,000+ plugins efficiently
- **Reliability**: Graceful error handling and recovery
- **Usability**: Intuitive CLI with comprehensive help

The codebase has excellent foundations. These improvements will transform it into a production-ready, high-performance tool that can handle large plugin libraries efficiently while providing an excellent user experience.
</file>

<file path="src/pedalboard_pluginary/core.py">
import json
from pathlib import Path
from typing import Dict

from .data import get_cache_path
from .models import PluginInfo
from .scanner import PedalboardScanner
from .serialization import PluginSerializer


class PedalboardPluginary:
    """Main class for the Pedalboard Pluginary application."""
    
    plugins_path: Path
    plugins: Dict[str, PluginInfo]

    def __init__(self) -> None:
        """Initialize the Pedalboard Pluginary instance."""
        self.plugins_path = get_cache_path("plugins")
        self.plugins = {}
        self.load_data()

    def load_data(self) -> None:
        """Load plugin data from cache or perform a scan if cache doesn't exist."""
        if not self.plugins_path.exists():
            scanner = PedalboardScanner()
            scanner.full_scan()

        # Load plugins using the serializer
        self.plugins = PluginSerializer.load_plugins(self.plugins_path)

    def list_plugins(self) -> str:
        """Returns a JSON string representation of the plugins."""
        # Convert PluginInfo objects to dictionaries for JSON serialization
        plugins_dict = {}
        for plugin_id, plugin in self.plugins.items():
            plugins_dict[plugin_id] = PluginSerializer.plugin_to_dict(plugin)
        
        return json.dumps(plugins_dict, indent=4)
</file>

<file path="src/pedalboard_pluginary/data.py">
import json
import os
import platform
import shutil
from importlib import resources
from pathlib import Path
from typing import Any, Dict, List, Set, Union

# JSON-serializable types
JSONValue = Union[str, int, float, bool, None, Dict[str, Any], List[Any]]
JSONDict = Dict[str, JSONValue]

from .utils import ensure_folder

APP_NAME: str = "com.twardoch.pedalboard-pluginary"
PLUGINS_CACHE_FILENAME_BASE: str = "plugins"  # To identify the plugins cache file


def get_cache_path(cache_name: str) -> Path:
    """Get the path to a cache file."""
    os_name = platform.system()
    if os_name == "Windows":
        app_data_env = os.getenv("APPDATA")
        if app_data_env is None:
            app_data_dir = (
                Path(os.path.expanduser("~")) / "AppData" / "Roaming" / APP_NAME
            )
        else:
            app_data_dir = Path(app_data_env) / APP_NAME
    elif os_name == "Darwin":  # macOS
        app_data_dir = Path.home() / "Library" / "Application Support" / APP_NAME
    else:  # Linux and other Unix-like systems
        xdg_cache_home_env = os.getenv("XDG_CACHE_HOME")
        if xdg_cache_home_env:
            app_data_dir = Path(xdg_cache_home_env) / APP_NAME
        else:
            app_data_dir = Path.home() / ".cache" / APP_NAME

    app_data_dir.mkdir(parents=True, exist_ok=True)  # Ensure base app dir exists
    return app_data_dir / f"{cache_name}.json"


def load_json_file(file_path: Path) -> Any:
    """Load JSON data from a file."""
    if not file_path.exists():
        return {}

    with open(file_path, "r", encoding="utf-8") as file:
        try:
            raw_data = json.load(file)
        except json.JSONDecodeError:
            return {}  # Return empty dict if JSON is corrupted

    return raw_data


def save_json_file(data: Any, file_path: Path) -> None:
    """Save JSON data to a file."""
    ensure_folder(file_path.parent)
    with open(file_path, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=4)


def load_ignores(ignores_path: Path) -> Set[str]:
    """Load ignores data (list of strings) from the file."""
    content = load_json_file(ignores_path)
    if isinstance(content, list):  # Expects a list of strings
        return set(item for item in content if isinstance(item, str))
    return set()


def save_ignores(ignores: Set[str], ignores_path: Path) -> None:
    """Save ignores data to the file."""
    save_json_file(sorted(list(ignores)), ignores_path)


def copy_default_ignores(destination_path: Path) -> None:
    """Copy the default ignores file to the destination if it does not exist."""
    try:
        import importlib.resources

        default_ignores_src_path = importlib.resources.files(
            "pedalboard_pluginary.resources"
        ).joinpath("default_ignores.json")

        if not destination_path.exists():
            ensure_folder(destination_path.parent)
            with importlib.resources.as_file(
                default_ignores_src_path
            ) as src_file_on_fs:
                if src_file_on_fs.exists():
                    shutil.copy(src_file_on_fs, destination_path)
                else:
                    save_json_file([], destination_path)
    except (ImportError, FileNotFoundError, TypeError) as e:
        print(
            f"Warning: Could not copy default ignores using importlib.resources: {e}. Creating empty ignores file."
        )
        if not destination_path.exists():
            ensure_folder(destination_path.parent)
            save_json_file([], destination_path)
</file>

<file path="pyproject.toml">
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[tool.setuptools_scm]
version_scheme = "no-guess-dev"

[project]
name = "pedalboard-pluginary"
version = "0.1.0"
description = "A plugin scanner for Pedalboard"
readme = "README.md"
requires-python = ">=3.9"
license = { text = "Apache-2.0" }
authors = [
    { name = "Adam Twardoch", email = "adam@twardoch.com" }
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Multimedia :: Sound/Audio",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
dependencies = [
    "pedalboard>=0.8.7",
    "fire>=0.5.0",
    "python-benedict>=0.33.0",
    "pyyaml>=6.0.1",
    "typing-extensions>=4.0.0; python_version < '3.11'",
    "tqdm>=4.60.0",
]

[project.urls]
Documentation = "https://github.com/twardoch/pedalboard-pluginary#readme"
Source = "https://github.com/twardoch/pedalboard-pluginary"
Tracker = "https://github.com/twardoch/pedalboard-pluginary/issues"

[project.optional-dependencies]
dev = [
    "pytest>=7.4.4",
    "pytest-cov>=4.1.0",
    "mypy>=1.8.0",
    "flake8>=7.0.0",
    "black>=24.1.1",
    "isort>=5.13.2",
]

[project.scripts]
pbpluginary = "pedalboard_pluginary.__main__:main"

[tool.setuptools]
packages = ["pedalboard_pluginary"]
package-dir = {"" = "src"}

[tool.pytest.ini_options]
addopts = "--cov=pedalboard_pluginary --cov-report=term-missing"
testpaths = ["tests"]

[tool.flake8]
max_line_length = 88
extend_ignore = "E203,W503"
exclude = [
    ".tox",
    "build",
    "dist",
    ".eggs",
    "docs/conf.py",
]

[tool.mypy]
python_version = "3.9"
mypy_path = "src"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["fire", "benedict"]
ignore_missing_imports = true

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3
</file>

<file path="src/pedalboard_pluginary/scanners/vst3_scanner.py">
"""
Handles scanning of VST3 plugins.
"""

import logging
import os
import platform
from pathlib import Path
from typing import Dict, List, Optional

import pedalboard

from ..async_scanner import AsyncScannerMixin
from ..base_scanner import BaseScanner
from ..constants import PLUGIN_TYPE_VST3, VST3_EXTENSION, PLUGIN_LOAD_TIMEOUT
from ..exceptions import PluginLoadError, PluginScanError
from ..models import PluginInfo, PluginParameter
from ..timeout import sync_timeout, TimeoutError
from ..utils import from_pb_param

logger = logging.getLogger(__name__)


class VST3Scanner(BaseScanner, AsyncScannerMixin):
    """Scanner for VST3 plugins."""
    
    @property
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles."""
        return PLUGIN_TYPE_VST3
    
    @property
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        return [VST3_EXTENSION]
    
    def _get_default_vst3_folders(self) -> List[Path]:
        """Get standard VST3 plugin folders for the current OS."""
        os_name = platform.system()
        folders: List[Path] = []
        
        if os_name == "Windows":
            program_files = os.getenv("ProgramFiles", "C:\\Program Files")
            program_files_x86 = os.getenv("ProgramFiles(x86)", "C:\\Program Files (x86)")
            folders = [
                Path(program_files) / "Common Files" / "VST3",
                Path(program_files_x86) / "Common Files" / "VST3",
            ]
        elif os_name == "Darwin":  # macOS
            folders = [
                Path("~/Library/Audio/Plug-Ins/VST3").expanduser(),
                Path("/Library/Audio/Plug-Ins/VST3"),
            ]
        elif os_name == "Linux":
            folders = [
                Path("~/.vst3").expanduser(),
                Path("/usr/lib/vst3"),
                Path("/usr/local/lib/vst3"),
            ]
        
        return [f for f in folders if f.exists()]
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all VST3 plugin files in standard and custom folders.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to VST3 plugin files found.
        """
        if paths:
            # Filter specific paths to only VST3 files
            vst3_paths = [p for p in paths if p.suffix in self.supported_extensions]
            return self._filter_plugin_paths(vst3_paths)
        
        # Search default VST3 folders
        search_folders = self._get_default_vst3_folders()
        
        if not search_folders:
            logger.warning("No VST3 folders to search.")
            return []
        
        logger.info(f"Searching for VST3 plugins in: {search_folders}")
        
        # Find all VST3 files
        discovered_plugins = set()
        for folder in search_folders:
            try:
                for vst3_file in folder.glob("*.vst3"):
                    if vst3_file.is_file():
                        discovered_plugins.add(vst3_file.resolve())
            except Exception as e:
                logger.error(f"Error searching folder {folder}: {e}")
        
        # Apply filtering
        plugin_list = sorted(list(discovered_plugins))
        filtered_list = self._filter_plugin_paths(plugin_list)
        
        logger.info(f"Found {len(filtered_list)} VST3 plugins after filtering.")
        return filtered_list
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a VST3 plugin and return its information.
        
        Args:
            path: Path to the VST3 plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        if not self.validate_plugin_path(path):
            logger.warning(f"Invalid plugin path: {path}")
            return None
        
        try:
            # Load the plugin to get its parameters with timeout
            logger.debug(f"Loading VST3 plugin: {path}")
            plugin = sync_timeout(pedalboard.load_plugin, PLUGIN_LOAD_TIMEOUT, str(path))
            
            # Extract parameters
            params: Dict[str, PluginParameter] = {}
            if hasattr(plugin, 'parameters'):
                for param_name, param_value in plugin.parameters.items():
                    # Convert the parameter value to our expected type
                    converted_value = from_pb_param(param_value)
                    params[param_name] = PluginParameter(
                        name=param_name,
                        value=converted_value,
                    )
            
            # Try to get manufacturer info if available
            manufacturer = None
            if hasattr(plugin, 'manufacturer'):
                manufacturer = str(plugin.manufacturer)
            
            # Get the plugin's display name if available
            display_name = path.stem
            if hasattr(plugin, 'name'):
                display_name = str(plugin.name)
            
            plugin_info = PluginInfo(
                id=self._create_plugin_id(path),
                name=display_name,
                path=str(path),
                filename=path.name,
                plugin_type=self.plugin_type,
                parameters=params,
                manufacturer=manufacturer,
            )
            
            logger.info(f"Successfully scanned VST3 plugin: {display_name}")
            return plugin_info
            
        except TimeoutError as e:
            logger.warning(f"VST3 plugin {path} timed out during loading: {e}")
            raise PluginLoadError(
                plugin_path=str(path),
                reason=f"Plugin loading timed out after {e.timeout}s"
            )
        except PluginLoadError:
            # Re-raise our custom exceptions
            raise
        except Exception as e:
            logger.error(f"Error scanning VST3 plugin {path}: {e}")
            raise PluginScanError(
                plugin_path=str(path),
                scanner_type=self.plugin_type,
                reason=str(e)
            )
</file>

<file path="src/pedalboard_pluginary/__main__.py">
#!/usr/bin/env python3
# benedict might also lack stubs.
import json
import logging  # For basicConfig
import sys  # For sys.stdout in Display lambda
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import fire
import yaml

# fire library might not have complete type stubs, common to ignore if problematic for mypy.
# Consider adding types-fire if available and it resolves issues.
from benedict import benedict as bdict

from .core import PedalboardPluginary
from .data import (
    PLUGINS_CACHE_FILENAME_BASE,
    get_cache_path,
    load_json_file,
    save_json_file,
)
from .models import PluginInfo
from .types import SerializedPlugin
from .scanner import PedalboardScanner

# Define a more specific type for extra_folders if it's always List[str] after split
ExtraFoldersType = Optional[List[str]]


def setup_logging(verbose_level: int = 0) -> None:
    """Configures basic logging for CLI output."""
    # verbose_level: 0 = WARNING, 1 = INFO, 2 = DEBUG
    log_level = logging.WARNING
    if verbose_level == 1:
        log_level = logging.INFO
    elif verbose_level >= 2:
        log_level = logging.DEBUG

    # Only configure if no handlers are already set (e.g., by tests or other imports)
    # This basicConfig will go to stderr by default for WARNING and above.
    # For INFO, let's direct to stdout for better CLI experience.
    if not logging.getLogger().hasHandlers():
        if log_level <= logging.INFO:
            # For INFO and DEBUG, use a more verbose format and stdout
            logging.basicConfig(
                stream=sys.stdout,
                level=log_level,
                format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            )
        else:
            # For WARNING, ERROR, CRITICAL, use stderr and simpler format
            logging.basicConfig(
                level=log_level, format="%(levelname)s: %(name)s: %(message)s"
            )


def scan_plugins_cli(extra_folders: Optional[str] = None, verbose: int = 0) -> None:
    """Scans all plugins, optionally including extra folders (comma-separated string)."""
    folders_list: List[str] = extra_folders.split(",") if extra_folders else []
    scanner = PedalboardScanner(specific_paths=folders_list)
    scanner.full_scan()  # This updates scanner.plugins
    if scanner.plugins:  # Only save if we found plugins
        cache_file = get_cache_path(PLUGINS_CACHE_FILENAME_BASE)
        save_json_file(scanner.plugins, cache_file)


def update_plugins_cli(extra_folders: Optional[str] = None, verbose: int = 0) -> None:
    """Updates the plugin cache, optionally including extra folders (comma-separated string)."""
    scan_plugins_cli(extra_folders, verbose)


def list_json_cli() -> Dict[str, SerializedPlugin]:
    """Lists all plugins in JSON format."""
    cache_file = get_cache_path(PLUGINS_CACHE_FILENAME_BASE)
    if not cache_file.exists():
        return {}
    data = load_json_file(cache_file)
    return data if isinstance(data, dict) else {}


def list_yaml_cli() -> str:
    """Lists all plugins in YAML format."""
    plugins = list_json_cli()
    return yaml.dump(plugins, sort_keys=False, indent=2)


def main() -> None:
    """Main entry point for the CLI."""
    fire.Fire({
        "scan": scan_plugins_cli,
        "list": list_json_cli,
        "json": list_json_cli,
        "yaml": list_yaml_cli,
        "update": update_plugins_cli,
    })


if __name__ == "__main__":
    main()
</file>

<file path="src/pedalboard_pluginary/scanner.py">
import asyncio
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, cast

from .scanners.au_scanner import AUScanner
from .scanners.vst3_scanner import VST3Scanner
from .async_scanner import AsyncScannerMixin

import pedalboard

from .constants import DEFAULT_MAX_CONCURRENT
from .data import (
    copy_default_ignores,
    get_cache_path,
    load_ignores,
    save_json_file,
)
from .exceptions import CacheCorruptedError, PluginScanError
from .models import PluginInfo, PluginParameter
from .progress import TqdmProgress
from .protocols import ProgressReporter
from .serialization import PluginSerializer
from .utils import ensure_folder, from_pb_param

logger: logging.Logger = logging.getLogger(__name__)


class PedalboardScanner:
    """Main scanner class that coordinates scanning of all plugin types."""

    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
        progress_reporter: Optional[ProgressReporter] = None,
        async_mode: bool = False,
        max_concurrent: int = DEFAULT_MAX_CONCURRENT,
    ):
        """Initialize the scanner with optional ignore paths and specific paths.
        
        Args:
            ignore_paths: List of regex patterns for paths to ignore.
            specific_paths: List of specific paths to scan.
            progress_reporter: Optional progress reporter instance.
            async_mode: Whether to use async scanning for better performance.
            max_concurrent: Maximum number of concurrent scans (async mode only).
        """
        self.ignore_paths = ignore_paths or []
        self.specific_paths = specific_paths or []
        self.plugins: Dict[str, PluginInfo] = {}
        self.plugins_path = get_cache_path("plugins")
        self.ignores_path = get_cache_path("ignores")
        self.progress_reporter = progress_reporter or TqdmProgress()
        self.async_mode = async_mode
        self.max_concurrent = max_concurrent
        
        # Initialize ignores
        copy_default_ignores(self.ignores_path)
        self.ignores = load_ignores(self.ignores_path)
        
        # Initialize scanners
        self.scanners = [
            AUScanner(
                ignore_paths=self.ignore_paths, specific_paths=self.specific_paths
            ),
            VST3Scanner(
                ignore_paths=self.ignore_paths, specific_paths=self.specific_paths
            ),
        ]
        
        # Load existing plugin data if available
        self.load_data()

    def load_data(self) -> None:
        """Load existing plugin data from cache."""
        try:
            self.plugins = PluginSerializer.load_plugins(self.plugins_path)
        except CacheCorruptedError as e:
            logger.warning(f"Cache corrupted, will perform full scan: {e}")
            self.plugins = {}

    def save_data(self) -> None:
        """Save plugin data to cache."""
        PluginSerializer.save_plugins(self.plugins, self.plugins_path)
        
        # Save updated ignores
        save_json_file(list(self.ignores), self.ignores_path)

    def full_scan(self) -> Dict[str, PluginInfo]:
        """Perform a full scan of all plugin types."""
        self.plugins = {}
        total_files = 0
        
        # First, count all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend([(scanner, pf) for pf in plugin_files])
            total_files += len(plugin_files)
        
        # Scan all plugins with progress reporting
        self.progress_reporter.start(total_files, "Scanning plugins")
        
        for scanner, plugin_file in all_plugin_files:
            plugin_key = f"{scanner.__class__.__name__.replace('Scanner', '').lower()}:{plugin_file}"
            
            # Skip ignored plugins
            if plugin_key in self.ignores:
                logger.info(f"Skipping ignored plugin: {plugin_file}")
                self.progress_reporter.update(1, f"Skipped: {plugin_file.name}")
                continue
            
            try:
                plugin_info = scanner.scan_plugin(plugin_file)
                if plugin_info:
                    self.plugins[plugin_info.id] = plugin_info
                    logger.info(f"Scanned plugin: {plugin_file}")
                    self.progress_reporter.update(1, f"Scanned: {plugin_info.name}")
                else:
                    self.progress_reporter.update(1)
            except PluginScanError as e:
                logger.error(f"Failed to scan plugin {plugin_file}: {e}")
                self.ignores.add(plugin_key)
                self.progress_reporter.update(1, f"Failed: {plugin_file.name}")
            except Exception as e:
                logger.error(f"Unexpected error scanning {plugin_file}: {e}")
                self.ignores.add(plugin_key)
                self.progress_reporter.update(1, f"Error: {plugin_file.name}")
        
        self.progress_reporter.finish(f"Scanned {len(self.plugins)} plugins")
        
        # Save the results
        self.save_data()
        return self.plugins

    def update_scan(self) -> Dict[str, PluginInfo]:
        """Update the scan with new plugins while preserving existing data."""
        # Keep track of existing plugins
        existing_plugins = set(self.plugins.keys())
        new_plugins = {}
        
        # Find all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend([(scanner, pf) for pf in plugin_files])
        
        # Only scan plugins that aren't already in the cache
        plugins_to_scan = []
        for scanner, plugin_file in all_plugin_files:
            plugin_type = scanner.__class__.__name__.replace('Scanner', '').lower()
            plugin_key = f"{plugin_type}:{plugin_file}"
            
            if plugin_key not in existing_plugins and plugin_key not in self.ignores:
                plugins_to_scan.append((scanner, plugin_file, plugin_key))
        
        # Scan new plugins with progress reporting
        if plugins_to_scan:
            self.progress_reporter.start(len(plugins_to_scan), "Scanning new plugins")
            
            for scanner, plugin_file, plugin_key in plugins_to_scan:
                try:
                    plugin_info = scanner.scan_plugin(plugin_file)
                    if plugin_info:
                        self.plugins[plugin_info.id] = plugin_info
                        new_plugins[plugin_info.id] = plugin_info
                        logger.info(f"Scanned new plugin: {plugin_file}")
                        self.progress_reporter.update(1, f"Scanned: {plugin_info.name}")
                    else:
                        self.progress_reporter.update(1)
                except PluginScanError as e:
                    logger.error(f"Failed to scan plugin {plugin_file}: {e}")
                    self.ignores.add(plugin_key)
                    self.progress_reporter.update(1, f"Failed: {plugin_file.name}")
                except Exception as e:
                    logger.error(f"Unexpected error scanning {plugin_file}: {e}")
                    self.ignores.add(plugin_key)
                    self.progress_reporter.update(1, f"Error: {plugin_file.name}")
            
            self.progress_reporter.finish(f"Found {len(new_plugins)} new plugins")
            
            # Save updated data
            self.save_data()
        
        return new_plugins

    async def full_scan_async(self) -> Dict[str, PluginInfo]:
        """Perform a full async scan of all plugin types."""
        if not self.async_mode:
            raise ValueError("Async mode not enabled. Set async_mode=True during initialization.")
        
        self.plugins = {}
        
        # Collect all plugin files from all scanners
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend(plugin_files)
        
        # Filter out ignored plugins
        files_to_scan = []
        for plugin_file in all_plugin_files:
            # Find the appropriate scanner for this file
            found_scanner = self._find_scanner_for_file(plugin_file)
            if found_scanner:
                plugin_key = f"{found_scanner.__class__.__name__.replace('Scanner', '').lower()}:{plugin_file}"
                if plugin_key not in self.ignores:
                    files_to_scan.append(plugin_file)
        
        # Scan plugins asynchronously
        if files_to_scan:
            # Use the first scanner that supports async (they all do now)
            async_scanner = cast(AsyncScannerMixin, self.scanners[0])  # VST3Scanner and AUScanner both have AsyncScannerMixin
            
            scanned_plugins = []
            async for plugin in async_scanner.scan_plugins_batch(
                files_to_scan, 
                max_concurrent=self.max_concurrent,
                progress_reporter=self.progress_reporter
            ):
                scanned_plugins.append(plugin)
                self.plugins[plugin.id] = plugin
        
        # Save the results
        self.save_data()
        return self.plugins
    
    async def update_scan_async(self) -> Dict[str, PluginInfo]:
        """Update the scan asynchronously with new plugins while preserving existing data."""
        if not self.async_mode:
            raise ValueError("Async mode not enabled. Set async_mode=True during initialization.")
        
        # Keep track of existing plugins
        existing_plugins = set(self.plugins.keys())
        new_plugins = {}
        
        # Find all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend(plugin_files)
        
        # Only scan plugins that aren't already in the cache
        files_to_scan = []
        for plugin_file in all_plugin_files:
            found_scanner = self._find_scanner_for_file(plugin_file)
            if found_scanner:
                plugin_type = found_scanner.__class__.__name__.replace('Scanner', '').lower()
                plugin_key = f"{plugin_type}:{plugin_file}"
                
                if plugin_key not in existing_plugins and plugin_key not in self.ignores:
                    files_to_scan.append(plugin_file)
        
        # Scan new plugins asynchronously
        if files_to_scan:
            async_scanner = cast(AsyncScannerMixin, self.scanners[0])
            
            async for plugin in async_scanner.scan_plugins_batch(
                files_to_scan,
                max_concurrent=self.max_concurrent,
                progress_reporter=self.progress_reporter
            ):
                self.plugins[plugin.id] = plugin
                new_plugins[plugin.id] = plugin
            
            # Save updated data
            self.save_data()
        
        return new_plugins
    
    def _find_scanner_for_file(self, plugin_file: Path) -> Optional[Union[AUScanner, VST3Scanner]]:
        """Find the appropriate scanner for a given plugin file."""
        for scanner in self.scanners:
            if scanner.validate_plugin_path(plugin_file):
                return scanner  # type: ignore[return-value]
        return None

    def get_json(self) -> str:
        """Return the plugins data as a JSON string."""
        # Use the serializer to convert plugins to dict format
        plugins_dict = {}
        for key, plugin_info in self.plugins.items():
            plugins_dict[key] = PluginSerializer.plugin_to_dict(plugin_info)
        
        return json.dumps(plugins_dict, indent=2)
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Created PLAN.md for implementation roadmap
- Created TODO.md for task tracking
- Created CHANGELOG.md for version history

### Changed
- Refactored scanner architecture to use modular scanner classes
- Improved type annotations throughout the codebase

### Fixed
- Fixed duplicate imports in scanner.py
- Fixed duplicate full_scan method definitions
- Fixed missing attributes in PedalboardScanner class (ignores, ignores_path)
- Fixed incorrect method calls to scanner instances
- Fixed parameter order in save_json_file calls
- Fixed VST3Scanner inheritance issue (removed BaseScanner dependency)
- Fixed missing scan_plugin method implementations in scanner classes
- Implemented proper plugin parameter extraction using pedalboard API
- Added progress bars using tqdm for plugin scanning
- Enhanced AU scanner with fallback to auval for metadata extraction
- Improved VST3 scanner with manufacturer and display name extraction

### Removed
- Removed obsolete scan_aufx_plugins and scan_vst3_plugins methods
- Removed redundant BaseScanner class definition in scanner.py
- Removed unnecessary type aliases in scanner modules

### Enhanced
- Rewrote VST3Scanner to properly load plugins and extract parameters
- Rewrote AUScanner to properly load plugins with fallback to auval
- Added proper plugin metadata extraction (manufacturer, display name)
- Improved plugin path discovery for both VST3 and AU formats
- Created scanner abstraction layer with BaseScanner class
- Added Protocol definitions for scanner interfaces
- Implemented type safety improvements with types.py
- Refactored scanners to use common base class functionality
- Created unified serialization layer with PluginSerializer
- Added cache versioning and metadata support
- Improved type safety with TypedDict definitions
- Centralized all JSON operations in serialization module
- Created custom exception hierarchy for better error handling
- Added constants module for configuration values
- Implemented progress reporting abstraction with multiple backends
- Added retry decorator for transient failures
- Enhanced error handling throughout the codebase
- Added py.typed marker for type checking support
- Fixed most mypy type errors
- Added typing-extensions dependency for Python 3.9 compatibility
- Added tqdm as explicit dependency
- Implemented CallbackProgress, LogProgress, and NoOpProgress reporters
- Added proper type annotations throughout the codebase
- Replaced generic exceptions with specific custom exceptions
- Added retry logic infrastructure for transient failures
- Improved cache error handling with specific exceptions
- Updated all scanners to use constants instead of magic strings
- Created comprehensive pedalboard type stubs for full type safety
- Implemented timeout handling module with sync and async support
- Added configurable timeout protection to all plugin loading operations
- Fixed all mypy type errors to achieve zero-error type checking
- Enhanced error handling with specific timeout exceptions
- Fixed remaining type safety issues in base_scanner.py and __main__.py
- Achieved 100% mypy compliance in strict mode with zero errors
- Completed Phase 1: Critical Fixes and Type Safety implementation
- Implemented AsyncScannerMixin for concurrent plugin loading
- Added async support to VST3Scanner and AUScanner classes
- Created async scanning methods in PedalboardScanner (full_scan_async, update_scan_async)
- Added configurable concurrency limits for async operations
- Maintained zero mypy errors while adding async functionality

## [1.1.0] - Previous Release

### Added
- Added `update` CLI command which only scans plugins that aren't cached yet
- Added `json` and `yaml` CLI commands

### Changed
- Additional refactorings

## [1.0.0] - Initial Release

### Added
- Initial release with basic scanning and listing of both VST-3 and AU plugins
- Command-line interface for easy interaction
- Support for macOS and Windows (Windows untested)
- Plugin parameter extraction with default values
- JSON cache file for plugin information
- Blacklist functionality for problematic plugins
</file>

<file path="README.md">
# Pedalboard Pluginary

[![Codecov](https://codecov.io/gh/twardoch/pedalboard-pluginary/branch/main/graph/badge.svg?token=YOUR_CODECOV_TOKEN_HERE)](https://codecov.io/gh/twardoch/pedalboard-pluginary)
<!-- Replace YOUR_CODECOV_TOKEN_HERE with the actual token from Codecov if needed, or remove the token part if your repo is public and Codecov supports tokenless uploads for it.
The URL should also be verified once the repo is active on Codecov. -->

_Pedalboard Pluginary_ is an independent Python-based package and command-line tool that scans and lists VST-3 plugins on macOS and Windows, and Audio Unit (AU) plugins on macOS. It’s intended as a companion for the _[Pedalboard](https://github.com/spotify/pedalboard)_ Python library by Spotify, but it’s not affiliated with _Pedalboard_ or Spotify.

## Features

With _Pedalboard Pluginary_, you can scan and list VST-3 and AU audio plugins installed on your machine, including their default parameters. 

- It automatically scans and catalogs VST-3 and AU plugins installed on your system.
- Provides a command-line interface (CLI) for quick access to your plugin library.
- Saves the plugin information in a JSON file. This file has the information about the plugin parameters and their default values. 
- Works on Windows and macOS (Windows is currently untested).
- It bundles an `ignores.json` file, which “blacklists” some plugins that are known to cause issues with Pedalboard. It will not scan these, and will not include them in the cache. If you find that some plugins are not working with Pedalboard, you can add them to your `ignores.json` file. See “Contributing” section below.

## Future plans

I plan to extend the package with another functionality, “jobs”, which will allow to load a stack of plugins with their parameter values from a dictionary or JSON file, and run them in a batch using Pedalboard. 

## Installation

To install _Pedalboard Pluginary_, run:

```bash
python3 -m pip install --upgrade pedalboard-pluginary
```

For the current development version:

```bash
python3 -m pip install --upgrade git+https://github.com/twardoch/pedalboard-pluginary
```

## Command-line usage

After installation, you can use `pbpluginary` from the command line.

### Commands:

- `pbpluginary list` displays the plugin information stored in the cache, as a JSON. If no cache exists, it will scan your system and create the cache.
- `pbpluginary scan` scans all available plugins, and caches the information. Run this if you’ve installed or upgraded some VST-3 or AU plugins.

## Python usage

You can use _Pedalboard Pluginary_ as a library in your Python scripts. Here's a quick example:

```python
from pedalboard_pluginary import PedalboardPluginary

pluginary = PedalboardPluginary()
print(pluginary.list_plugins())
```

This snippet will list all plugins that have been scanned and cached, as a JSON.

## Changes

- **v1.1.0**: Added `update` CLI command which only scans plugins that aren’t cached yet. Not perfect. Added `json` and `yaml` CLI commands. Additional refactorings. 
- **v1.0.0**: Initial release with basic scanning and listing of both VST-3 and AU plugins, and command-line interface for easy interaction.

## License

- **Pedalboard Pluginary** is written by Adam Twardoch, with assistance from GPT-4.
- Copyright (c) 2023 Adam Twardoch.
- Licensed under the [Apache-2.0 license](https://raw.githubusercontent.com/twardoch/pedalboard-pluginary/main/LICENSE.txt).
- _Pedalboard Pluginary_ is not affiliated with [Pedalboard](https://github.com/spotify/pedalboard) or Spotify.

## Contributing

- If you encounter any issues or have suggestions, feel free to open an [issue](https://github.com/twardoch/pedalboard-pluginary/issues) on GitHub. 
- If you find that some plugins are not working with Pedalboard, open an issue that lists the key, which is the plugin type and the base filename, like `"aufx/CoreAudio"` or `"vst3/RX 10 Connect"`. You can also modify the [`default_ignores.json`](https://raw.githubusercontent.com/twardoch/pedalboard-pluginary/main/src/pedalboard_pluginary/resources/default_ignores.json) file, and submit a pull request.
- If you want to contribute code, please open a pull request.
</file>

<file path="TODO.md">
# Pedalboard Pluginary - Implementation Roadmap

## Phase 1: Async Performance Revolution (Week 1-2)

### Async Scanner Architecture
- [x] Create AsyncScannerMixin with concurrent plugin loading
- [x] Implement AsyncVST3Scanner and AsyncAUScanner classes
- [x] Add configurable concurrency limits in constants
- [x] Update PedalboardScanner to support async mode
- [ ] Benchmark performance improvements (target: 5-10x speedup)

### SQLite Cache Backend
- [ ] Create cache package with SQLiteCacheBackend
- [ ] Implement indexed search with full-text capabilities
- [ ] Add lazy loading for large datasets
- [ ] Create migration from JSON to SQLite
- [ ] Add cache statistics and management commands

### Smart Cache Invalidation
- [ ] Create ChangeDetector for file-based change detection
- [ ] Implement incremental scan functionality
- [ ] Add modification time tracking
- [ ] Create smart update logic for changed/added/removed plugins

## Phase 2: Modern CLI Revolution (Week 3)

### Migrate to Click + Rich
- [ ] Replace Fire with Click for command structure
- [ ] Add Rich for tables, progress bars, and formatting
- [ ] Implement comprehensive help system
- [ ] Add plugin search and filtering commands
- [ ] Create cache management subcommands

### Advanced Search and Filtering
- [ ] Create PluginSearchEngine with multiple filter options
- [ ] Implement fuzzy search using similarity matching
- [ ] Add suggestion system for similar plugins
- [ ] Create parameter-based filtering
- [ ] Add sorting and pagination support

## Phase 3: Production Hardening (Week 4)

### Comprehensive Testing Strategy
- [ ] Create comprehensive integration test suite
- [ ] Add performance benchmarks and regression tests
- [ ] Test error scenarios and edge cases
- [ ] Add cross-platform compatibility tests
- [ ] Implement continuous integration with multiple Python versions

### Error Recovery and Resilience
- [ ] Create ResilienceManager for error recovery
- [ ] Implement plugin blacklisting for problematic plugins
- [ ] Add safe cache operation wrappers
- [ ] Create cache repair and validation functionality
- [ ] Add health monitoring and status reporting

## Phase 4: Advanced Features (Week 5-6)

### Configuration Management System
- [ ] Create Pydantic-based settings with environment variable support
- [ ] Add configuration file support (.env, config files)
- [ ] Implement configuration validation and defaults
- [ ] Create CLI commands for configuration management

### Plugin Categorization System
- [ ] Create PluginCategory enum and categorization rules
- [ ] Implement intelligent categorization based on names and parameters
- [ ] Add category-based filtering and search
- [ ] Create category statistics and reporting

### Export and Integration Features
- [ ] Implement CSV export functionality
- [ ] Add JSON export with comprehensive metadata
- [ ] Create plugin preset system
- [ ] Add bulk import/export capabilities
- [ ] Create DAW integration helpers

## Quality Gates

### Phase 1 Completion Criteria
- [ ] Async scanning 5x faster than sync
- [ ] SQLite cache working for 1000+ plugins
- [ ] Memory usage < 50MB baseline
- [ ] Zero mypy errors maintained

### Phase 2 Completion Criteria
- [ ] New CLI fully replaces Fire-based interface
- [ ] Rich output formatting working
- [ ] All commands have comprehensive help
- [ ] Search and filtering functional

### Phase 3 Completion Criteria
- [ ] >90% test coverage achieved
- [ ] CI passing on all platforms (Windows, macOS, Linux)
- [ ] Performance benchmarks in place
- [ ] Error recovery working reliably

### Phase 4 Completion Criteria
- [ ] Plugin categorization working accurately
- [ ] Export formats functional
- [ ] Search capabilities fully implemented
- [ ] Configuration system operational

## Dependencies to Add

### Core Dependencies
- [ ] click>=8.0.0 (CLI framework)
- [ ] rich>=13.0.0 (output formatting)
- [ ] pydantic>=2.0.0 (configuration management)

### Development Dependencies
- [ ] pytest-benchmark (performance testing)
- [ ] pytest-asyncio (async testing)
- [ ] psutil (memory testing)

## Success Metrics

### Performance Targets
- [ ] 10-20 plugins/second scan speed (vs 1-2 current)
- [ ] O(log n) search performance (vs O(n) current)
- [ ] Constant memory usage with lazy loading

### Reliability Targets
- [ ] Graceful error handling and recovery
- [ ] 99%+ plugin compatibility
- [ ] Zero data loss on crashes

### Usability Targets
- [ ] Intuitive CLI with comprehensive help
- [ ] Rich formatting and progress reporting
- [ ] Fast search and filtering capabilities

## Current Status

### ✅ Completed (Phase 0)
- Type-safe architecture with comprehensive TypedDict and protocols
- Modular scanner design with extensible BaseScanner
- Robust error handling with custom exception hierarchy
- Unified serialization layer with validation
- Zero mypy errors in strict mode
- Timeout protection for plugin loading

### 🚧 In Progress
- Planning and design for async implementation
- Architecture review for performance improvements

### 📋 Next Priority
1. Implement async scanner architecture (highest impact)
2. Create SQLite cache backend (scalability)
3. Migrate to Click CLI framework (user experience)
</file>

<file path="src/pedalboard_pluginary/scanners/au_scanner.py">
"""
Handles scanning of Audio Unit (AU) plugins on macOS.
"""

import logging
import platform
import subprocess
from pathlib import Path
from typing import Dict, List, Optional

import pedalboard

from ..async_scanner import AsyncScannerMixin
from ..base_scanner import BaseScanner
from ..constants import AU_EXTENSION, PLATFORM_MACOS, PLUGIN_TYPE_AU, PLUGIN_LOAD_TIMEOUT
from ..exceptions import PlatformError, PluginLoadError, PluginScanError
from ..models import PluginInfo, PluginParameter
from ..timeout import sync_timeout, TimeoutError
from ..utils import from_pb_param

logger = logging.getLogger(__name__)


class AUScanner(BaseScanner, AsyncScannerMixin):
    """Scanner for Audio Unit plugins."""
    
    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
    ):
        """Initialize the AU scanner with optional ignore paths and specific paths."""
        super().__init__(ignore_paths, specific_paths)
        self._is_macos = platform.system() == PLATFORM_MACOS
        if not self._is_macos:
            logger.info("AU scanning is only available on macOS.")
    
    @property
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles."""
        return PLUGIN_TYPE_AU
    
    @property
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        return [AU_EXTENSION]
    
    def _get_au_plugin_locations(self) -> List[Path]:
        """Get standard AU plugin locations on macOS."""
        return [
            Path("/Library/Audio/Plug-Ins/Components"),
            Path("~/Library/Audio/Plug-Ins/Components").expanduser(),
            Path("/System/Library/Components"),
        ]
    
    def _list_aufx_plugins_raw(self) -> List[str]:
        """List all Audio Unit effects plugins using auval."""
        if not self._is_macos:
            return []
        
        try:
            result = subprocess.run(
                ["auval", "-a"], 
                capture_output=True, 
                text=True, 
                check=True,
                timeout=30
            )
            return result.stdout.splitlines()
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.warning(f"Failed to run auval command: {e}")
            return []
    
    def _parse_aufx_path_from_auval(self, plugin_str: str) -> Optional[Path]:
        """Parse the AU plugin path from auval output."""
        parts = plugin_str.strip().split()
        if len(parts) >= 3 and parts[0] == "aufx":
            bundle_id = parts[2]
            
            # Search in standard locations
            for location in self._get_au_plugin_locations():
                if location.exists():
                    # First try exact match
                    component_path = location / f"{bundle_id}.component"
                    if component_path.exists():
                        return component_path
                    
                    # Then search for partial match
                    for component in location.glob("*.component"):
                        if bundle_id in str(component):
                            return component
        
        return None
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all AU plugin files.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to AU plugin files found.
        """
        if not self._is_macos:
            return []
        
        if paths:
            # Filter specific paths to only AU component files
            au_paths = [p for p in paths if p.suffix in self.supported_extensions]
            return self._filter_plugin_paths(au_paths)
        
        # Use auval to discover plugins
        discovered_plugins = []
        auval_output = self._list_aufx_plugins_raw()
        
        for line in auval_output:
            if line.strip().startswith("aufx"):
                plugin_path = self._parse_aufx_path_from_auval(line)
                if plugin_path:
                    discovered_plugins.append(plugin_path)
        
        # Also scan standard directories for any missed plugins
        for location in self._get_au_plugin_locations():
            if location.exists():
                try:
                    for component in location.glob("*.component"):
                        if component not in discovered_plugins:
                            discovered_plugins.append(component)
                except Exception as e:
                    logger.error(f"Error scanning directory {location}: {e}")
        
        # Apply filtering
        filtered_list = self._filter_plugin_paths(discovered_plugins)
        logger.info(f"Found {len(filtered_list)} AU plugins after filtering.")
        return filtered_list
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan an AU plugin and return its information.
        
        Args:
            path: Path to the AU plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        if not self.validate_plugin_path(path):
            logger.warning(f"Invalid plugin path: {path}")
            return None
        
        try:
            # Try to load the plugin using pedalboard with timeout
            logger.debug(f"Loading AU plugin: {path}")
            plugin = sync_timeout(pedalboard.load_plugin, PLUGIN_LOAD_TIMEOUT, str(path))
            
            # Extract parameters
            params: Dict[str, PluginParameter] = {}
            if hasattr(plugin, 'parameters'):
                for param_name, param_value in plugin.parameters.items():
                    # Convert the parameter value to our expected type
                    converted_value = from_pb_param(param_value)
                    params[param_name] = PluginParameter(
                        name=param_name,
                        value=converted_value,
                    )
            
            # Try to get manufacturer info
            manufacturer = None
            if hasattr(plugin, 'manufacturer'):
                manufacturer = str(plugin.manufacturer)
            
            # Get the plugin's display name
            display_name = path.stem
            if hasattr(plugin, 'name'):
                display_name = str(plugin.name)
            
            plugin_info = PluginInfo(
                id=self._create_plugin_id(path),
                name=display_name,
                path=str(path),
                filename=path.name,
                plugin_type=self.plugin_type,
                parameters=params,
                manufacturer=manufacturer,
            )
            
            logger.info(f"Successfully scanned AU plugin: {display_name}")
            return plugin_info
            
        except TimeoutError as e:
            logger.warning(f"AU plugin {path} timed out during loading: {e}")
            # Fall back to basic info extraction from auval
            return self._scan_with_auval(path)
        except PluginLoadError:
            # Re-raise our custom exceptions
            raise
        except Exception as e:
            logger.error(f"Failed to scan AU plugin {path} with pedalboard: {e}")
            # Fall back to basic info extraction from auval
            return self._scan_with_auval(path)
    
    def _scan_with_auval(self, path: Path) -> Optional[PluginInfo]:
        """Scan plugin using auval as fallback method."""
        try:
            result = subprocess.run(
                ["auval", "-v", str(path)],
                capture_output=True,
                text=True,
                timeout=10,
            )
            
            # Parse basic info from auval output
            name = path.stem
            manufacturer = None
            
            for line in result.stdout.splitlines():
                if "NAME:" in line:
                    name = line.split("NAME:", 1)[1].strip()
                elif "MANUFACTURER:" in line:
                    manufacturer = line.split("MANUFACTURER:", 1)[1].strip()
            
            if name:
                plugin_info = PluginInfo(
                    id=self._create_plugin_id(path),
                    name=name,
                    path=str(path),
                    filename=path.name,
                    plugin_type=self.plugin_type,
                    manufacturer=manufacturer,
                    parameters={},  # No parameters from auval
                )
                logger.info(f"Scanned AU plugin with auval: {name}")
                return plugin_info
                
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.error(f"Failed to scan {path} with auval: {e}")
        
        return None
</file>

</files>
