This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.giga/
  specifications.json
.github/
  workflows/
    ci.yml
src/
  pedalboard_pluginary/
    cache/
      __init__.py
      json_backend.py
      migration.py
      sqlite_backend.py
    resources/
      default_ignores.json
    scanners/
      __init__.py
      au_scanner.py
      vst3_scanner.py
    __init__.py
    __main__.py
    async_scanner.py
    base_scanner.py
    constants.py
    core.py
    data.py
    exceptions.py
    models.py
    progress.py
    protocols.py
    retry.py
    scanner.py
    serialization.py
    timeout.py
    types.py
    utils.py
  pedalboard-stubs/
    __init__.pyi
tests/
  scanners/
    __init__.py
    test_au_scanner.py
    test_vst3_scanner.py
  test_cli.py
  test_data.py
  test_sqlite_performance.py
  test_utils.py
.coveragerc
.gitignore
.isort.cfg
.pre-commit-config.yaml
AUTHORS.md
build.sh
CHANGELOG.md
LICENSE.txt
PLAN.md
pyproject.toml
README.md
TODO.md
tox.ini
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".giga/specifications.json">
[
  {
    "fileName": "main-overview.mdc",
    "description": "Complete system overview documenting the core architecture of the uicu package, its integration with PyICU and fontTools.unicodedata, and the high-level component relationships"
  },
  {
    "fileName": "unicode-wrapper-models.mdc",
    "description": "Detailed documentation of the data models and classes that wrap PyICU functionality, including their attributes, relationships, and interfaces with Python's native Unicode handling"
  },
  {
    "fileName": "unicode-algorithms.mdc",
    "description": "Implementation details of key algorithms used to process and transform Unicode data, including any custom logic for character handling, normalization, or locale-specific processing"
  },
  {
    "fileName": "data-flow.mdc",
    "description": "Documentation of how Unicode data flows through the system, from PyICU/fontTools input through transformation stages to final output, including error handling and edge cases"
  }
]
</file>

<file path="src/pedalboard_pluginary/cache/__init__.py">
"""Cache backends for Pedalboard Pluginary."""

from .sqlite_backend import SQLiteCacheBackend
from .json_backend import JSONCacheBackend
from .migration import migrate_json_to_sqlite

__all__ = ["SQLiteCacheBackend", "JSONCacheBackend", "migrate_json_to_sqlite"]
</file>

<file path="src/pedalboard_pluginary/cache/json_backend.py">
"""JSON cache backend for backward compatibility."""

from typing import Dict
from pathlib import Path

from ..models import PluginInfo
from ..protocols import CacheBackend
from ..serialization import PluginSerializer


class JSONCacheBackend(CacheBackend):
    """Legacy JSON cache backend for backward compatibility."""
    
    def __init__(self, json_path: Path):
        self.json_path = json_path
    
    def load(self) -> Dict[str, PluginInfo]:
        """Load plugins from JSON cache."""
        return PluginSerializer.load_plugins(self.json_path)
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Save plugins to JSON cache."""
        PluginSerializer.save_plugins(plugins, self.json_path)
    
    def update(self, plugin_id: str, plugin: PluginInfo) -> None:
        """Update a single plugin in JSON cache."""
        plugins = self.load()
        plugins[plugin_id] = plugin
        self.save(plugins)
    
    def delete(self, plugin_id: str) -> None:
        """Remove a plugin from JSON cache."""
        plugins = self.load()
        if plugin_id in plugins:
            del plugins[plugin_id]
            self.save(plugins)
    
    def clear(self) -> None:
        """Clear JSON cache."""
        self.save({})
    
    def exists(self) -> bool:
        """Check if JSON cache exists."""
        return self.json_path.exists()
</file>

<file path="src/pedalboard_pluginary/cache/migration.py">
"""Migration utilities for cache backends."""

import logging
from pathlib import Path
from typing import Dict

from ..models import PluginInfo
from .json_backend import JSONCacheBackend
from .sqlite_backend import SQLiteCacheBackend

logger = logging.getLogger(__name__)


def migrate_json_to_sqlite(json_path: Path, sqlite_path: Path) -> int:
    """Migrate plugins from JSON cache to SQLite cache.
    
    Args:
        json_path: Path to existing JSON cache file.
        sqlite_path: Path to new SQLite cache database.
        
    Returns:
        Number of plugins migrated.
        
    Raises:
        FileNotFoundError: If JSON cache doesn't exist.
        Exception: If migration fails.
    """
    if not json_path.exists():
        raise FileNotFoundError(f"JSON cache not found: {json_path}")
    
    logger.info(f"Migrating plugins from {json_path} to {sqlite_path}")
    
    # Load from JSON
    json_backend = JSONCacheBackend(json_path)
    plugins = json_backend.load()
    
    if not plugins:
        logger.warning("No plugins found in JSON cache")
        return 0
    
    # Save to SQLite
    sqlite_backend = SQLiteCacheBackend(sqlite_path)
    sqlite_backend.save(plugins)
    
    plugin_count = len(plugins)
    logger.info(f"Successfully migrated {plugin_count} plugins to SQLite cache")
    
    return plugin_count


def backup_json_cache(json_path: Path, backup_suffix: str = ".backup") -> Path:
    """Create a backup of the JSON cache before migration.
    
    Args:
        json_path: Path to JSON cache file.
        backup_suffix: Suffix to add to backup filename.
        
    Returns:
        Path to backup file.
    """
    if not json_path.exists():
        raise FileNotFoundError(f"JSON cache not found: {json_path}")
    
    backup_path = json_path.with_suffix(json_path.suffix + backup_suffix)
    backup_path.write_bytes(json_path.read_bytes())
    
    logger.info(f"Created backup: {backup_path}")
    return backup_path
</file>

<file path="src/pedalboard_pluginary/cache/sqlite_backend.py">
"""SQLite cache backend for high-performance plugin storage."""

import sqlite3
import json
import time
from typing import Dict, Optional, Iterator, List
from pathlib import Path

from ..models import PluginInfo
from ..protocols import CacheBackend
from ..serialization import PluginSerializer
from ..exceptions import CacheError


class SQLiteCacheBackend(CacheBackend):
    """High-performance SQLite cache with indexing and full-text search."""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_schema()
    
    def _connect(self) -> sqlite3.Connection:
        """Create database connection with optimizations."""
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        
        # Performance optimizations
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")  
        conn.execute("PRAGMA cache_size=10000")
        conn.execute("PRAGMA temp_store=MEMORY")
        
        return conn
    
    def _init_schema(self) -> None:
        """Initialize optimized database schema."""
        with self._connect() as conn:
            conn.executescript("""
                -- Main plugins table with optimized indexes
                CREATE TABLE IF NOT EXISTS plugins (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    path TEXT NOT NULL UNIQUE,
                    plugin_type TEXT NOT NULL,
                    manufacturer TEXT,
                    parameter_count INTEGER NOT NULL,
                    data TEXT NOT NULL,  -- JSON blob for full plugin data
                    file_mtime REAL NOT NULL,
                    created_at REAL NOT NULL,
                    updated_at REAL NOT NULL
                );
                
                -- Performance indexes
                CREATE INDEX IF NOT EXISTS idx_plugins_name ON plugins(name);
                CREATE INDEX IF NOT EXISTS idx_plugins_type ON plugins(plugin_type);
                CREATE INDEX IF NOT EXISTS idx_plugins_manufacturer ON plugins(manufacturer);
                CREATE INDEX IF NOT EXISTS idx_plugins_path ON plugins(path);
                CREATE INDEX IF NOT EXISTS idx_plugins_mtime ON plugins(file_mtime);
                
                -- Full-text search virtual table
                CREATE VIRTUAL TABLE IF NOT EXISTS plugins_fts USING fts5(
                    id UNINDEXED,
                    name,
                    manufacturer,
                    content='plugins',
                    content_rowid='rowid'
                );
                
                -- FTS triggers to keep search index updated
                CREATE TRIGGER IF NOT EXISTS plugins_fts_insert AFTER INSERT ON plugins
                BEGIN
                    INSERT INTO plugins_fts(rowid, id, name, manufacturer)
                    VALUES (new.rowid, new.id, new.name, new.manufacturer);
                END;
                
                CREATE TRIGGER IF NOT EXISTS plugins_fts_delete AFTER DELETE ON plugins
                BEGIN
                    INSERT INTO plugins_fts(plugins_fts, rowid, id, name, manufacturer)
                    VALUES ('delete', old.rowid, old.id, old.name, old.manufacturer);
                END;
                
                CREATE TRIGGER IF NOT EXISTS plugins_fts_update AFTER UPDATE ON plugins
                BEGIN
                    INSERT INTO plugins_fts(plugins_fts, rowid, id, name, manufacturer)
                    VALUES ('delete', old.rowid, old.id, old.name, old.manufacturer);
                    INSERT INTO plugins_fts(rowid, id, name, manufacturer)
                    VALUES (new.rowid, new.id, new.name, new.manufacturer);
                END;
                
                -- Cache metadata table
                CREATE TABLE IF NOT EXISTS cache_meta (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    updated_at REAL NOT NULL
                );
                
            """)
            
            # Initialize cache version
            conn.execute("""
                INSERT OR IGNORE INTO cache_meta (key, value, updated_at)
                VALUES ('version', '1.0', ?)
            """, (time.time(),))
    
    def load(self) -> Dict[str, PluginInfo]:
        """Load all cached plugins."""
        plugins = {}
        
        try:
            with self._connect() as conn:
                cursor = conn.execute("""
                    SELECT id, data FROM plugins
                    ORDER BY name
                """)
                
                for row in cursor:
                    try:
                        plugin_data = json.loads(row['data'])
                        plugin = PluginSerializer.dict_to_plugin(plugin_data)
                        plugins[row['id']] = plugin
                    except (json.JSONDecodeError, KeyError, TypeError) as e:
                        # Skip corrupted plugin data
                        continue
                        
        except sqlite3.Error as e:
            raise CacheError(f"Failed to load plugins from SQLite cache: {e}")
        
        return plugins
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Save plugins to cache."""
        try:
            with self._connect() as conn:
                # Clear existing data
                conn.execute("DELETE FROM plugins")
                
                # Insert all plugins
                current_time = time.time()
                for plugin_id, plugin in plugins.items():
                    self._insert_plugin(conn, plugin_id, plugin, current_time)
                
                conn.commit()
                
        except sqlite3.Error as e:
            raise CacheError(f"Failed to save plugins to SQLite cache: {e}")
    
    def update(self, plugin_id: str, plugin: PluginInfo) -> None:
        """Update a single plugin in cache."""
        try:
            with self._connect() as conn:
                current_time = time.time()
                
                # Check if plugin exists
                cursor = conn.execute("SELECT id FROM plugins WHERE id = ?", (plugin_id,))
                if cursor.fetchone():
                    # Update existing plugin
                    self._update_plugin(conn, plugin_id, plugin, current_time)
                else:
                    # Insert new plugin
                    self._insert_plugin(conn, plugin_id, plugin, current_time)
                
                conn.commit()
                
        except sqlite3.Error as e:
            raise CacheError(f"Failed to update plugin {plugin_id} in SQLite cache: {e}")
    
    def delete(self, plugin_id: str) -> None:
        """Remove a plugin from cache."""
        try:
            with self._connect() as conn:
                conn.execute("DELETE FROM plugins WHERE id = ?", (plugin_id,))
                conn.commit()
                
        except sqlite3.Error as e:
            raise CacheError(f"Failed to delete plugin {plugin_id} from SQLite cache: {e}")
    
    def clear(self) -> None:
        """Clear entire cache."""
        try:
            with self._connect() as conn:
                conn.execute("DELETE FROM plugins")
                conn.commit()
                
        except sqlite3.Error as e:
            raise CacheError(f"Failed to clear SQLite cache: {e}")
    
    def exists(self) -> bool:
        """Check if cache exists."""
        return self.db_path.exists()
    
    def search(self, query: str, limit: int = 50) -> List[PluginInfo]:
        """Full-text search for plugins."""
        plugins = []
        
        try:
            with self._connect() as conn:
                cursor = conn.execute("""
                    SELECT p.id, p.data 
                    FROM plugins p
                    JOIN plugins_fts fts ON p.rowid = fts.rowid
                    WHERE plugins_fts MATCH ?
                    ORDER BY rank
                    LIMIT ?
                """, (query, limit))
                
                for row in cursor:
                    try:
                        plugin_data = json.loads(row['data'])
                        plugin = PluginSerializer.dict_to_plugin(plugin_data)
                        plugins.append(plugin)
                    except (json.JSONDecodeError, KeyError, TypeError):
                        continue
                        
        except sqlite3.Error as e:
            raise CacheError(f"Failed to search plugins: {e}")
        
        return plugins
    
    def filter_by_type(self, plugin_type: str) -> List[PluginInfo]:
        """Filter plugins by type."""
        plugins = []
        
        try:
            with self._connect() as conn:
                cursor = conn.execute("""
                    SELECT id, data FROM plugins
                    WHERE plugin_type = ?
                    ORDER BY name
                """, (plugin_type,))
                
                for row in cursor:
                    try:
                        plugin_data = json.loads(row['data'])
                        plugin = PluginSerializer.dict_to_plugin(plugin_data)
                        plugins.append(plugin)
                    except (json.JSONDecodeError, KeyError, TypeError):
                        continue
                        
        except sqlite3.Error as e:
            raise CacheError(f"Failed to filter plugins by type: {e}")
        
        return plugins
    
    def get_stats(self) -> Dict[str, int]:
        """Get cache statistics."""
        stats = {}
        
        try:
            with self._connect() as conn:
                # Total plugin count
                cursor = conn.execute("SELECT COUNT(*) as count FROM plugins")
                stats['total_plugins'] = cursor.fetchone()['count']
                
                # Plugin counts by type
                cursor = conn.execute("""
                    SELECT plugin_type, COUNT(*) as count 
                    FROM plugins 
                    GROUP BY plugin_type
                """)
                for row in cursor:
                    stats[f"{row['plugin_type']}_plugins"] = row['count']
                
                # Database size
                cursor = conn.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                stats['db_size_bytes'] = cursor.fetchone()['size']
                
        except sqlite3.Error as e:
            raise CacheError(f"Failed to get cache stats: {e}")
        
        return stats
    
    def _insert_plugin(self, conn: sqlite3.Connection, plugin_id: str, plugin: PluginInfo, current_time: float) -> None:
        """Insert a plugin into the database."""
        plugin_data = PluginSerializer.plugin_to_dict(plugin)
        
        # Handle path - it's a string in the current model
        plugin_path = plugin.path if isinstance(plugin.path, str) else str(plugin.path)
        
        conn.execute("""
            INSERT INTO plugins (
                id, name, path, plugin_type, manufacturer, parameter_count,
                data, file_mtime, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            plugin_id,
            plugin.name,
            plugin_path,
            plugin.plugin_type,
            plugin.manufacturer,
            len(plugin.parameters),
            json.dumps(plugin_data),
            0,  # No file mtime since path is string
            current_time,
            current_time
        ))
    
    def _update_plugin(self, conn: sqlite3.Connection, plugin_id: str, plugin: PluginInfo, current_time: float) -> None:
        """Update an existing plugin in the database."""
        plugin_data = PluginSerializer.plugin_to_dict(plugin)
        
        # Handle path - it's a string in the current model
        plugin_path = plugin.path if isinstance(plugin.path, str) else str(plugin.path)
        
        conn.execute("""
            UPDATE plugins SET
                name = ?, path = ?, plugin_type = ?, manufacturer = ?,
                parameter_count = ?, data = ?, file_mtime = ?, updated_at = ?
            WHERE id = ?
        """, (
            plugin.name,
            plugin_path,
            plugin.plugin_type,
            plugin.manufacturer,
            len(plugin.parameters),
            json.dumps(plugin_data),
            0,  # No file mtime since path is string
            current_time,
            plugin_id
        ))
</file>

<file path="src/pedalboard_pluginary/resources/default_ignores.json">
[
    "aufx/ANIMATE",
    "aufx/AudioDSP",
    "aufx/CoreAudio",
    "aufx/Dynamics",
    "aufx/iZNectar4Auto-LevelAUHook",
    "aufx/iZNeutron4AUHook",
    "aufx/iZNeutron4CompressorAUHook",
    "aufx/iZNeutron4EqualizerAUHook",
    "aufx/iZNeutron4ExciterAUHook",
    "aufx/iZNeutron4GateAUHook",
    "aufx/iZNeutron4SculptorAUHook",
    "aufx/iZNeutron4TransientShaperAUHook",
    "aufx/iZNeutron4UnmaskAUHook",
    "aufx/iZRX10ConnectAUHook",
    "aufx/iZRelayAUHook",
    "aufx/smartEQ3",
    "aufx/smartcomp2",
    "aufx/smartgate",
    "aufx/unknown URL",
    "vst3/RX 10 Connect",
    "vst3/RX 10 Repair Assistant",
    "vst3/smartEQ3"
]
</file>

<file path="src/pedalboard_pluginary/scanners/__init__.py">
# pedalboard_pluginary/scanners/__init__.py

# This file makes Python treat the `scanners` directory as a package.

# Optionally, you can import specific classes or functions here to make them
# available at the package level, e.g.:
# from .au_scanner import AUScanner
# from .vst3_scanner import VST3Scanner

# For now, it will be kept empty.
</file>

<file path="src/pedalboard_pluginary/exceptions.py">
"""
Custom exception hierarchy for pedalboard_pluginary.
"""

from typing import Optional


class PluginaryError(Exception):
    """Base exception for all Pluginary errors."""
    
    def __init__(self, message: str, details: Optional[str] = None):
        """Initialize the exception with a message and optional details.
        
        Args:
            message: Main error message.
            details: Optional additional details about the error.
        """
        super().__init__(message)
        self.message = message
        self.details = details
    
    def __str__(self) -> str:
        """Return string representation of the error."""
        if self.details:
            return f"{self.message} - {self.details}"
        return self.message


class ScannerError(PluginaryError):
    """Base exception for scanner-related errors."""
    pass


class PluginLoadError(ScannerError):
    """Raised when a plugin fails to load."""
    
    def __init__(self, plugin_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            plugin_path: Path to the plugin that failed to load.
            reason: Optional reason for the failure.
        """
        message = f"Failed to load plugin: {plugin_path}"
        super().__init__(message, reason)
        self.plugin_path = plugin_path


class PluginScanError(ScannerError):
    """Raised when scanning a plugin fails."""
    
    def __init__(self, plugin_path: str, scanner_type: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            plugin_path: Path to the plugin that failed to scan.
            scanner_type: Type of scanner that failed (e.g., 'vst3', 'aufx').
            reason: Optional reason for the failure.
        """
        message = f"Failed to scan {scanner_type} plugin: {plugin_path}"
        super().__init__(message, reason)
        self.plugin_path = plugin_path
        self.scanner_type = scanner_type


class CacheError(PluginaryError):
    """Base exception for cache-related errors."""
    pass


class CacheCorruptedError(CacheError):
    """Raised when cache file is corrupted."""
    
    def __init__(self, cache_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            cache_path: Path to the corrupted cache file.
            reason: Optional reason or details about the corruption.
        """
        message = f"Cache file is corrupted: {cache_path}"
        super().__init__(message, reason)
        self.cache_path = cache_path


class CacheVersionError(CacheError):
    """Raised when cache version is incompatible."""
    
    def __init__(self, expected: str, actual: str, cache_path: str):
        """Initialize the exception.
        
        Args:
            expected: Expected cache version.
            actual: Actual cache version found.
            cache_path: Path to the cache file.
        """
        message = f"Cache version mismatch: expected {expected}, got {actual}"
        details = f"Cache file: {cache_path}"
        super().__init__(message, details)
        self.expected_version = expected
        self.actual_version = actual
        self.cache_path = cache_path


class CacheWriteError(CacheError):
    """Raised when writing to cache fails."""
    
    def __init__(self, cache_path: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            cache_path: Path to the cache file.
            reason: Optional reason for the write failure.
        """
        message = f"Failed to write cache: {cache_path}"
        super().__init__(message, reason)
        self.cache_path = cache_path


class ConfigError(PluginaryError):
    """Base exception for configuration-related errors."""
    pass


class InvalidConfigError(ConfigError):
    """Raised when configuration is invalid."""
    
    def __init__(self, config_key: str, invalid_value: str, reason: Optional[str] = None):
        """Initialize the exception.
        
        Args:
            config_key: Configuration key that has invalid value.
            invalid_value: The invalid value.
            reason: Optional reason why the value is invalid.
        """
        message = f"Invalid configuration value for '{config_key}': {invalid_value}"
        super().__init__(message, reason)
        self.config_key = config_key
        self.invalid_value = invalid_value


class PlatformError(PluginaryError):
    """Raised when an operation is not supported on the current platform."""
    
    def __init__(self, operation: str, platform: str, supported_platforms: Optional[list[str]] = None):
        """Initialize the exception.
        
        Args:
            operation: Operation that is not supported.
            platform: Current platform.
            supported_platforms: Optional list of supported platforms.
        """
        message = f"Operation '{operation}' is not supported on {platform}"
        if supported_platforms:
            details = f"Supported platforms: {', '.join(supported_platforms)}"
        else:
            details = None
        super().__init__(message, details)
        self.operation = operation
        self.platform = platform
        self.supported_platforms = supported_platforms or []
</file>

<file path="src/pedalboard_pluginary/protocols.py">
"""
Protocol definitions for plugin scanner implementations.
"""

from typing import Protocol, List, Optional, Dict, runtime_checkable
from pathlib import Path

from .models import PluginInfo


@runtime_checkable
class PluginScanner(Protocol):
    """Protocol defining the interface for plugin scanners."""
    
    plugin_type: str
    supported_extensions: List[str]
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all plugin files of this scanner's type.
        
        Args:
            paths: Optional list of specific paths to check. If None, searches default locations.
            
        Returns:
            List of paths to plugin files found.
        """
        ...
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a single plugin file and return its information.
        
        Args:
            path: Path to the plugin file to scan.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        ...
    
    def validate_plugin_path(self, path: Path) -> bool:
        """Validate if a path is a valid plugin for this scanner.
        
        Args:
            path: Path to validate.
            
        Returns:
            True if the path is a valid plugin file, False otherwise.
        """
        ...


@runtime_checkable
class ProgressReporter(Protocol):
    """Protocol for progress reporting implementations."""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        ...
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        ...
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        ...


@runtime_checkable
class CacheBackend(Protocol):
    """Protocol for cache backend implementations."""
    
    def load(self) -> Dict[str, PluginInfo]:
        """Load all cached plugins.
        
        Returns:
            Dictionary mapping plugin IDs to PluginInfo objects.
        """
        ...
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Save plugins to cache.
        
        Args:
            plugins: Dictionary mapping plugin IDs to PluginInfo objects.
        """
        ...
    
    def update(self, plugin_id: str, plugin: PluginInfo) -> None:
        """Update a single plugin in cache.
        
        Args:
            plugin_id: ID of the plugin to update.
            plugin: Updated PluginInfo object.
        """
        ...
    
    def delete(self, plugin_id: str) -> None:
        """Remove a plugin from cache.
        
        Args:
            plugin_id: ID of the plugin to remove.
        """
        ...
    
    def clear(self) -> None:
        """Clear entire cache."""
        ...
    
    def exists(self) -> bool:
        """Check if cache exists.
        
        Returns:
            True if cache exists, False otherwise.
        """
        ...
</file>

<file path="src/pedalboard_pluginary/retry.py">
"""
Retry logic for handling transient failures.
"""

import functools
import logging
import time
from typing import Any, Callable, Optional, Tuple, Type, TypeVar, Union

from .constants import MAX_SCAN_RETRIES, SCAN_RETRY_DELAY

logger = logging.getLogger(__name__)

F = TypeVar('F', bound=Callable[..., Any])


def with_retry(
    exceptions: Union[Type[Exception], Tuple[Type[Exception], ...]],
    max_attempts: int = MAX_SCAN_RETRIES,
    delay: float = SCAN_RETRY_DELAY,
    backoff_factor: float = 2.0,
    max_delay: float = 60.0,
) -> Callable[[F], F]:
    """Decorator that retries a function on specified exceptions.
    
    Args:
        exceptions: Exception or tuple of exceptions to catch and retry on.
        max_attempts: Maximum number of attempts (including the first).
        delay: Initial delay between retries in seconds.
        backoff_factor: Factor to multiply delay by after each failure.
        max_delay: Maximum delay between retries in seconds.
        
    Returns:
        Decorated function that will retry on failure.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            current_delay = delay
            last_exception: Optional[Exception] = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_attempts - 1:
                        # Last attempt, re-raise
                        logger.warning(
                            f"{func.__name__} failed after {max_attempts} attempts: {e}"
                        )
                        raise
                    
                    logger.info(
                        f"{func.__name__} failed (attempt {attempt + 1}/{max_attempts}): {e}. "
                        f"Retrying in {current_delay:.1f}s..."
                    )
                    time.sleep(current_delay)
                    
                    # Exponential backoff with max delay
                    current_delay = min(current_delay * backoff_factor, max_delay)
            
            # This should never be reached, but just in case
            if last_exception:
                raise last_exception
            else:
                raise RuntimeError(f"{func.__name__} failed with unknown error")
        
        return wrapper  # type: ignore[return-value]
    
    return decorator


def with_timeout(timeout: float) -> Callable[[F], F]:
    """Decorator that adds a timeout to a function.
    
    Note: This is a placeholder for future implementation.
    Proper timeout handling requires different approaches for
    synchronous vs asynchronous functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            # TODO: Implement proper timeout handling
            # For now, just pass through
            return func(*args, **kwargs)
        
        return wrapper  # type: ignore[return-value]
    
    return decorator
</file>

<file path="src/pedalboard_pluginary/timeout.py">
"""
Timeout handling for plugin operations.
"""

import asyncio
import concurrent.futures
import functools
import logging
from typing import Any, Awaitable, Callable, TypeVar, Union

from .constants import PLUGIN_LOAD_TIMEOUT

logger = logging.getLogger(__name__)

T = TypeVar('T')
F = TypeVar('F', bound=Callable[..., Any])


class TimeoutError(Exception):
    """Raised when an operation times out."""
    
    def __init__(self, message: str, timeout: float):
        super().__init__(message)
        self.timeout = timeout


def sync_timeout(func: Callable[..., T], timeout: float, *args: Any, **kwargs: Any) -> T:
    """Execute synchronous function with timeout using ThreadPoolExecutor.
    
    Args:
        func: Function to execute.
        timeout: Timeout in seconds.
        *args: Positional arguments for the function.
        **kwargs: Keyword arguments for the function.
        
    Returns:
        Function result.
        
    Raises:
        TimeoutError: If function execution exceeds timeout.
        Exception: Any exception raised by the function.
    """
    logger.debug(f"Executing {func.__name__} with {timeout}s timeout")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(func, *args, **kwargs)
        try:
            result = future.result(timeout=timeout)
            logger.debug(f"{func.__name__} completed successfully")
            return result
        except concurrent.futures.TimeoutError:
            logger.warning(f"{func.__name__} timed out after {timeout}s")
            # Cancel the future to prevent resource leaks
            future.cancel()
            raise TimeoutError(f"{func.__name__} timed out after {timeout}s", timeout)
        except Exception as e:
            logger.error(f"{func.__name__} failed: {e}")
            raise


async def async_timeout(coro_func: Callable[..., Awaitable[T]], timeout: float, *args: Any, **kwargs: Any) -> T:
    """Execute coroutine function with timeout.
    
    Args:
        coro_func: Coroutine function to execute.
        timeout: Timeout in seconds.
        *args: Positional arguments for the function.
        **kwargs: Keyword arguments for the function.
        
    Returns:
        Function result.
        
    Raises:
        TimeoutError: If function execution exceeds timeout.
        Exception: Any exception raised by the function.
    """
    logger.debug(f"Executing {coro_func.__name__} with {timeout}s timeout")
    
    try:
        result: T = await asyncio.wait_for(coro_func(*args, **kwargs), timeout=timeout)
        logger.debug(f"{coro_func.__name__} completed successfully")
        return result
    except asyncio.TimeoutError:
        logger.warning(f"{coro_func.__name__} timed out after {timeout}s")
        raise TimeoutError(f"{coro_func.__name__} timed out after {timeout}s", timeout)
    except Exception as e:
        logger.error(f"{coro_func.__name__} failed: {e}")
        raise


def with_sync_timeout(timeout: float = PLUGIN_LOAD_TIMEOUT) -> Callable[[F], F]:
    """Decorator that adds timeout to synchronous functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            return sync_timeout(func, timeout, *args, **kwargs)
        return wrapper  # type: ignore[return-value]
    return decorator


def with_async_timeout(timeout: float = PLUGIN_LOAD_TIMEOUT) -> Callable[[F], F]:
    """Decorator that adds timeout to async functions.
    
    Args:
        timeout: Timeout in seconds.
        
    Returns:
        Decorated function.
    """
    def decorator(func: F) -> F:
        @functools.wraps(func)
        async def async_wrapper(*args: Any, **kwargs: Any) -> Any:
            return await async_timeout(func, timeout, *args, **kwargs)
        return async_wrapper  # type: ignore[return-value]
    return decorator
</file>

<file path="src/pedalboard-stubs/__init__.pyi">
"""Type stubs for pedalboard library."""

from typing import Dict, Union, Any, Optional, TypeVar
from pathlib import Path

# Parameter value types that pedalboard can return
ParameterValue = Union[float, int, bool, str]

class Plugin:
    """Base plugin class."""
    
    # Core attributes that all plugins have
    parameters: Dict[str, ParameterValue]
    name: str
    manufacturer: Optional[str]
    
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...

class AudioUnitPlugin(Plugin):
    """Audio Unit plugin class."""
    pass

class VST3Plugin(Plugin):
    """VST3 plugin class."""
    pass

# Plugin loading function
def load_plugin(
    path_or_name: Union[str, Path], 
    plugin_name: Optional[str] = None,
    disable_caching: bool = False,
    **kwargs: Any
) -> Plugin: ...

# Re-export common types
__all__ = [
    "Plugin",
    "AudioUnitPlugin", 
    "VST3Plugin",
    "load_plugin",
    "ParameterValue",
]
</file>

<file path="tests/scanners/__init__.py">
# tests/scanners/__init__.py
# This file makes Python treat the `tests/scanners` directory as a package.
</file>

<file path="tests/scanners/test_au_scanner.py">
# tests/scanners/test_au_scanner.py
import os
import platform
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock

from pedalboard_pluginary.scanners.au_scanner import AUScanner

# Sample auval output
AUVAL_OUTPUT_VALID = """
 Westwood AU Test
--------------------------------------------------
AUVALTOOL Discount AU
--------------------------------------------------
PLAYER version 2.0.13 (build 17)
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'dely' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: AUDelay
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/AUDelay.component
Component AU Path: /Library/Audio/Plug-Ins/Components/AUDelay.component/Contents/MacOS/AUDelay

* * PASS
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'mcmp' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: AUMultibandCompressor
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component
Component AU Path: /Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component/Contents/MacOS/AUMultibandCompressor

* * PASS
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aumf' - 'dls ' - 'appl'
--------------------------------------------------
Manufacturer String: Apple
AudioUnit Name: DLSMusicDevice
Component Version: 1.7.0
Component Bundle Path: /Library/Audio/Plug-Ins/Components/DLSMusicDevice.component
Component AU Path: /Library/Audio/Plug-Ins/Components/DLSMusicDevice.component/Contents/MacOS/DLSMusicDevice

* * PASS
--------------------------------------------------
TESTING OPEN TIMES:
COLD:
Time to open AudioUnit:      21.112 ms
WARM:
Time to open AudioUnit:      0.042  ms
This AudioUnit is a version 3 implementation.
FIRST TIME:
FATAL ERROR: Initialize: result: -50


--------------------------------------------------
AU VALIDATION SUCCEEDED.
--------------------------------------------------
"""

AUVAL_OUTPUT_GARBAGE_URL = """
 Westwood AU Test
--------------------------------------------------
VALIDATING AUDIO UNIT: 'aufx' - 'xxxx' - 'test'
--------------------------------------------------
Manufacturer String: Test Inc
AudioUnit Name: BadURLPlugin
Component Version: 1.0.0
Component Bundle Path: /path/to/plugin with spaces.component
Component AU Path: (null)

* * PASS
--------------------------------------------------
AU VALIDATION SUCCEEDED.
--------------------------------------------------
"""


@pytest.fixture
def au_scanner_instance():
    return AUScanner(ignores=set())

@pytest.fixture
def au_scanner_with_ignores_instance():
    return AUScanner(ignores={"aufx/DLSMusicDevice"}) # Key is type/stem

@patch('platform.system', return_value='Darwin') # Assume macOS for these tests
class TestAUScanner:

    @patch('subprocess.run')
    def test_list_aufx_plugins_raw_success(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        lines = au_scanner_instance._list_aufx_plugins_raw()
        assert len(lines) > 0
        assert "AUDelay" in AUVAL_OUTPUT_VALID
        mock_subprocess_run.assert_called_once_with(
            ["auval", "-l"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )

    @patch('subprocess.run', side_effect=FileNotFoundError("auval not found"))
    def test_list_aufx_plugins_raw_auval_not_found(self, mock_subprocess_run, au_scanner_instance):
        lines = au_scanner_instance._list_aufx_plugins_raw()
        assert lines == []

    @patch('subprocess.run')
    def test_find_plugin_files_valid_output(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        # Mock Path.resolve() and Path.exists() for paths found by auval
        with patch.object(Path, 'resolve') as mock_resolve, \
             patch.object(Path, 'exists', return_value=True) as mock_exists:

            # Make resolve return a Path object that can be further manipulated if needed
            # and also has an 'exists' method.
            def side_effect_resolve(*args, **kwargs):
                # The input to resolve is the path string from auval output
                # e.g., Path("/Library/Audio/Plug-Ins/Components/AUDelay.component")
                # We want it to return itself basically, but as a mock if needed for exists()
                p = Path(*args) # Reconstruct the original path
                # Mock the exists for this specific path if needed, though global mock_exists might cover it
                # For bundle path logic, ensure suffix is checked on original path.
                return p
            mock_resolve.side_effect = side_effect_resolve

            plugin_files = au_scanner_instance.find_plugin_files()

            assert len(plugin_files) == 3 # AUDelay, AUMultibandCompressor, DLSMusicDevice
            expected_paths = [
                Path("/Library/Audio/Plug-Ins/Components/AUDelay.component"),
                Path("/Library/Audio/Plug-Ins/Components/AUMultibandCompressor.component"),
                Path("/Library/Audio/Plug-Ins/Components/DLSMusicDevice.component"),
            ]
            for p in expected_paths:
                assert p.resolve() in plugin_files # Comparing resolved paths

    @patch('subprocess.run')
    def test_find_plugin_files_with_ignores(self, mock_subprocess_run, au_scanner_with_ignores_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_with_ignores_instance.find_plugin_files()

            # DLSMusicDevice should be ignored
            assert len(plugin_files) == 2
            ignored_path = Path("/Library/Audio/Plug-Ins/Components/DLSMusicDevice.component").resolve()
            assert ignored_path not in plugin_files
            delay_path = Path("/Library/Audio/Plug-Ins/Components/AUDelay.component").resolve()
            assert delay_path in plugin_files


    @patch('subprocess.run')
    def test_find_plugin_files_garbage_url(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_GARBAGE_URL # Contains (null) URL
        mock_subprocess_run.return_value = mock_process

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p) if p else None), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_instance.find_plugin_files()
            assert len(plugin_files) == 0 # Should skip the one with (null) URL

    @patch('platform.system', return_value='Linux') # Test non-Darwin platform
    def test_scanner_on_non_macos(self, mock_platform_system_linux, au_scanner_instance):
        assert au_scanner_instance._list_aufx_plugins_raw() == []
        assert au_scanner_instance.find_plugin_files() == []

    @patch('subprocess.run')
    def test_find_plugin_files_with_specific_paths_filter(self, mock_subprocess_run, au_scanner_instance):
        mock_process = MagicMock()
        mock_process.stdout = AUVAL_OUTPUT_VALID
        mock_subprocess_run.return_value = mock_process

        # User wants to check only AUDelay
        specific_paths_to_check = [Path("/Library/Audio/Plug-Ins/Components/AUDelay.component")]

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):
            plugin_files = au_scanner_instance.find_plugin_files(plugin_paths=specific_paths_to_check)

            assert len(plugin_files) == 1
            assert Path("/Library/Audio/Plug-Ins/Components/AUDelay.component").resolve() in plugin_files

    # Test for bundle path resolution logic
    # This requires more intricate mocking of Path objects if we don't want to rely on filesystem
    @patch('subprocess.run')
    def test_bundle_path_resolution(self, mock_subprocess_run, au_scanner_instance):
        # Simulate auval output where path is deep inside the bundle
        deep_path_auval_output = """
        VALIDATING AUDIO UNIT: 'aufx' - 'test' - 'tstc'
        --------------------------------------------------
        Manufacturer String: TestCompany
        AudioUnit Name: DeepTestPlugin
        Component Version: 1.0.0
        Component Bundle Path: /Some/Path/DeepTestPlugin.component/Contents/MacOS/DeepTestPlugin
        Component AU Path: /Some/Path/DeepTestPlugin.component/Contents/MacOS/DeepTestPlugin
        * * PASS
        --------------------------------------------------
        AU VALIDATION SUCCEEDED.
        --------------------------------------------------
        """
        mock_process = MagicMock()
        mock_process.stdout = deep_path_auval_output
        mock_subprocess_run.return_value = mock_process

        # We need to mock Path behavior for suffix and parent
        # The Path object created from the string should behave as expected.
        # No complex mocking needed if Path objects work as standard for these attributes.
        # We only need to ensure Path.resolve and Path.exists are controlled.

        with patch.object(Path, 'resolve', side_effect=lambda p: Path(p)), \
             patch.object(Path, 'exists', return_value=True):

            plugin_files = au_scanner_instance.find_plugin_files()

            assert len(plugin_files) == 1
            # The scanner should correctly identify the .component bundle path
            expected_bundle_path = Path("/Some/Path/DeepTestPlugin.component").resolve()
            assert expected_bundle_path in plugin_files

# TODO: Add tests for error conditions in auval (e.g., CalledProcessError)
# TODO: Add tests for when Path.resolve() or other Path operations raise exceptions
# (though these are less likely for valid path strings)
</file>

<file path="tests/test_cli.py">
# tests/test_cli.py
import os
import subprocess
import json
import yaml
from pathlib import Path
from unittest.mock import patch, MagicMock
import pytest

from pedalboard_pluginary.data import APP_NAME, PLUGINS_CACHE_FILENAME_BASE, get_cache_path

# Helper to get the cache file path for plugins
def get_plugins_cache_file():
    return get_cache_path(PLUGINS_CACHE_FILENAME_BASE)

@pytest.fixture(autouse=True)
def manage_plugin_cache():
    """Fixture to ensure plugin cache is handled before and after tests."""
    cache_file = get_plugins_cache_file()
    original_content = None

    if cache_file.exists():
        original_content = cache_file.read_text()
        cache_file.unlink() # Remove before test

    yield # Test runs here

    # Cleanup: remove cache file created by test, or restore original
    if cache_file.exists():
        cache_file.unlink()
    if original_content:
        # Ensure parent directory exists before writing back
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        cache_file.write_text(original_content)


# Mocked data for PedalboardScanner.scan_all_plugins and load_json_file
# This data will be "written" by the mocked scan and "read" by list/json/yaml
MOCK_PLUGIN_DATA = {
    "vst3/MockSynth": {
        "id": "vst3/MockSynth",
        "name": "MockSynth",
        "path": "/fake/path/to/MockSynth.vst3",
        "filename": "MockSynth.vst3",
        "plugin_type": "vst3",
        "parameters": {
            "Volume": {"name": "Volume", "value": 0.75},
            "Pan": {"name": "Pan", "value": 0.0}
        },
        "manufacturer": "FakePlugins",
        "name_in_file": "MockSynth"
    },
    "aufx/MockEffect": {
        "id": "aufx/MockEffect",
        "name": "MockEffect",
        "path": "/fake/path/to/MockEffect.component",
        "filename": "MockEffect.component",
        "plugin_type": "aufx",
        "parameters": {
            "Wet/Dry": {"name": "Wet/Dry", "value": 0.5}
        },
        "manufacturer": "FakeAudio",
        "name_in_file": "MockEffect"
    }
}

# This mock will replace the actual PedalboardScanner instance or its methods
@patch('pedalboard_pluginary.scanner.PedalboardScanner.scan_all_plugins')
@patch('pedalboard_pluginary.scanner.PedalboardScanner.update_scan') # Also mock update_scan
@patch('pedalboard_pluginary.scanner.PedalboardScanner.save_plugins') # Mock save_plugins
@patch('pedalboard_pluginary.core.PedalboardPluginary.load_data') # Mock load_data in core
def run_cli_command(
    cli_args_list,
    mock_core_load_data,
    mock_scanner_save_plugins,
    mock_scanner_update_scan,
    mock_scanner_scan_all,
    expected_exit_code=0
):
    """Helper to run CLI commands and capture output."""

    # If scan or update is called, make them "create" the mock cache file
    def side_effect_scan_or_update(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4)
        # The actual scan methods in PedalboardScanner don't return anything.
        # They modify self.plugins and then call self.save_plugins.
        # We've mocked save_plugins separately.

    mock_scanner_scan_all.side_effect = side_effect_scan_or_update
    mock_scanner_update_scan.side_effect = side_effect_scan_or_update

    # Mock load_data to set the plugins attribute on an instance if needed,
    # or simply prevent it from trying to load a real file during list commands
    # if scan hasn't run.
    # For 'list', 'json', 'yaml', the PedalboardPluginary instance will try to load.
    # We can patch load_json_file used by PedalboardPluginary.load_data

    base_command = ["pbpluginary"]
    full_command = base_command + cli_args_list

    try:
        result = subprocess.run(full_command, capture_output=True, text=True, check=False)
        if result.returncode != expected_exit_code:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        assert result.returncode == expected_exit_code
        return result
    except FileNotFoundError:
        pytest.fail("pbpluginary command not found. Ensure it's installed and in PATH for testing.")


# Test 'scan' command
# Patching at the source of where PedalboardScanner is instantiated or used by CLI
@patch('pedalboard_pluginary.__main__.PedalboardScanner')
def test_cli_scan(MockScannerConstructor):
    # Mock the instance methods that would be called
    mock_scanner_instance = MockScannerConstructor.return_value
    mock_scanner_instance.rescan.return_value = None # rescan calls full_scan which calls scan_all_plugins

    # We need rescan (which is an alias for full_scan) to effectively create the cache
    # by having its underlying scan_all_plugins call write the MOCK_PLUGIN_DATA
    def mock_rescan_writes_cache(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4)
    mock_scanner_instance.rescan.side_effect = mock_rescan_writes_cache

    result = run_cli_command(["scan"]) # Uses the patches from run_cli_command's decorators

    # Check that the cache file was created with mock data
    cache_file = get_plugins_cache_file()
    assert cache_file.exists()
    with open(cache_file, 'r') as f:
        data_from_cache = json.load(f)
    assert data_from_cache == MOCK_PLUGIN_DATA

    # Check if scan method on the instance was called
    mock_scanner_instance.rescan.assert_called_once()


# Test 'list' command (implicitly tests JSON output)
# For list, we need to ensure that the cache exists or that PedalboardPluginary can load it.
# The manage_plugin_cache fixture helps here.
# We also need to control what PedalboardPluginary.load_data does.
@patch('pedalboard_pluginary.data.load_json_file') # Patch where load_json_file is defined
def test_cli_list(mock_load_json, capsys):
    # Setup: Ensure a cache file with MOCK_PLUGIN_DATA exists for 'list' to read
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    # Configure the mock for load_json_file used by PedalboardPluginary
    # It should return the MOCK_PLUGIN_DATA when the specific plugins cache path is requested
    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            # Return raw dict, PedalboardPluginary.load_data will handle reconstruction
            return MOCK_PLUGIN_DATA
        return {} # Default for other calls
    mock_load_json.side_effect = side_effect_load_json

    # Run the 'list' command
    # Using direct function call to avoid subprocess complexity with stdout/stderr and fire's display hook
    from pedalboard_pluginary.__main__ import list_json_cli

    # Fire's Display hook is tricky to test with subprocess.run, so call directly.
    # list_json_cli returns a string.
    # We need to ensure that when `pbpluginary list` is run, this function is called
    # and its output (which is JSON string) is printed.
    # For simplicity here, just test the function that `fire` would call.

    # To test the actual CLI output, we need to let pbpluginary run as subprocess
    # and capture stdout. This means not mocking PedalboardPluginary or its load_data directly here
    # but ensuring the underlying data.load_json_file behaves as expected due to the patch.

    result = subprocess.run(["pbpluginary", "list"], capture_output=True, text=True, check=True)

    # The output should be the MOCK_PLUGIN_DATA formatted as JSON
    # Fire wraps output, so it might not be exact JSON string if printed line-by-line.
    # The default 'list' command in __main__.py calls bdict().to_json() and fire prints it.
    # Let's parse the stdout.
    try:
        output_data = json.loads(result.stdout)
        assert output_data == MOCK_PLUGIN_DATA
    except json.JSONDecodeError:
        pytest.fail(f"CLI output was not valid JSON. Output:\n{result.stdout}")


# Test 'json' command (should be identical to 'list')
@patch('pedalboard_pluginary.data.load_json_file')
def test_cli_json_output(mock_load_json_for_json_cmd, capsys):
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            return MOCK_PLUGIN_DATA
        return {}
    mock_load_json_for_json_cmd.side_effect = side_effect_load_json

    result = subprocess.run(["pbpluginary", "json"], capture_output=True, text=True, check=True)
    try:
        output_data = json.loads(result.stdout)
        assert output_data == MOCK_PLUGIN_DATA
    except json.JSONDecodeError:
        pytest.fail(f"CLI output for 'json' was not valid JSON. Output:\n{result.stdout}")


# Test 'yaml' command
@patch('pedalboard_pluginary.data.load_json_file')
def test_cli_yaml_output(mock_load_json_for_yaml_cmd, capsys):
    cache_file = get_plugins_cache_file()
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(MOCK_PLUGIN_DATA, f, indent=4)

    def side_effect_load_json(path_arg):
        if path_arg == cache_file:
            return MOCK_PLUGIN_DATA
        return {}
    mock_load_json_for_yaml_cmd.side_effect = side_effect_load_json

    result = subprocess.run(["pbpluginary", "yaml"], capture_output=True, text=True, check=True)
    try:
        # python-benedict's to_yaml output might have specific formatting.
        # For robustness, parse it back and compare with original data.
        output_data = yaml.safe_load(result.stdout)
        # YAML load might produce slightly different types (e.g. list for dict items sometimes)
        # A direct comparison MOCK_PLUGIN_DATA might be tricky if numbers are float vs int.
        # For now, let's assume benedict produces standard YAML that converts back cleanly.
        assert json.dumps(output_data, sort_keys=True) == json.dumps(MOCK_PLUGIN_DATA, sort_keys=True)
    except yaml.YAMLError:
        pytest.fail(f"CLI output for 'yaml' was not valid YAML. Output:\n{result.stdout}")
    except Exception as e:
        pytest.fail(f"Error comparing YAML output: {e}. Output:\n{result.stdout}")


# Test 'update' command
@patch('pedalboard_pluginary.__main__.PedalboardScanner')
def test_cli_update(MockScannerConstructorForUpdate):
    mock_scanner_instance = MockScannerConstructorForUpdate.return_value

    # Simulate that update_scan effectively writes the cache
    def mock_update_scan_writes_cache(*args, **kwargs):
        cache_file = get_plugins_cache_file()
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        # Update might add to existing data or rescan if no cache.
        # For this test, assume it behaves like scan if no cache.
        with open(cache_file, 'w') as f:
            json.dump(MOCK_PLUGIN_DATA, f, indent=4) # For simplicity, same as scan
    mock_scanner_instance.update_scan.side_effect = mock_update_scan_writes_cache

    result = run_cli_command(["update"]) # Uses patches from run_cli_command

    cache_file = get_plugins_cache_file()
    assert cache_file.exists()
    with open(cache_file, 'r') as f:
        data_from_cache = json.load(f)
    assert data_from_cache == MOCK_PLUGIN_DATA # Assuming update wrote this

    mock_scanner_instance.update_scan.assert_called_once()


# TODO: Test for verbose logging options (--verbose=1, --verbose=2)
# TODO: Test for --extra-folders option with scan and update
# TODO: Test scan/update when cache already exists (for update's diff logic, though that's scanner internal)
# TODO: Test error conditions (e.g., unparseable cache, permissions issues - harder to mock)

# Note: The run_cli_command helper and its patches are quite broad.
# For more targeted tests, especially for 'list', 'json', 'yaml',
# it might be better to directly call the CLI functions from __main__.py
# and mock their dependencies (like PedalboardPluginary instance) instead of using subprocess.
# However, subprocess tests the actual command-line invocation.
# The current `test_cli_list` and `test_cli_json_output`, `test_cli_yaml_output`
# have been changed to use subprocess.run directly.
</file>

<file path="tests/test_sqlite_performance.py">
"""Performance tests for SQLite cache backend."""

import time
import tempfile
import pytest
from pathlib import Path
from typing import Dict, List

from pedalboard_pluginary.cache.sqlite_backend import SQLiteCacheBackend
from pedalboard_pluginary.cache.json_backend import JSONCacheBackend
from pedalboard_pluginary.models import PluginInfo, PluginParameter


def create_test_plugin(plugin_id: str, name: str, manufacturer: str = "TestMfg") -> PluginInfo:
    """Create a test plugin for benchmarking."""
    return PluginInfo(
        id=plugin_id,
        name=name,
        path=Path(f"/test/path/{name.replace(' ', '_')}.plugin"),
        plugin_type="test",
        manufacturer=manufacturer,
        parameters=[
            PluginParameter(
                name=f"param_{i}",
                default_value=float(i),
                min_value=0.0,
                max_value=100.0,
                value=float(i)
            ) for i in range(10)  # 10 parameters per plugin
        ]
    )


def generate_test_plugins(count: int) -> Dict[str, PluginInfo]:
    """Generate test plugins for benchmarking."""
    plugins = {}
    manufacturers = ["TestMfg", "SynthCorp", "AudioTech", "PluginLabs", "MusicSoft"]
    
    for i in range(count):
        plugin_id = f"test_plugin_{i:04d}"
        name = f"Test Plugin {i}"
        manufacturer = manufacturers[i % len(manufacturers)]
        
        plugins[plugin_id] = create_test_plugin(plugin_id, name, manufacturer)
    
    return plugins


class TestSQLitePerformance:
    """Performance tests comparing SQLite vs JSON backends."""
    
    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield Path(temp_dir)
    
    @pytest.fixture
    def sqlite_backend(self, temp_dir):
        """Create SQLite backend for testing."""
        return SQLiteCacheBackend(temp_dir / "test.db")
    
    @pytest.fixture
    def json_backend(self, temp_dir):
        """Create JSON backend for testing."""
        return JSONCacheBackend(temp_dir / "test.json")
    
    @pytest.fixture
    def small_dataset(self):
        """Small dataset for basic tests."""
        return generate_test_plugins(100)
    
    @pytest.fixture
    def medium_dataset(self):
        """Medium dataset for performance tests."""
        return generate_test_plugins(500)
    
    @pytest.fixture
    def large_dataset(self):
        """Large dataset for scalability tests."""
        return generate_test_plugins(1000)
    
    def test_sqlite_save_performance(self, sqlite_backend, medium_dataset):
        """Test SQLite save performance."""
        start_time = time.time()
        sqlite_backend.save(medium_dataset)
        save_time = time.time() - start_time
        
        assert save_time < 2.0, f"SQLite save took {save_time:.2f}s, expected < 2.0s"
        
        # Verify data was saved
        stats = sqlite_backend.get_stats()
        assert stats['total_plugins'] == len(medium_dataset)
    
    def test_json_save_performance(self, json_backend, medium_dataset):
        """Test JSON save performance."""
        start_time = time.time()
        json_backend.save(medium_dataset)
        save_time = time.time() - start_time
        
        # JSON should be reasonably fast for medium datasets
        assert save_time < 3.0, f"JSON save took {save_time:.2f}s, expected < 3.0s"
    
    def test_sqlite_load_performance(self, sqlite_backend, medium_dataset):
        """Test SQLite load performance."""
        # First save the data
        sqlite_backend.save(medium_dataset)
        
        # Then test load performance
        start_time = time.time()
        loaded_plugins = sqlite_backend.load()
        load_time = time.time() - start_time
        
        assert load_time < 1.0, f"SQLite load took {load_time:.2f}s, expected < 1.0s"
        assert len(loaded_plugins) == len(medium_dataset)
    
    def test_json_load_performance(self, json_backend, medium_dataset):
        """Test JSON load performance."""
        # First save the data
        json_backend.save(medium_dataset)
        
        # Then test load performance
        start_time = time.time()
        loaded_plugins = json_backend.load()
        load_time = time.time() - start_time
        
        assert load_time < 2.0, f"JSON load took {load_time:.2f}s, expected < 2.0s"
        assert len(loaded_plugins) == len(medium_dataset)
    
    def test_sqlite_search_performance(self, sqlite_backend, medium_dataset):
        """Test SQLite search performance."""
        # Save test data
        sqlite_backend.save(medium_dataset)
        
        # Test search performance
        start_time = time.time()
        results = sqlite_backend.search("TestMfg", limit=50)
        search_time = time.time() - start_time
        
        assert search_time < 0.1, f"SQLite search took {search_time:.3f}s, expected < 0.1s"
        assert len(results) > 0, "Search should return results"
    
    def test_sqlite_filter_performance(self, sqlite_backend, medium_dataset):
        """Test SQLite filter performance."""
        # Save test data
        sqlite_backend.save(medium_dataset)
        
        # Test filter performance
        start_time = time.time()
        results = sqlite_backend.filter_by_type("test")
        filter_time = time.time() - start_time
        
        assert filter_time < 0.1, f"SQLite filter took {filter_time:.3f}s, expected < 0.1s"
        assert len(results) == len(medium_dataset), "All plugins should match filter"
    
    def test_sqlite_update_performance(self, sqlite_backend, small_dataset):
        """Test SQLite update performance."""
        # Save initial data
        sqlite_backend.save(small_dataset)
        
        # Test update performance
        plugin_id = list(small_dataset.keys())[0]
        plugin = small_dataset[plugin_id]
        
        start_time = time.time()
        sqlite_backend.update(plugin_id, plugin)
        update_time = time.time() - start_time
        
        assert update_time < 0.1, f"SQLite update took {update_time:.3f}s, expected < 0.1s"
    
    def test_scalability_comparison(self, sqlite_backend, json_backend, large_dataset):
        """Compare scalability between SQLite and JSON backends."""
        # Test SQLite with large dataset
        start_time = time.time()
        sqlite_backend.save(large_dataset)
        sqlite_save_time = time.time() - start_time
        
        start_time = time.time()
        sqlite_loaded = sqlite_backend.load()
        sqlite_load_time = time.time() - start_time
        
        # Test JSON with large dataset
        start_time = time.time()
        json_backend.save(large_dataset)
        json_save_time = time.time() - start_time
        
        start_time = time.time()
        json_loaded = json_backend.load()
        json_load_time = time.time() - start_time
        
        # SQLite should be competitive or better
        print(f"SQLite save: {sqlite_save_time:.2f}s, load: {sqlite_load_time:.2f}s")
        print(f"JSON save: {json_save_time:.2f}s, load: {json_load_time:.2f}s")
        
        # Verify both loaded correctly
        assert len(sqlite_loaded) == len(large_dataset)
        assert len(json_loaded) == len(large_dataset)
        
        # SQLite should be reasonably fast
        assert sqlite_save_time < 5.0, "SQLite save should be under 5 seconds"
        assert sqlite_load_time < 2.0, "SQLite load should be under 2 seconds"
    
    def test_memory_efficiency(self, sqlite_backend, large_dataset):
        """Test that SQLite backend is memory efficient."""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        
        # Measure memory before
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Save large dataset
        sqlite_backend.save(large_dataset)
        
        # Measure memory after
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        memory_increase = memory_after - memory_before
        
        # Memory increase should be reasonable (< 50MB for 1000 plugins)
        assert memory_increase < 50, f"Memory increase {memory_increase:.1f}MB too high"
        
        # Test that loading doesn't keep everything in memory
        loaded_plugins = sqlite_backend.load()
        memory_after_load = process.memory_info().rss / 1024 / 1024  # MB
        
        # Memory shouldn't increase much more after loading
        load_memory_increase = memory_after_load - memory_after
        assert load_memory_increase < 30, f"Load memory increase {load_memory_increase:.1f}MB too high"
        
        assert len(loaded_plugins) == len(large_dataset)
</file>

<file path="tests/test_utils.py">
import pytest
from pedalboard_pluginary.utils import ensure_folder
from pathlib import Path

def test_ensure_folder(tmp_path):
    test_folder = tmp_path / "test_folder"
    ensure_folder(test_folder)
    assert test_folder.exists()
</file>

<file path=".coveragerc">
# .coveragerc to control coverage.py
[run]
branch = True
source = pedalboard_pluginary
# omit = bad_file.py

[paths]
source =
    src/
    */site-packages/

[report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
</file>

<file path=".isort.cfg">
[settings]
profile = black
known_first_party = pedalboard_pluginary
</file>

<file path="build.sh">
#!/usr/bin/env bash
# this_file: build.sh

set -e # Exit on error

echo "🧹 Cleaning up previous builds..."
rm -rf build/ dist/ *.egg-info .eggs/ .pytest_cache/ .coverage .tox/ .mypy_cache/

echo "🔍 Running type checks with mypy..."
python -m mypy src/pedalboard_pluginary

echo "�� Running tests..."
PYTHONPATH=src pytest tests/ -p no:flake8 -p no:briefcase

echo "📦 Building package..."
python -m build

echo "🚀 Installing locally..."
pip install -e .

echo "✨ Build and installation complete!"
</file>

<file path="LICENSE.txt">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="src/pedalboard_pluginary/async_scanner.py">
"""
Async scanner implementation for concurrent plugin scanning.
"""

import asyncio
import logging
from pathlib import Path
from typing import AsyncIterator, List, Optional, TYPE_CHECKING

from .constants import PLUGIN_LOAD_TIMEOUT
from .models import PluginInfo
from .protocols import ProgressReporter
from .timeout import TimeoutError

if TYPE_CHECKING:
    from .protocols import PluginScanner

logger = logging.getLogger(__name__)


class AsyncScannerMixin:
    """Mixin to add async capabilities to scanners.
    
    This mixin expects to be mixed with a class that implements the PluginScanner protocol.
    """
    
    async def scan_plugin_async(self, path: Path) -> Optional[PluginInfo]:
        """Async wrapper for plugin scanning with timeout.
        
        Args:
            path: Path to the plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        try:
            # Use asyncio.to_thread for CPU-bound plugin loading
            loop = asyncio.get_event_loop()
            return await asyncio.wait_for(
                loop.run_in_executor(None, self.scan_plugin, path),  # type: ignore[attr-defined]
                timeout=PLUGIN_LOAD_TIMEOUT
            )
        except asyncio.TimeoutError:
            logger.warning(f"Plugin {path} timed out during async scan")
            return None
        except Exception as e:
            logger.error(f"Error in async plugin scan for {path}: {e}")
            return None
    
    async def scan_plugins_batch(
        self, 
        paths: List[Path], 
        max_concurrent: int = 10,
        progress_reporter: Optional[ProgressReporter] = None
    ) -> AsyncIterator[PluginInfo]:
        """Scan multiple plugins concurrently with backpressure control.
        
        Args:
            paths: List of plugin paths to scan.
            max_concurrent: Maximum number of concurrent scans.
            progress_reporter: Optional progress reporter.
            
        Yields:
            PluginInfo objects for successfully scanned plugins.
        """
        if not paths:
            return
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def scan_with_semaphore(path: Path) -> Optional[PluginInfo]:
            async with semaphore:
                return await self.scan_plugin_async(path)
        
        # Create tasks for all paths
        tasks = [scan_with_semaphore(path) for path in paths]
        
        # Start progress tracking
        if progress_reporter:
            progress_reporter.start(len(tasks), f"Scanning {len(tasks)} plugins")
        
        completed = 0
        successful = 0
        
        # Process tasks as they complete
        for coro in asyncio.as_completed(tasks):
            result = await coro
            completed += 1
            
            if result:
                successful += 1
                yield result
            
            # Update progress
            if progress_reporter:
                message = f"Completed {completed}/{len(tasks)} ({successful} successful)"
                progress_reporter.update(1, message)
        
        # Finish progress tracking
        if progress_reporter:
            progress_reporter.finish(f"Scan completed: {successful}/{len(tasks)} plugins")
    
    async def scan_directory_async(
        self, 
        directory: Path,
        max_concurrent: int = 10,
        progress_reporter: Optional[ProgressReporter] = None
    ) -> List[PluginInfo]:
        """Async scan of an entire directory.
        
        Args:
            directory: Directory to scan for plugins.
            max_concurrent: Maximum number of concurrent scans.
            progress_reporter: Optional progress reporter.
            
        Returns:
            List of successfully scanned plugins.
        """
        # Find plugin files
        plugin_files = self.find_plugin_files([directory])  # type: ignore[attr-defined]
        
        # Scan plugins concurrently
        plugins = []
        async for plugin in self.scan_plugins_batch(
            plugin_files, 
            max_concurrent=max_concurrent,
            progress_reporter=progress_reporter
        ):
            plugins.append(plugin)
        
        return plugins


class AsyncVST3Scanner(AsyncScannerMixin):
    """VST3 scanner with async capabilities.
    
    This is a placeholder for the async VST3 scanner that will inherit
    from both VST3Scanner and AsyncScannerMixin.
    """
    pass


class AsyncAUScanner(AsyncScannerMixin):
    """AU scanner with async capabilities.
    
    This is a placeholder for the async AU scanner that will inherit
    from both AUScanner and AsyncScannerMixin.
    """
    pass
</file>

<file path="src/pedalboard_pluginary/base_scanner.py">
"""
Base scanner class implementing common functionality for all plugin scanners.
"""

import logging
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Optional

from .models import PluginInfo
from .protocols import PluginScanner

logger = logging.getLogger(__name__)


class BaseScanner(ABC):
    """Base class for plugin scanner implementations."""
    
    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
    ):
        """Initialize the scanner with optional ignore paths and specific paths.
        
        Args:
            ignore_paths: List of regex patterns for paths to ignore.
            specific_paths: List of specific paths to scan (if provided, only these are scanned).
        """
        self.ignore_paths = ignore_paths or []
        self.specific_paths = specific_paths or []
        self._compiled_ignore_patterns = [re.compile(pattern) for pattern in self.ignore_paths]
    
    @property
    @abstractmethod
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles (e.g., 'vst3', 'aufx')."""
        ...
    
    @property
    @abstractmethod
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        ...
    
    @abstractmethod
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all plugin files of this scanner's type.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to plugin files found.
        """
        ...
    
    @abstractmethod
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a single plugin file and return its information.
        
        Args:
            path: Path to the plugin file to scan.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        ...
    
    def validate_plugin_path(self, path: Path) -> bool:
        """Validate if a path is a valid plugin for this scanner.
        
        Args:
            path: Path to validate.
            
        Returns:
            True if the path is a valid plugin file, False otherwise.
        """
        if not path.exists():
            return False
        
        # Check extension
        if path.suffix not in self.supported_extensions:
            return False
        
        # Check against ignore patterns
        if self._should_ignore_path(path):
            return False
        
        # Check if specific paths are set and this path is in them
        if self.specific_paths and str(path) not in self.specific_paths:
            return False
        
        return True
    
    def _should_ignore_path(self, path: Path) -> bool:
        """Check if a path should be ignored based on ignore patterns.
        
        Args:
            path: Path to check.
            
        Returns:
            True if the path should be ignored, False otherwise.
        """
        path_str = str(path)
        for pattern in self._compiled_ignore_patterns:
            if pattern.search(path_str):
                logger.debug(f"Ignoring path {path} due to pattern {pattern.pattern}")
                return True
        return False
    
    def _filter_plugin_paths(self, paths: List[Path]) -> List[Path]:
        """Filter plugin paths based on validation criteria.
        
        Args:
            paths: List of paths to filter.
            
        Returns:
            Filtered list of valid plugin paths.
        """
        valid_paths = []
        for path in paths:
            if self.validate_plugin_path(path):
                valid_paths.append(path)
            else:
                logger.debug(f"Filtered out invalid plugin path: {path}")
        
        return valid_paths
    
    def _create_plugin_id(self, path: Path) -> str:
        """Create a unique plugin ID from its path.
        
        Args:
            path: Path to the plugin.
            
        Returns:
            Unique plugin ID string.
        """
        return f"{self.plugin_type}/{path.stem}"
</file>

<file path="src/pedalboard_pluginary/models.py">
# pedalboard_pluginary/models.py
"""
Dataclasses for representing plugin information.
"""
from dataclasses import dataclass, field
from pathlib import Path
from typing import Union, Dict, Optional, Any

# ParameterValue is what we store (after conversion from pedalboard's raw param value)
ParameterValue = Union[float, bool, str]

@dataclass
class PluginParameter:
    """Represents a single parameter of a plugin."""
    name: str
    value: ParameterValue
    # Optional: Add other attributes like min_value, max_value, string_value, etc.
    # if Pedalboard consistently provides them and they are useful to store.
    # For now, keeping it simple with just name and current (default) value.
    # raw_pedalboard_value: Any # Could store the original pedalboard value if needed for debugging

@dataclass
class PluginInfo:
    """Represents a scanned audio plugin."""
    # Unique key for this plugin, e.g., "vst3/FabFilter Pro-Q 3" or "aufx/ChannelEQ"
    # This key might be different from `name` if a file contains multiple plugins or
    # if the user-facing name has characters not suitable for a key.
    # This will be the key in the main dictionary of plugins.
    id: str

    name: str # The display name of the plugin
    path: str # Path to the plugin file or bundle (as string for JSON serialization)
    filename: str # Filename of the plugin (e.g., "FabFilter Pro-Q 3.vst3")
    plugin_type: str # "vst3" or "aufx"

    # Parameters: dict where key is param name, value is PluginParameter object
    parameters: Dict[str, PluginParameter] = field(default_factory=dict)

    manufacturer: Optional[str] = None # Optional: Plugin manufacturer name

    # Optional: If a plugin file (e.g. VST3) can contain multiple uniquely identifiable
    # plugins, this field could store the specific name used to load this plugin
    # from the file, if different from the main `name`.
    # E.g. `pedalboard.load_plugin(path, plugin_name=name_in_file)`
    name_in_file: Optional[str] = None

    def __post_init__(self) -> None:
        # Ensure path is stored as a string for easier JSON serialization
        # Note: self.path is already typed as str, so this check is defensive
        pass

    # Consider adding methods for to_dict/from_dict if needed for complex serialization,
    # though dataclasses.asdict and direct instantiation usually suffice.

# Example usage:
# if __name__ == "__main__":
#     eq_param = PluginParameter(name="Frequency", value=1000.0)
#     gain_param = PluginParameter(name="Gain", value=0.0)
#     bypass_param = PluginParameter(name="Bypass", value=False)

#     example_plugin = PluginInfo(
#         id="vst3/AwesomeEQ",
#         name="Awesome EQ",
#         path="/path/to/AwesomeEQ.vst3",
#         filename="AwesomeEQ.vst3",
#         plugin_type="vst3",
#         parameters={
#             "Frequency": eq_param,
#             "Gain": gain_param,
#             "Bypass": bypass_param
#         },
#         manufacturer="MyPluginCompany"
#     )
#     import json
#     from dataclasses import asdict
#     print(json.dumps(asdict(example_plugin), indent=2))
</file>

<file path="src/pedalboard_pluginary/progress.py">
"""
Progress reporting implementations.
"""

import logging
from typing import Any, Callable, Optional

from tqdm import tqdm

from .protocols import ProgressReporter

logger = logging.getLogger(__name__)


class TqdmProgress(ProgressReporter):
    """Progress reporter using tqdm progress bars."""
    
    def __init__(self) -> None:
        """Initialize the progress reporter."""
        self._pbar: Optional[tqdm[Any]] = None
        self._total: int = 0
        self._current: int = 0
        self._description: str = ""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        self._description = description
        self._pbar = tqdm(total=total, desc=description)
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        if self._pbar is None:
            return
        
        self._current += amount
        self._pbar.update(amount)
        
        if message and hasattr(self._pbar, 'set_description'):
            # Update description with message
            self._pbar.set_description(f"{self._description} - {message}")
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        if self._pbar is None:
            return
        
        # Ensure we reach 100%
        if self._current < self._total:
            self._pbar.update(self._total - self._current)
        
        if message and hasattr(self._pbar, 'set_description'):
            # Update description with message
            self._pbar.set_description(f"{self._description} - {message}")
        
        self._pbar.close()
        self._pbar = None


class NoOpProgress(ProgressReporter):
    """No-operation progress reporter for quiet mode."""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking (no-op)."""
        pass
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress (no-op)."""
        pass
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking (no-op)."""
        pass


class LogProgress(ProgressReporter):
    """Progress reporter that logs to standard logging."""
    
    def __init__(self, log_level: int = logging.INFO):
        """Initialize the progress reporter.
        
        Args:
            log_level: Logging level to use for progress messages.
        """
        self._log_level = log_level
        self._total: int = 0
        self._current: int = 0
        self._description: str = ""
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        self._description = description
        
        logger.log(
            self._log_level,
            f"Starting: {description} (0/{total})"
        )
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        self._current += amount
        
        progress_pct = (self._current / self._total * 100) if self._total > 0 else 0
        status = f"{self._description}: {self._current}/{self._total} ({progress_pct:.1f}%)"
        
        if message:
            status += f" - {message}"
        
        logger.log(self._log_level, status)
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        status = f"Completed: {self._description}"
        if message:
            status += f" - {message}"
        
        logger.log(self._log_level, status)


class CallbackProgress(ProgressReporter):
    """Progress reporter that calls user-provided callbacks."""
    
    def __init__(
        self,
        on_start: Optional[Callable[[int, str], None]] = None,
        on_update: Optional[Callable[[int, int, Optional[str]], None]] = None,
        on_finish: Optional[Callable[[Optional[str]], None]] = None,
    ):
        """Initialize the progress reporter with callbacks.
        
        Args:
            on_start: Callback for start(total, description).
            on_update: Callback for update(current, total, message).
            on_finish: Callback for finish(message).
        """
        self._on_start = on_start
        self._on_update = on_update
        self._on_finish = on_finish
        self._total: int = 0
        self._current: int = 0
    
    def start(self, total: int, description: str = "") -> None:
        """Start progress tracking.
        
        Args:
            total: Total number of items to process.
            description: Optional description of the operation.
        """
        self._total = total
        self._current = 0
        
        if self._on_start:
            self._on_start(total, description)
    
    def update(self, amount: int = 1, message: Optional[str] = None) -> None:
        """Update progress.
        
        Args:
            amount: Number of items completed (default: 1).
            message: Optional status message.
        """
        self._current += amount
        
        if self._on_update:
            self._on_update(self._current, self._total, message)
    
    def finish(self, message: Optional[str] = None) -> None:
        """Finish progress tracking.
        
        Args:
            message: Optional completion message.
        """
        if self._on_finish:
            self._on_finish(message)
</file>

<file path="tests/scanners/test_vst3_scanner.py">
# tests/scanners/test_vst3_scanner.py
import os
import platform
from pathlib import Path
from unittest.mock import MagicMock, mock_open, patch

import pytest

from pedalboard_pluginary.scanners.vst3_scanner import VST3Scanner


# Helper to create dummy VST3 files and folders
def create_dummy_vst3_structure(tmp_path, structure):
    """
    Creates a dummy VST3 plugin directory structure.
    structure is a dict like:
    {
        "folder_name": ["plugin1.vst3", "plugin2.vst3", {"subfolder": ["plugin3.vst3"]}]
    }
    """
    for name, contents in structure.items():
        current_path = tmp_path / name
        current_path.mkdir(parents=True, exist_ok=True)
        for item in contents:
            if isinstance(item, str):  # It's a file
                (current_path / item).touch()
            elif isinstance(item, dict):  # It's a sub-structure
                create_dummy_vst3_structure(current_path, item)


@pytest.fixture
def vst3_scanner_instance():
    return VST3Scanner()


@pytest.fixture
def vst3_scanner_with_ignores_instance():
    return VST3Scanner(ignore_paths=["vst3/IgnoredPlugin"])


class TestVST3Scanner:
    @patch("platform.system", return_value="Windows")
    @patch.dict(
        os.environ,
        {
            "ProgramFiles": "C:\\Program Files",
            "ProgramFiles(x86)": "C:\\Program Files (x86)",
        },
    )
    def test_get_default_vst3_folders_windows(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        # Create dummy common VST3 folders for Windows
        win_vst3_path1 = tmp_path / "Program Files" / "Common Files" / "VST3"
        win_vst3_path1.mkdir(parents=True, exist_ok=True)
        win_vst3_path2 = tmp_path / "Program Files (x86)" / "Common Files" / "VST3"
        win_vst3_path2.mkdir(parents=True, exist_ok=True)

        # Patch os.getenv to return mocked ProgramFiles paths relative to tmp_path
        def mock_getenv_windows(var_name, default=None):
            if var_name == "ProgramFiles":
                return str(tmp_path / "Program Files")
            if var_name == "ProgramFiles(x86)":
                return str(tmp_path / "Program Files (x86)")
            return default

        with patch("os.getenv", side_effect=mock_getenv_windows):
            folders = vst3_scanner_instance._get_default_vst3_folders()
            assert Path(win_vst3_path1).resolve() in folders
            assert Path(win_vst3_path2).resolve() in folders

    @patch("platform.system", return_value="Darwin")
    def test_get_default_vst3_folders_macos(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        mac_vst3_path1 = tmp_path / "Library" / "Audio" / "Plug-Ins" / "VST3"  # System
        mac_vst3_path1.mkdir(parents=True, exist_ok=True)
        # User path needs to be mocked for expanduser
        user_home_vst3_path = (
            tmp_path / "Users" / "testuser" / "Library" / "Audio" / "Plug-Ins" / "VST3"
        )
        user_home_vst3_path.mkdir(parents=True, exist_ok=True)

        with (
            patch("pathlib.Path.home", return_value=tmp_path / "Users" / "testuser"),
            patch(
                "pathlib.Path.expanduser",
                side_effect=lambda p: p
                if not str(p).startswith("~")
                else user_home_vst3_path,
            ),
        ):
            # Mock /Library path to point to our tmp_path version
            original_path_init = Path.__init__

            def mocked_path_init(self, *args, **kwargs):
                if args and args[0] == "/Library/Audio/Plug-Ins/VST3":
                    args = (str(mac_vst3_path1),) + args[1:]
                original_path_init(self, *args, **kwargs)

            with patch("pathlib.Path.__init__", mocked_path_init):
                folders = vst3_scanner_instance._get_default_vst3_folders()
                assert user_home_vst3_path.resolve() in folders
                assert mac_vst3_path1.resolve() in folders

    @patch("platform.system", return_value="Linux")
    def test_get_default_vst3_folders_linux(
        self, mock_platform_system, vst3_scanner_instance, tmp_path
    ):
        linux_vst3_path1 = tmp_path / ".vst3"  # User
        linux_vst3_path1.mkdir(parents=True, exist_ok=True)
        linux_vst3_path2 = tmp_path / "usr" / "lib" / "vst3"  # System
        linux_vst3_path2.mkdir(parents=True, exist_ok=True)

        with (
            patch("pathlib.Path.home", return_value=tmp_path),
            patch(
                "pathlib.Path.expanduser",
                side_effect=lambda p: p
                if not str(p).startswith("~")
                else linux_vst3_path1,
            ),
        ):
            # Mock /usr/lib/vst3 to point to our tmp_path version
            original_path_init = Path.__init__

            def mocked_path_init(self, *args, **kwargs):
                if args and args[0] == "/usr/lib/vst3":
                    args = (str(linux_vst3_path2),) + args[1:]
                elif (
                    args and args[0] == "/usr/local/lib/vst3"
                ):  # Also mock this common path
                    args = (str(tmp_path / "usr" / "local" / "lib" / "vst3"),) + args[
                        1:
                    ]
                    (tmp_path / "usr" / "local" / "lib" / "vst3").mkdir(
                        parents=True, exist_ok=True
                    )

                original_path_init(self, *args, **kwargs)

            with patch("pathlib.Path.__init__", mocked_path_init):
                folders = vst3_scanner_instance._get_default_vst3_folders()
                assert linux_vst3_path1.resolve() in folders
                assert linux_vst3_path2.resolve() in folders

    def test_find_plugin_files_discovery(self, vst3_scanner_instance, tmp_path):
        # Create a dummy default folder and put some plugins in it
        default_folder = tmp_path / "DefaultVST3s"
        default_folder.mkdir()
        (default_folder / "PluginA.vst3").touch()
        (default_folder / "PluginB.vst3").touch()

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 2
            assert default_folder / "PluginA.vst3" in found_plugins
            assert default_folder / "PluginB.vst3" in found_plugins

    def test_find_plugin_files_with_extra_folders(
        self, vst3_scanner_instance, tmp_path
    ):
        extra_folder1 = tmp_path / "ExtraVST3s1"
        extra_folder1.mkdir()
        (extra_folder1 / "PluginC.vst3").touch()

        extra_folder2 = tmp_path / "ExtraVST3s2"  # Non-existent

        # Mock default folders to be empty to isolate test to extra_folders
        with patch.object(VST3Scanner, "_get_default_vst3_folders", return_value=[]):
            found_plugins = vst3_scanner_instance.find_plugin_files(
                extra_folders=[str(extra_folder1), str(extra_folder2)]
            )
            assert len(found_plugins) == 1
            assert extra_folder1 / "PluginC.vst3" in found_plugins

    def test_find_plugin_files_with_specific_paths(
        self, vst3_scanner_instance, tmp_path
    ):
        plugin_path1 = tmp_path / "SpecificPlugin1.vst3"
        plugin_path1.touch()
        plugin_path2 = tmp_path / "SpecificPlugin2.vst3"  # Non-existent for this call

        found_plugins = vst3_scanner_instance.find_plugin_files(
            plugin_paths=[plugin_path1, plugin_path2]
        )
        assert len(found_plugins) == 1
        assert (
            plugin_path1 in found_plugins
        )  # plugin_path2 should not be found as it doesn't exist yet

    def test_find_plugin_files_with_ignores(
        self, vst3_scanner_with_ignores_instance, tmp_path
    ):
        default_folder = tmp_path / "VST3WithIgnores"
        default_folder.mkdir()
        (default_folder / "NormalPlugin.vst3").touch()
        (
            default_folder / "IgnoredPlugin.vst3"
        ).touch()  # This one has stem "IgnoredPlugin"

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_with_ignores_instance.find_plugin_files()
            assert len(found_plugins) == 1
            assert default_folder / "NormalPlugin.vst3" in found_plugins
            assert default_folder / "IgnoredPlugin.vst3" not in found_plugins

    def test_find_plugin_files_no_folders_exist(self, vst3_scanner_instance):
        with patch.object(VST3Scanner, "_get_default_vst3_folders", return_value=[]):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 0

    def test_find_plugin_files_skips_directories_with_vst3_suffix(
        self, vst3_scanner_instance, tmp_path
    ):
        default_folder = tmp_path / "VST3WithDirs"
        default_folder.mkdir()
        (default_folder / "RealPlugin.vst3").touch()
        (default_folder / "FakePlugin.vst3").mkdir()  # A directory named like a plugin

        with patch.object(
            VST3Scanner, "_get_default_vst3_folders", return_value=[default_folder]
        ):
            found_plugins = vst3_scanner_instance.find_plugin_files()
            assert len(found_plugins) == 1
            assert default_folder / "RealPlugin.vst3" in found_plugins
            assert default_folder / "FakePlugin.vst3" not in found_plugins


# TODO: Test case where a plugin_path provided to find_plugin_files is a directory (should be ignored)
# TODO: Test case with symlinks if relevant (Path.resolve() should handle them, but good to be aware)
# TODO: Test case for duplicate plugin paths from overlapping folder definitions (should be unique)
#       (find_plugin_files uses a set internally for discovery before sorting, so this should be handled)
</file>

<file path="tests/test_data.py">
import os
from pedalboard_pluginary.data import get_cache_path
from unittest.mock import patch

def test_get_cache_path_windows():
    with patch.dict(os.environ, {"APPDATA": "C:\\Users\\TestUser\\AppData"}):
        path = get_cache_path("test_cache")
        assert str(path) == "C:\\Users\\TestUser\\AppData\\com.twardoch.pedalboard-pluginary\\test_cache.json"

@patch('platform.system', return_value='Darwin')
def test_get_cache_path_macos(mock_platform_system):
    # Test for macOS when APPDATA is not set (should not be used)
    # and XDG_CACHE_HOME is not set (should not be used)
    with patch.dict(os.environ, {}, clear=True):
        path = get_cache_path("test_cache")
        home = os.path.expanduser("~")
        expected_path = f"{home}/Library/Application Support/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path

@patch('platform.system', return_value='Linux')
def test_get_cache_path_linux_xdg_set(mock_platform_system):
    xdg_cache_dir = "/custom/xdg/cache"
    with patch.dict(os.environ, {"XDG_CACHE_HOME": xdg_cache_dir}, clear=True):
        path = get_cache_path("test_cache")
        expected_path = f"{xdg_cache_dir}/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path

@patch('platform.system', return_value='Linux')
def test_get_cache_path_linux_xdg_not_set(mock_platform_system):
    # Test when XDG_CACHE_HOME is not set
    with patch.dict(os.environ, {}, clear=True): # Ensure XDG_CACHE_HOME is not set
        path = get_cache_path("test_cache")
        home = os.path.expanduser("~")
        expected_path = f"{home}/.cache/com.twardoch.pedalboard-pluginary/test_cache.json"
        assert str(path) == expected_path
</file>

<file path=".gitignore">
temp/

# Temporary and binary files
*~
*.py[cod]
*.so
*.cfg
!.isort.cfg
!setup.cfg
*.orig
*.log
*.pot
__pycache__/*
.cache/*
.*.swp
*/.ipynb_checkpoints/*
.DS_Store

# Project files
.ropeproject
.project
.pydevproject
.settings
.idea
.vscode
tags

# Package files
*.egg
*.eggs/
.installed.cfg
*.egg-info

# Unittest and coverage
htmlcov/*
.coverage
.coverage.*
.tox
junit*.xml
coverage.xml
.pytest_cache/

# Build and docs folder/files
build/*
dist/*
sdist/*
docs/api/*
docs/_rst/*
docs/_build/*
cover/*
MANIFEST

# Per-project virtualenvs
.venv*/
.conda*/
.python-version
</file>

<file path=".pre-commit-config.yaml">
exclude: '^docs/conf.py'

repos:
- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v4.5.0
  hooks:
  - id: trailing-whitespace
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
  - id: debug-statements
  - id: end-of-file-fixer
  - id: requirements-txt-fixer
  - id: mixed-line-ending
    args: ['--fix=auto']  # replace 'auto' with 'lf' to enforce Linux/Mac line endings or 'crlf' for Windows

## If you want to automatically "modernize" your Python code:
# - repo: https://github.com/asottile/pyupgrade
#   rev: v3.7.0
#   hooks:
#   - id: pyupgrade
#     args: ['--py37-plus']

## If you want to avoid flake8 errors due to unused vars or imports:
# - repo: https://github.com/PyCQA/autoflake
#   rev: v2.1.1
#   hooks:
#   - id: autoflake
#     args: [
#       --in-place,
#       --remove-all-unused-imports,
#       --remove-unused-variables,
#     ]

- repo: https://github.com/PyCQA/isort
  rev: 5.12.0
  hooks:
  - id: isort

- repo: https://github.com/psf/black
  rev: 23.11.0
  hooks:
  - id: black
    language_version: python3

## If like to embrace black styles even in the docs:
# - repo: https://github.com/asottile/blacken-docs
#   rev: v1.13.0
#   hooks:
#   - id: blacken-docs
#     additional_dependencies: [black]

- repo: https://github.com/PyCQA/flake8
  rev: 6.1.0
  hooks:
  - id: flake8
  ## You can add flake8 plugins via `additional_dependencies`:
  #  additional_dependencies: [flake8-bugbear]

- repo: https://github.com/pre-commit/mirrors-mypy
  rev: v1.7.0 # Or choose the latest version
  hooks:
  - id: mypy
    # You might need to specify `additional_dependencies` for mypy to find your project's dependencies
    # e.g., additional_dependencies: [types-setuptools, types-requests]
    # For this project:
    additional_dependencies: [
      types-setuptools, # For pkg_resources, etc.
      # Add stubs for other dependencies if mypy complains and they exist
      # types-fire, types-tqdm, types-python-benedict might not exist or be mature.
      # For now, we'll rely on inline # type: ignore for problematic libs
      # and the mypy config in pyproject.toml for global settings.
      "pedalboard", # To make mypy aware of pedalboard, even if it has no stubs
      "fire",
      "tqdm",
      "python-benedict"
    ]
    # It's good practice to also configure mypy via pyproject.toml or mypy.ini
    # For example, to specify the Python version, follow imports, etc.
    args: [--config-file=pyproject.toml] # Point to pyproject.toml for config

## Check for misspells in documentation files:
# - repo: https://github.com/codespell-project/codespell
#   rev: v2.2.5
#   hooks:
#   - id: codespell
</file>

<file path="AUTHORS.md">
# Contributors

* Adam Twardoch <adam+github@twardoch.com>
</file>

<file path="tox.ini">
# Tox configuration file
# Read more under https://tox.wiki/
# THIS SCRIPT IS SUPPOSED TO BE AN EXAMPLE. MODIFY IT ACCORDING TO YOUR NEEDS!

[tox]
minversion = 3.24
envlist = default
isolated_build = True

[testenv]
description = Invoke pytest to run automated tests
setenv =
    TOXINIDIR = {toxinidir}
passenv =
    HOME
    SETUPTOOLS_*
extras =
    testing
commands =
    pytest {posargs}

# To run `tox -e lint` you need to make sure you have a
# `.pre-commit-config.yaml` file. See https://pre-commit.com
# [testenv:lint]
# description = Perform static analysis and style checks
# skip_install = True
# deps = pre-commit
# passenv =
#     HOMEPATH
#     PROGRAMDATA
#     SETUPTOOLS_*
# commands =
#     pre-commit run --all-files {posargs:--show-diff-on-failure}

[testenv:{build,clean}]
description =
    build: Build the package in isolation according to PEP517, see https://github.com/pypa/build
    clean: Remove old distribution files and temporary build artifacts (./build and ./dist)
skip_install = True
changedir = {toxinidir}
deps =
    build: build[virtualenv]
passenv =
    SETUPTOOLS_*
commands =
    clean: python -c 'import shutil; [shutil.rmtree(p, True) for p in ("build", "dist")]'
    clean: python -c 'import pathlib, shutil; [shutil.rmtree(p, True) for p in pathlib.Path("src").glob("*.egg-info")]'
    build: python -m build {posargs}

[testenv:publish]
description =
    Publish the package you have been developing to a package index server.
    By default, it uses testpypi. If you really want to publish your package
    to be publicly accessible in PyPI, use the `-- --repository pypi` option.
skip_install = True
changedir = {toxinidir}
passenv =
    TWINE_USERNAME
    TWINE_PASSWORD
    TWINE_REPOSITORY
    TWINE_REPOSITORY_URL
deps = twine
commands =
    python -m twine check dist/*
    python -m twine upload {posargs:--repository {env:TWINE_REPOSITORY:testpypi}} dist/*
</file>

<file path=".github/workflows/ci.yml">
name: Python package CI

on:
  push:
    branches: [main]
    tags: ['v*']
  pull_request:
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install fire pytest pytest-cov # Added pytest-cov for coverage
    - name: Run tests with coverage
      run: |
        python -m pip install -e .
        # Pytest is configured in pyproject.toml to run with --cov
        # and output to term-missing. It also creates .coverage file.
        pytest
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        # token: ${{ secrets.CODECOV_TOKEN }} # Only if needed for private repos or specific cases
        fail_ci_if_error: true # Optional: fail CI if coverage upload fails

  publish:
    needs: build-and-test
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    - name: Build and publish
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
      run: |
        python -m pip install --upgrade build twine
        python -m build
        twine upload dist/*
</file>

<file path="src/pedalboard_pluginary/__init__.py">
from importlib.metadata import PackageNotFoundError, version

try:
    # Change here if project is renamed and does not equal the package name
    dist_name = __name__
    __version__ = version(dist_name)
except PackageNotFoundError:  # pragma: no cover
    __version__ = "unknown"
finally:
    del version, PackageNotFoundError

from .core import PedalboardPluginary
</file>

<file path="src/pedalboard_pluginary/constants.py">
"""
Constants and configuration values for pedalboard_pluginary.
"""

from typing import Final

# Application metadata
APP_NAME: Final[str] = "com.twardoch.pedalboard-pluginary"
APP_VERSION: Final[str] = "0.1.0"  # TODO: Get from package metadata

# Cache configuration
CACHE_VERSION: Final[str] = "2.0.0"
PLUGINS_CACHE_FILENAME: Final[str] = "plugins"
IGNORES_CACHE_FILENAME: Final[str] = "ignores"

# Scanner configuration
DEFAULT_SCAN_TIMEOUT: Final[int] = 10  # seconds
PLUGIN_LOAD_TIMEOUT: Final[float] = 10.0  # seconds for individual plugin loading
MAX_SCAN_RETRIES: Final[int] = 3
SCAN_RETRY_DELAY: Final[float] = 1.0  # seconds

# Async scanning configuration
DEFAULT_MAX_CONCURRENT: Final[int] = 10  # concurrent async scans
MIN_CONCURRENT_SCANS: Final[int] = 1
MAX_CONCURRENT_SCANS: Final[int] = 50

# Plugin types
PLUGIN_TYPE_VST3: Final[str] = "vst3"
PLUGIN_TYPE_AU: Final[str] = "aufx"
SUPPORTED_PLUGIN_TYPES: Final[list[str]] = [PLUGIN_TYPE_VST3, PLUGIN_TYPE_AU]

# File extensions
VST3_EXTENSION: Final[str] = ".vst3"
AU_EXTENSION: Final[str] = ".component"

# Platform names
PLATFORM_WINDOWS: Final[str] = "Windows"
PLATFORM_MACOS: Final[str] = "Darwin"
PLATFORM_LINUX: Final[str] = "Linux"

# Progress reporting
DEFAULT_PROGRESS_BAR_WIDTH: Final[int] = 80
PROGRESS_UPDATE_INTERVAL: Final[float] = 0.1  # seconds

# Logging configuration
LOG_FORMAT: Final[str] = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_DATE_FORMAT: Final[str] = "%Y-%m-%d %H:%M:%S"

# CLI configuration
DEFAULT_OUTPUT_FORMAT: Final[str] = "json"
SUPPORTED_OUTPUT_FORMATS: Final[list[str]] = ["json", "yaml", "table", "csv"]

# Resource paths
RESOURCES_PACKAGE: Final[str] = "pedalboard_pluginary.resources"
DEFAULT_IGNORES_FILENAME: Final[str] = "default_ignores.json"
</file>

<file path="src/pedalboard_pluginary/serialization.py">
"""
Unified serialization module for plugin data.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from .constants import APP_VERSION, CACHE_VERSION
from .exceptions import CacheCorruptedError, CacheVersionError, CacheWriteError
from .models import PluginInfo, PluginParameter
from .types import (
    CacheData,
    CacheMetadata,
    SerializedParameter,
    SerializedPlugin,
    is_serialized_parameter,
    is_serialized_plugin,
)
from .utils import ensure_folder

logger = logging.getLogger(__name__)


class PluginSerializer:
    """Handles serialization and deserialization of plugin data."""
    
    @staticmethod
    def parameter_to_dict(param: PluginParameter) -> SerializedParameter:
        """Convert PluginParameter to serializable dictionary.
        
        Args:
            param: PluginParameter object to serialize.
            
        Returns:
            SerializedParameter dictionary.
        """
        return {
            "name": param.name,
            "value": param.value,
        }
    
    @staticmethod
    def dict_to_parameter(data: Dict[str, Any]) -> Optional[PluginParameter]:
        """Convert dictionary to PluginParameter with validation.
        
        Args:
            data: Dictionary containing parameter data.
            
        Returns:
            PluginParameter object if valid, None otherwise.
        """
        if not is_serialized_parameter(data):
            logger.warning(f"Invalid parameter data: {data}")
            return None
        
        return PluginParameter(
            name=data["name"],
            value=data["value"],
        )
    
    @staticmethod
    def plugin_to_dict(plugin: PluginInfo) -> SerializedPlugin:
        """Convert PluginInfo to serializable dictionary.
        
        Args:
            plugin: PluginInfo object to serialize.
            
        Returns:
            SerializedPlugin dictionary.
        """
        # Convert parameters
        params_dict: Dict[str, SerializedParameter] = {}
        for param_name, param in plugin.parameters.items():
            params_dict[param_name] = PluginSerializer.parameter_to_dict(param)
        
        result: SerializedPlugin = {
            "id": plugin.id,
            "name": plugin.name,
            "path": plugin.path,
            "filename": plugin.filename,
            "plugin_type": plugin.plugin_type,
            "parameters": params_dict,
            "manufacturer": plugin.manufacturer,
            "name_in_file": plugin.name_in_file,
        }
        
        return result
    
    @staticmethod
    def dict_to_plugin(data: Dict[str, Any]) -> Optional[PluginInfo]:
        """Convert dictionary to PluginInfo with validation.
        
        Args:
            data: Dictionary containing plugin data.
            
        Returns:
            PluginInfo object if valid, None otherwise.
        """
        if not is_serialized_plugin(data):
            logger.warning(f"Invalid plugin data for ID: {data.get('id', 'unknown')}")
            return None
        
        # Convert parameters
        params: Dict[str, PluginParameter] = {}
        for param_name, param_data in data.get("parameters", {}).items():
            param = PluginSerializer.dict_to_parameter(param_data)
            if param:
                params[param_name] = param
        
        return PluginInfo(
            id=data["id"],
            name=data["name"],
            path=data["path"],
            filename=data["filename"],
            plugin_type=data["plugin_type"],
            parameters=params,
            manufacturer=data.get("manufacturer"),
            name_in_file=data.get("name_in_file"),
        )
    
    @classmethod
    def create_cache_metadata(cls, plugin_count: int) -> CacheMetadata:
        """Create cache metadata.
        
        Args:
            plugin_count: Number of plugins in the cache.
            
        Returns:
            CacheMetadata dictionary.
        """
        now = datetime.utcnow().isoformat()
        return {
            "version": CACHE_VERSION,
            "created_at": now,
            "updated_at": now,
            "plugin_count": plugin_count,
            "scanner_version": APP_VERSION,
        }
    
    @classmethod
    def save_plugins(cls, plugins: Dict[str, PluginInfo], path: Path) -> None:
        """Save plugins to JSON file with metadata and error handling.
        
        Args:
            plugins: Dictionary mapping plugin IDs to PluginInfo objects.
            path: Path to the cache file.
        """
        ensure_folder(path.parent)
        
        # Convert plugins to serializable format
        plugins_dict: Dict[str, SerializedPlugin] = {}
        for plugin_id, plugin in plugins.items():
            try:
                plugins_dict[plugin_id] = cls.plugin_to_dict(plugin)
            except Exception as e:
                logger.error(f"Failed to serialize plugin {plugin_id}: {e}")
                continue
        
        # Create cache data with metadata
        cache_data: CacheData = {
            "metadata": cls.create_cache_metadata(len(plugins_dict)),
            "plugins": plugins_dict,
        }
        
        # Write to file with error handling
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2)
            logger.info(f"Saved {len(plugins_dict)} plugins to {path}")
        except Exception as e:
            logger.error(f"Failed to save plugins to {path}: {e}")
            raise CacheWriteError(str(path), str(e))
    
    @classmethod
    def load_plugins(cls, path: Path) -> Dict[str, PluginInfo]:
        """Load plugins from JSON file with validation.
        
        Args:
            path: Path to the cache file.
            
        Returns:
            Dictionary mapping plugin IDs to PluginInfo objects.
        """
        if not path.exists():
            logger.info(f"Cache file not found: {path}")
            return {}
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in cache file {path}: {e}")
            raise CacheCorruptedError(str(path), f"JSON decode error: {e}")
        except Exception as e:
            logger.error(f"Failed to read cache file {path}: {e}")
            raise CacheCorruptedError(str(path), str(e))
        
        # Handle both old format (direct plugin dict) and new format (with metadata)
        if isinstance(data, dict) and "metadata" in data and "plugins" in data:
            # New format with metadata
            metadata = data.get("metadata", {})
            cache_version = metadata.get("version", "1.0.0")
            
            if cache_version != CACHE_VERSION:
                logger.warning(f"Cache version mismatch: expected {CACHE_VERSION}, got {cache_version}")
                raise CacheVersionError(CACHE_VERSION, cache_version, str(path))
            
            plugins_data = data.get("plugins", {})
        else:
            # Old format - direct plugin dictionary
            logger.info("Loading cache in legacy format")
            plugins_data = data
        
        # Convert to PluginInfo objects
        plugins: Dict[str, PluginInfo] = {}
        for plugin_id, plugin_data in plugins_data.items():
            if not isinstance(plugin_data, dict):
                logger.warning(f"Invalid plugin data for ID {plugin_id}")
                continue
            
            plugin = cls.dict_to_plugin(plugin_data)
            if plugin:
                plugins[plugin_id] = plugin
            else:
                logger.warning(f"Failed to deserialize plugin {plugin_id}")
        
        logger.info(f"Loaded {len(plugins)} plugins from {path}")
        return plugins
    
    @classmethod
    def migrate_cache(cls, old_data: Dict[str, Any], old_version: str, new_version: str) -> Dict[str, Any]:
        """Migrate cache data from old version to new version.
        
        Args:
            old_data: Cache data in old format.
            old_version: Version of the old cache format.
            new_version: Target version to migrate to.
            
        Returns:
            Migrated cache data.
        """
        # TODO: Implement cache migration logic as needed
        logger.info(f"Migrating cache from version {old_version} to {new_version}")
        return old_data
</file>

<file path="src/pedalboard_pluginary/types.py">
"""
Type definitions and aliases for the pedalboard_pluginary package.
"""

from typing import Union, Dict, Any, TypedDict, Optional
import sys

if sys.version_info >= (3, 11):
    from typing import NotRequired
else:
    from typing_extensions import NotRequired

# Basic type aliases
ParameterValue = Union[float, bool, str]
PluginID = str
PluginType = str  # "vst3" or "aufx"
PluginPath = str  # String representation of path for JSON serialization


class SerializedParameter(TypedDict):
    """TypedDict for serialized plugin parameter."""
    name: str
    value: ParameterValue


class SerializedPlugin(TypedDict):
    """TypedDict for serialized plugin data."""
    id: str
    name: str
    path: str
    filename: str
    plugin_type: str
    parameters: Dict[str, SerializedParameter]
    manufacturer: NotRequired[Optional[str]]
    name_in_file: NotRequired[Optional[str]]


class CacheMetadata(TypedDict):
    """TypedDict for cache metadata."""
    version: str
    created_at: str
    updated_at: str
    plugin_count: int
    scanner_version: str


class CacheData(TypedDict):
    """TypedDict for complete cache data structure."""
    metadata: CacheMetadata
    plugins: Dict[str, SerializedPlugin]


# Type guards
def is_parameter_value(value: Any) -> bool:
    """Check if a value is a valid ParameterValue."""
    return isinstance(value, (float, bool, str))


def is_serialized_parameter(data: Any) -> bool:
    """Check if data is a valid SerializedParameter."""
    return (
        isinstance(data, dict) and
        "name" in data and
        "value" in data and
        isinstance(data["name"], str) and
        is_parameter_value(data["value"])
    )


def is_serialized_plugin(data: Any) -> bool:
    """Check if data is a valid SerializedPlugin."""
    if not isinstance(data, dict):
        return False
    
    required_fields = ["id", "name", "path", "filename", "plugin_type", "parameters"]
    for field in required_fields:
        if field not in data:
            return False
    
    # Check types of required fields
    if not all(isinstance(data[field], str) for field in ["id", "name", "path", "filename", "plugin_type"]):
        return False
    
    # Check parameters
    if not isinstance(data["parameters"], dict):
        return False
    
    for param_name, param_data in data["parameters"].items():
        if not isinstance(param_name, str) or not is_serialized_parameter(param_data):
            return False
    
    # Check optional fields
    if "manufacturer" in data and data["manufacturer"] is not None:
        if not isinstance(data["manufacturer"], str):
            return False
    
    if "name_in_file" in data and data["name_in_file"] is not None:
        if not isinstance(data["name_in_file"], str):
            return False
    
    return True
</file>

<file path="src/pedalboard_pluginary/utils.py">
from pathlib import Path
from typing import Any, Union

def ensure_folder(path: Path) -> None:
    """ Ensure that a folder exists. """
    path.parent.mkdir(parents=True, exist_ok=True)

def from_pb_param(data: Any) -> Union[float, bool, str]:
    """
    Converts a pedalboard parameter value to a Python native type.
    Pedalboard parameter values can be string representations of floats, booleans, or just strings.
    """
    drep = str(data)
    try:
        return float(drep)
    except ValueError:
        pass
    if drep.lower() == "true":
        return True
    if drep.lower() == "false":
        return False
    return drep
</file>

<file path="src/pedalboard_pluginary/core.py">
import json
from pathlib import Path
from typing import Dict

from .data import get_cache_path
from .models import PluginInfo
from .scanner import PedalboardScanner
from .serialization import PluginSerializer


class PedalboardPluginary:
    """Main class for the Pedalboard Pluginary application."""
    
    plugins_path: Path
    plugins: Dict[str, PluginInfo]

    def __init__(self) -> None:
        """Initialize the Pedalboard Pluginary instance."""
        self.plugins_path = get_cache_path("plugins")
        self.plugins = {}
        self.load_data()

    def load_data(self) -> None:
        """Load plugin data from cache or perform a scan if cache doesn't exist."""
        if not self.plugins_path.exists():
            scanner = PedalboardScanner()
            scanner.full_scan()

        # Load plugins using the serializer
        self.plugins = PluginSerializer.load_plugins(self.plugins_path)

    def list_plugins(self) -> str:
        """Returns a JSON string representation of the plugins."""
        # Convert PluginInfo objects to dictionaries for JSON serialization
        plugins_dict = {}
        for plugin_id, plugin in self.plugins.items():
            plugins_dict[plugin_id] = PluginSerializer.plugin_to_dict(plugin)
        
        return json.dumps(plugins_dict, indent=4)
</file>

<file path="PLAN.md">
# Pedalboard Pluginary - Production Readiness Implementation Plan

## Executive Summary

Based on comprehensive codebase analysis, Pedalboard Pluginary has achieved **exceptional async infrastructure** and **enterprise-grade type safety**. The async scanner implementation provides foundation for 10x performance improvements. Current focus must shift to **SQLite scalability**, **modern CLI experience**, and **production features** to complete the transformation into a high-performance, professional-grade tool.

## Current State Assessment

### ✅ **Major Achievements Completed**

**Async Performance Infrastructure (100% Complete)**
- AsyncScannerMixin with semaphore-based concurrency control
- Full integration in VST3Scanner and AUScanner classes  
- PedalboardScanner async methods (full_scan_async, update_scan_async)
- Configurable concurrency limits (1-50 concurrent scans)
- Progress reporting integration with real-time feedback

**Type Safety Excellence (100% Complete)**
- Zero mypy errors in strict mode across entire codebase
- Comprehensive protocol-based architecture
- Complete TypedDict definitions for serialization
- Robust custom exception hierarchy

**Foundation Infrastructure (100% Complete)**
- Timeout protection with sync_timeout and async_timeout
- Retry logic with exponential backoff
- Multiple progress reporter implementations
- Unified serialization layer with validation

### 🎯 **Critical Gaps Requiring Immediate Action**

**Performance Scalability Crisis**
- JSON cache becomes bottleneck at 500+ plugins (O(n) operations)
- No indexed search capabilities
- Missing lazy loading for large datasets
- Full rescans required (no incremental updates)

**User Experience Deficiencies**
- Outdated Fire-based CLI with poor error handling
- No search/filtering capabilities
- Missing rich output formatting
- No comprehensive help system

**Production Readiness Gaps**
- No configuration management system
- Missing health monitoring and error recovery
- No automated testing for async functionality
- Basic cache management without repair capabilities

## Phase 1: SQLite Cache Backend Revolution (Days 1-5)

### 1.1 Core SQLite Implementation

**Problem**: JSON cache hits performance wall at ~500 plugins with O(n) operations.

**Solution**: SQLite backend with indexing, FTS, and lazy loading.

```python
# src/pedalboard_pluginary/cache/__init__.py
from .sqlite_backend import SQLiteCacheBackend
from .json_backend import JSONCacheBackend  # Legacy compatibility
from .migration import migrate_json_to_sqlite

# src/pedalboard_pluginary/cache/sqlite_backend.py
import sqlite3
import json
from typing import Dict, Optional, Iterator, List
from pathlib import Path
from ..models import PluginInfo
from ..protocols import CacheBackend

class SQLiteCacheBackend(CacheBackend):
    """High-performance SQLite cache with indexing and FTS."""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_schema()
    
    def _init_schema(self) -> None:
        """Initialize optimized database schema."""
        with self._connect() as conn:
            conn.executescript("""
                -- Main plugins table with optimized indexes
                CREATE TABLE IF NOT EXISTS plugins (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    path TEXT NOT NULL UNIQUE,
                    plugin_type TEXT NOT NULL,
                    manufacturer TEXT,
                    parameter_count INTEGER NOT NULL,
                    data TEXT NOT NULL,  -- JSON blob for full plugin data
                    file_mtime REAL NOT NULL,
                    created_at REAL NOT NULL,
                    updated_at REAL NOT NULL
                );
                
                -- Performance indexes
                CREATE INDEX IF NOT EXISTS idx_plugins_name ON plugins(name);
                CREATE INDEX IF NOT EXISTS idx_plugins_type ON plugins(plugin_type);
                CREATE INDEX IF NOT EXISTS idx_plugins_manufacturer ON plugins(manufacturer);
                CREATE INDEX IF NOT EXISTS idx_plugins_path ON plugins(path);
                CREATE INDEX IF NOT EXISTS idx_plugins_mtime ON plugins(file_mtime);
                CREATE INDEX IF NOT EXISTS idx_plugins_param_count ON plugins(parameter_count);
                
                -- Full-text search for names and manufacturers
                CREATE VIRTUAL TABLE IF NOT EXISTS plugins_fts USING fts5(
                    id UNINDEXED,
                    name,
                    manufacturer,
                    content='plugins',
                    content_rowid='rowid'
                );
                
                -- FTS triggers for automatic index maintenance
                CREATE TRIGGER IF NOT EXISTS plugins_fts_insert AFTER INSERT ON plugins
                BEGIN
                    INSERT INTO plugins_fts(rowid, id, name, manufacturer)
                    VALUES (new.rowid, new.id, new.name, new.manufacturer);
                END;
                
                CREATE TRIGGER IF NOT EXISTS plugins_fts_delete AFTER DELETE ON plugins
                BEGIN
                    INSERT INTO plugins_fts(plugins_fts, rowid, id, name, manufacturer)
                    VALUES ('delete', old.rowid, old.id, old.name, old.manufacturer);
                END;
                
                CREATE TRIGGER IF NOT EXISTS plugins_fts_update AFTER UPDATE ON plugins
                BEGIN
                    INSERT INTO plugins_fts(plugins_fts, rowid, id, name, manufacturer)
                    VALUES ('delete', old.rowid, old.id, old.name, old.manufacturer);
                    INSERT INTO plugins_fts(rowid, id, name, manufacturer)
                    VALUES (new.rowid, new.id, new.name, new.manufacturer);
                END;
                
                -- Cache metadata table
                CREATE TABLE IF NOT EXISTS cache_metadata (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    updated_at REAL NOT NULL
                );
                
                -- Insert initial metadata
                INSERT OR IGNORE INTO cache_metadata (key, value, updated_at)
                VALUES 
                    ('version', '2.0.0', strftime('%s', 'now')),
                    ('created_at', strftime('%s', 'now'), strftime('%s', 'now'));
            """)
    
    def get(self, plugin_id: str) -> Optional[PluginInfo]:
        """Get single plugin without loading entire cache."""
        with self._connect() as conn:
            row = conn.execute(
                "SELECT data FROM plugins WHERE id = ?", 
                (plugin_id,)
            ).fetchone()
            
            if row:
                data = json.loads(row[0])
                return PluginSerializer.dict_to_plugin(data)
            return None
    
    def search(
        self,
        query: Optional[str] = None,
        plugin_type: Optional[str] = None,
        manufacturer: Optional[str] = None,
        parameter_count_range: Optional[tuple] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Iterator[PluginInfo]:
        """Advanced search with multiple filters and pagination."""
        conditions = []
        params = []
        
        # Full-text search
        if query:
            conditions.append("""
                id IN (
                    SELECT id FROM plugins_fts 
                    WHERE plugins_fts MATCH ?
                )
            """)
            params.append(query)
        
        # Type filter
        if plugin_type:
            conditions.append("plugin_type = ?")
            params.append(plugin_type)
        
        # Manufacturer filter (supports partial matching)
        if manufacturer:
            conditions.append("manufacturer LIKE ?")
            params.append(f"%{manufacturer}%")
        
        # Parameter count range
        if parameter_count_range:
            min_params, max_params = parameter_count_range
            conditions.append("parameter_count BETWEEN ? AND ?")
            params.extend([min_params, max_params])
        
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        
        sql = f"""
            SELECT data FROM plugins 
            WHERE {where_clause}
            ORDER BY name COLLATE NOCASE
            LIMIT ? OFFSET ?
        """
        params.extend([limit, offset])
        
        with self._connect() as conn:
            for row in conn.execute(sql, params):
                data = json.loads(row[0])
                plugin = PluginSerializer.dict_to_plugin(data)
                if plugin:
                    yield plugin
    
    def save(self, plugins: Dict[str, PluginInfo]) -> None:
        """Efficiently save plugins with transaction."""
        import time
        
        with self._connect() as conn:
            conn.execute("BEGIN TRANSACTION")
            try:
                for plugin_id, plugin in plugins.items():
                    data = PluginSerializer.plugin_to_dict(plugin)
                    
                    # Get file modification time
                    file_mtime = 0
                    try:
                        file_mtime = Path(plugin.path).stat().st_mtime
                    except (OSError, ValueError):
                        pass  # File might not exist or be invalid
                    
                    conn.execute("""
                        INSERT OR REPLACE INTO plugins 
                        (id, name, path, plugin_type, manufacturer, parameter_count,
                         data, file_mtime, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, 
                               COALESCE((SELECT created_at FROM plugins WHERE id = ?), ?),
                               ?)
                    """, (
                        plugin_id,
                        plugin.name,
                        plugin.path,
                        plugin.plugin_type,
                        plugin.manufacturer,
                        len(plugin.parameters),
                        json.dumps(data),
                        file_mtime,
                        plugin_id,  # For COALESCE lookup
                        time.time(),  # created_at if new
                        time.time()   # updated_at
                    ))
                
                conn.execute("COMMIT")
                
                # Update cache metadata
                conn.execute("""
                    UPDATE cache_metadata 
                    SET value = ?, updated_at = ?
                    WHERE key = 'last_updated'
                """, (str(time.time()), time.time()))
                
            except Exception:
                conn.execute("ROLLBACK")
                raise
    
    def delete(self, plugin_id: str) -> None:
        """Delete plugin efficiently."""
        with self._connect() as conn:
            conn.execute("DELETE FROM plugins WHERE id = ?", (plugin_id,))
    
    def clear(self) -> None:
        """Clear all plugins efficiently."""
        with self._connect() as conn:
            conn.execute("DELETE FROM plugins")
            # FTS table will be automatically updated by triggers
    
    def exists(self) -> bool:
        """Check if cache exists and has plugins."""
        if not self.db_path.exists():
            return False
        
        with self._connect() as conn:
            count = conn.execute("SELECT COUNT(*) FROM plugins").fetchone()[0]
            return count > 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics."""
        with self._connect() as conn:
            # Total count
            total = conn.execute("SELECT COUNT(*) FROM plugins").fetchone()[0]
            
            # By type
            type_stats = {}
            for row in conn.execute("""
                SELECT plugin_type, COUNT(*) 
                FROM plugins 
                GROUP BY plugin_type
            """):
                type_stats[row[0]] = row[1]
            
            # By manufacturer (top 10)
            manufacturer_stats = {}
            for row in conn.execute("""
                SELECT manufacturer, COUNT(*) 
                FROM plugins 
                WHERE manufacturer IS NOT NULL
                GROUP BY manufacturer 
                ORDER BY COUNT(*) DESC 
                LIMIT 10
            """):
                manufacturer_stats[row[0]] = row[1]
            
            # Database size
            db_size = 0
            if self.db_path.exists():
                db_size = self.db_path.stat().st_size
            
            # Last updated
            last_updated = conn.execute("""
                SELECT value FROM cache_metadata 
                WHERE key = 'last_updated'
            """).fetchone()
            
            return {
                "total_plugins": total,
                "by_type": type_stats,
                "top_manufacturers": manufacturer_stats,
                "database_size_bytes": db_size,
                "last_updated": last_updated[0] if last_updated else None
            }
    
    def get_changed_plugins(self, since_mtime: float) -> List[str]:
        """Get plugins that have changed since given modification time."""
        with self._connect() as conn:
            rows = conn.execute("""
                SELECT id FROM plugins 
                WHERE file_mtime > ?
                ORDER BY file_mtime DESC
            """, (since_mtime,)).fetchall()
            
            return [row[0] for row in rows]
    
    def vacuum(self) -> None:
        """Optimize database performance."""
        with self._connect() as conn:
            conn.execute("VACUUM")
            conn.execute("ANALYZE")
    
    def _connect(self) -> sqlite3.Connection:
        """Create optimized database connection."""
        # Ensure parent directory exists
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        
        # Performance optimizations
        conn.execute("PRAGMA journal_mode=WAL")        # Better concurrency
        conn.execute("PRAGMA synchronous=NORMAL")      # Balance safety/speed
        conn.execute("PRAGMA cache_size=-64000")       # 64MB cache
        conn.execute("PRAGMA temp_store=MEMORY")       # Memory temp tables
        conn.execute("PRAGMA mmap_size=268435456")     # 256MB memory mapping
        
        return conn
```

**Implementation Steps**:
1. Create cache package structure
2. Implement SQLiteCacheBackend with schema and indexes
3. Add comprehensive search capabilities with FTS
4. Create migration utility from JSON to SQLite
5. Add cache statistics and management methods

**Expected Impact**:
- **Search Performance**: O(log n) vs O(n) - 100x improvement for large datasets
- **Memory Usage**: Constant vs linear - handle unlimited plugin counts
- **Query Capabilities**: Full-text search, filtering, sorting, pagination
- **Scalability**: Handle 10,000+ plugins efficiently

### 1.2 JSON to SQLite Migration

```python
# src/pedalboard_pluginary/cache/migration.py
import logging
from pathlib import Path
from typing import Dict
from ..data import load_json_file
from ..models import PluginInfo
from ..serialization import PluginSerializer
from .sqlite_backend import SQLiteCacheBackend

logger = logging.getLogger(__name__)

class CacheMigration:
    """Handles migration from JSON to SQLite cache."""
    
    @staticmethod
    def migrate_json_to_sqlite(
        json_path: Path, 
        sqlite_path: Path,
        backup: bool = True
    ) -> bool:
        """Migrate existing JSON cache to SQLite format."""
        if not json_path.exists():
            logger.info("No JSON cache found, starting with empty SQLite cache")
            return True
        
        # Backup existing JSON if requested
        if backup:
            backup_path = json_path.with_suffix('.json.backup')
            backup_path.write_bytes(json_path.read_bytes())
            logger.info(f"JSON cache backed up to {backup_path}")
        
        try:
            # Load existing JSON data
            json_data = load_json_file(json_path)
            if not json_data:
                logger.info("Empty JSON cache, starting fresh")
                return True
            
            # Convert to PluginInfo objects
            plugins: Dict[str, PluginInfo] = {}
            for plugin_id, plugin_data in json_data.items():
                plugin = PluginSerializer.dict_to_plugin(plugin_data)
                if plugin:
                    plugins[plugin_id] = plugin
                else:
                    logger.warning(f"Failed to migrate plugin: {plugin_id}")
            
            # Save to SQLite
            sqlite_backend = SQLiteCacheBackend(sqlite_path)
            sqlite_backend.save(plugins)
            
            logger.info(f"Successfully migrated {len(plugins)} plugins to SQLite")
            return True
            
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            return False
    
    @staticmethod
    def verify_migration(json_path: Path, sqlite_path: Path) -> bool:
        """Verify migration completed successfully."""
        if not json_path.exists() or not sqlite_path.exists():
            return False
        
        try:
            # Count JSON plugins
            json_data = load_json_file(json_path)
            json_count = len(json_data) if json_data else 0
            
            # Count SQLite plugins
            sqlite_backend = SQLiteCacheBackend(sqlite_path)
            sqlite_stats = sqlite_backend.get_stats()
            sqlite_count = sqlite_stats["total_plugins"]
            
            return json_count == sqlite_count
            
        except Exception as e:
            logger.error(f"Migration verification failed: {e}")
            return False
```

### 1.3 Cache Backend Factory

```python
# src/pedalboard_pluginary/cache/factory.py
from pathlib import Path
from typing import Union
from ..protocols import CacheBackend
from .sqlite_backend import SQLiteCacheBackend
from .json_backend import JSONCacheBackend
from .migration import CacheMigration

def create_cache_backend(
    backend_type: str,
    cache_dir: Path,
    auto_migrate: bool = True
) -> CacheBackend:
    """Factory for creating cache backends with automatic migration."""
    
    if backend_type == "sqlite":
        sqlite_path = cache_dir / "plugins.db"
        json_path = cache_dir / "plugins.json"
        
        # Auto-migrate from JSON if needed
        if auto_migrate and json_path.exists() and not sqlite_path.exists():
            CacheMigration.migrate_json_to_sqlite(json_path, sqlite_path)
        
        return SQLiteCacheBackend(sqlite_path)
    
    elif backend_type == "json":
        return JSONCacheBackend(cache_dir / "plugins.json")
    
    else:
        raise ValueError(f"Unknown cache backend: {backend_type}")
```

## Phase 2: Modern CLI Revolution (Days 6-10)

### 2.1 Click Framework Migration

**Problem**: Fire-based CLI lacks modern features, poor error handling, no help system.

**Solution**: Complete migration to Click with Rich formatting.

```python
# src/pedalboard_pluginary/cli.py
import asyncio
from pathlib import Path
from typing import Optional, List

import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.panel import Panel
from rich.prompt import Confirm

from .core import PedalboardPluginary
from .models import PluginInfo
from .constants import DEFAULT_MAX_CONCURRENT

console = Console()

@click.group()
@click.version_option()
@click.option('--config-file', type=click.Path(exists=True), help='Configuration file path')
@click.option('--cache-dir', type=click.Path(), help='Cache directory override')
@click.option('--verbose', '-v', count=True, help='Increase verbosity (-v, -vv, -vvv)')
@click.pass_context
def cli(ctx: click.Context, config_file: Optional[str], cache_dir: Optional[str], verbose: int):
    """Pedalboard Pluginary - Professional audio plugin scanner and manager.
    
    Discover, catalog, and manage VST3 and Audio Unit plugins on your system
    with lightning-fast async scanning and powerful search capabilities.
    
    Examples:
        pbpluginary scan --async                    # Fast async plugin scan
        pbpluginary list --filter "reverb"         # Find reverb plugins  
        pbpluginary search "FabFilter" --type vst3 # Search FabFilter VST3s
        pbpluginary info "vst3/Pro-Q 3"           # Detailed plugin info
    """
    # Store context for subcommands
    ctx.ensure_object(dict)
    ctx.obj['config_file'] = config_file
    ctx.obj['cache_dir'] = Path(cache_dir) if cache_dir else None
    ctx.obj['verbose'] = verbose
    
    # Setup logging based on verbosity
    setup_logging(verbose)

@cli.command()
@click.option('--async/--sync', 'async_mode', default=True, 
              help='Use async scanning for better performance (default: async)')
@click.option('--concurrency', default=DEFAULT_MAX_CONCURRENT, 
              help=f'Max concurrent scans in async mode (default: {DEFAULT_MAX_CONCURRENT})')
@click.option('--timeout', default=10.0, 
              help='Plugin load timeout in seconds (default: 10.0)')
@click.option('--folders', help='Additional folders to scan (comma-separated)')
@click.option('--force', is_flag=True, 
              help='Force full rescan (ignore existing cache)')
@click.option('--cache-backend', type=click.Choice(['sqlite', 'json']), default='sqlite',
              help='Cache backend to use (default: sqlite)')
@click.pass_context
def scan(ctx: click.Context, async_mode: bool, concurrency: int, timeout: float, 
         folders: Optional[str], force: bool, cache_backend: str):
    """Scan system for audio plugins with high-performance async processing.
    
    Discovers VST3 and Audio Unit plugins in standard system locations
    and any additional folders specified. Results are cached for fast
    subsequent operations.
    
    Examples:
        pbpluginary scan                            # Quick async scan
        pbpluginary scan --sync                     # Synchronous scan
        pbpluginary scan --folders ~/MyPlugins      # Include custom folder
        pbpluginary scan --force                    # Rebuild entire cache
        pbpluginary scan --concurrency 20          # Higher concurrency
    """
    extra_folders = [Path(f.strip()) for f in folders.split(',')] if folders else []
    
    # Initialize scanner
    try:
        scanner = PedalboardPluginary(
            cache_dir=ctx.obj['cache_dir'],
            async_mode=async_mode,
            max_concurrent=concurrency,
            timeout=timeout,
            cache_backend=cache_backend
        )
    except Exception as e:
        console.print(f"[red]Failed to initialize scanner: {e}[/red]")
        raise click.Abort()
    
    # Clear cache if force requested
    if force:
        if scanner.cache_exists():
            if Confirm.ask("Clear existing cache and perform full rescan?"):
                scanner.clear_cache()
                console.print("[yellow]Cache cleared[/yellow]")
            else:
                console.print("[yellow]Scan cancelled[/yellow]")
                return
    
    # Perform scan with progress
    scan_start_time = time.time()
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=40),
        TaskProgressColumn(),
        console=console
    ) as progress:
        
        def progress_callback(current: int, total: int, message: str = ""):
            if not hasattr(progress_callback, 'task_id'):
                progress_callback.task_id = progress.add_task("Scanning...", total=total)
            
            progress.update(
                progress_callback.task_id, 
                completed=current, 
                total=total,
                description=f"Scanning plugins... {message}"
            )
        
        try:
            if async_mode:
                plugins = asyncio.run(scanner.scan_async(
                    extra_folders=extra_folders,
                    progress_callback=progress_callback
                ))
            else:
                plugins = scanner.scan(
                    extra_folders=extra_folders,
                    progress_callback=progress_callback
                )
        except KeyboardInterrupt:
            console.print("\n[yellow]Scan interrupted by user[/yellow]")
            return
        except Exception as e:
            console.print(f"\n[red]Scan failed: {e}[/red]")
            raise click.Abort()
    
    scan_duration = time.time() - scan_start_time
    
    # Display results summary
    _display_scan_results(plugins, scan_duration, async_mode)

@cli.command('list')
@click.option('--format', type=click.Choice(['table', 'json', 'yaml', 'csv']), 
              default='table', help='Output format (default: table)')
@click.option('--filter', 'filter_text', help='Filter plugins by name or manufacturer')
@click.option('--type', 'plugin_type', type=click.Choice(['vst3', 'au']), 
              help='Filter by plugin type')
@click.option('--manufacturer', help='Filter by manufacturer')
@click.option('--parameters', help='Filter by parameter count (e.g., "10-50", ">20", "<10")')
@click.option('--sort', type=click.Choice(['name', 'type', 'manufacturer', 'parameters']),
              default='name', help='Sort by field (default: name)')
@click.option('--reverse', is_flag=True, help='Reverse sort order')
@click.option('--limit', default=50, help='Limit number of results (default: 50)')
@click.option('--offset', default=0, help='Skip first N results (default: 0)')
@click.pass_context
def list_plugins(ctx: click.Context, format: str, filter_text: Optional[str], 
                plugin_type: Optional[str], manufacturer: Optional[str],
                parameters: Optional[str], sort: str, reverse: bool,
                limit: int, offset: int):
    """List discovered plugins with advanced filtering and formatting.
    
    Display plugins in various formats with powerful filtering capabilities.
    Supports full-text search, type filtering, and parameter-based queries.
    
    Examples:
        pbpluginary list                            # Show all plugins (table)
        pbpluginary list --filter "reverb"         # Search for reverb plugins
        pbpluginary list --type vst3 --format json # VST3 plugins as JSON
        pbpluginary list --manufacturer FabFilter   # FabFilter plugins only
        pbpluginary list --parameters ">50"        # Plugins with 50+ parameters
        pbpluginary list --sort manufacturer       # Sort by manufacturer
    """
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        
        # Parse parameter filter
        param_range = None
        if parameters:
            param_range = _parse_parameter_filter(parameters)
        
        # Search plugins
        plugins = list(scanner.search_plugins(
            query=filter_text,
            plugin_type=plugin_type,
            manufacturer=manufacturer,
            parameter_count_range=param_range,
            sort_by=sort,
            sort_desc=reverse,
            limit=limit,
            offset=offset
        ))
        
        if not plugins:
            console.print("[yellow]No plugins found matching criteria[/yellow]")
            return
        
        # Output results
        _output_plugins(plugins, format)
        
        # Show pagination info
        if len(plugins) == limit:
            console.print(f"\n[dim]Showing results {offset + 1}-{offset + len(plugins)}. "
                         f"Use --offset {offset + limit} to see more.[/dim]")
        
    except Exception as e:
        console.print(f"[red]Failed to list plugins: {e}[/red]")
        raise click.Abort()

@cli.command()
@click.argument('query')
@click.option('--type', 'plugin_type', type=click.Choice(['vst3', 'au']), 
              help='Filter by plugin type')
@click.option('--fuzzy', is_flag=True, help='Enable fuzzy matching')
@click.option('--limit', default=20, help='Limit number of results (default: 20)')
@click.pass_context
def search(ctx: click.Context, query: str, plugin_type: Optional[str], 
           fuzzy: bool, limit: int):
    """Search plugins with full-text search and fuzzy matching.
    
    Performs advanced search across plugin names, manufacturers, and metadata.
    Supports exact matching and fuzzy search for approximate results.
    
    Examples:
        pbpluginary search "Pro-Q"                  # Exact search
        pbpluginary search "compressor" --fuzzy     # Fuzzy search
        pbpluginary search "reverb" --type vst3     # Type-filtered search
    """
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        
        if fuzzy:
            plugins = scanner.fuzzy_search(query, limit=limit)
        else:
            plugins = list(scanner.search_plugins(
                query=query,
                plugin_type=plugin_type,
                limit=limit
            ))
        
        if not plugins:
            console.print(f"[yellow]No plugins found for '{query}'[/yellow]")
            if not fuzzy:
                console.print("[dim]Try --fuzzy for approximate matching[/dim]")
            return
        
        # Display as table with relevance scores for fuzzy search
        _output_search_results(plugins, query, fuzzy)
        
    except Exception as e:
        console.print(f"[red]Search failed: {e}[/red]")
        raise click.Abort()

@cli.command()
@click.argument('plugin_id')
@click.option('--parameters/--no-parameters', default=True, 
              help='Show plugin parameters (default: show)')
@click.option('--test', is_flag=True, help='Test plugin loading')
@click.option('--suggest', is_flag=True, help='Show similar plugins')
@click.pass_context
def info(ctx: click.Context, plugin_id: str, parameters: bool, test: bool, suggest: bool):
    """Show detailed information about a specific plugin.
    
    Display comprehensive plugin information including metadata,
    parameters, and optionally test plugin loading capabilities.
    
    Examples:
        pbpluginary info "vst3/FabFilter Pro-Q 3"   # Basic plugin info
        pbpluginary info "au/ChromaVerb" --test     # Test loading
        pbpluginary info "vst3/Reverb" --suggest    # Show similar plugins
    """
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        plugin = scanner.get_plugin(plugin_id)
        
        if not plugin:
            console.print(f"[red]Plugin '{plugin_id}' not found[/red]")
            # Try fuzzy search for suggestions
            suggestions = scanner.fuzzy_search(plugin_id, limit=5)
            if suggestions:
                console.print("\n[yellow]Did you mean one of these?[/yellow]")
                for suggestion in suggestions:
                    console.print(f"  {suggestion.id}")
            raise click.Abort()
        
        # Display plugin information
        _display_plugin_info(plugin, parameters, test)
        
        # Show similar plugins if requested
        if suggest:
            similar = scanner.suggest_similar(plugin_id, limit=5)
            if similar:
                console.print("\n[bold]Similar Plugins:[/bold]")
                _output_plugins(similar, 'table')
        
    except Exception as e:
        console.print(f"[red]Failed to get plugin info: {e}[/red]")
        raise click.Abort()

# Cache management subcommands
@cli.group()
def cache():
    """Cache management commands."""
    pass

@cache.command()
@click.pass_context
def stats(ctx: click.Context):
    """Show detailed cache statistics and health information."""
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        stats = scanner.get_cache_stats()
        
        _display_cache_stats(stats)
        
    except Exception as e:
        console.print(f"[red]Failed to get cache stats: {e}[/red]")
        raise click.Abort()

@cache.command()
@click.option('--backup/--no-backup', default=True, 
              help='Backup cache before clearing (default: backup)')
@click.pass_context
def clear(ctx: click.Context, backup: bool):
    """Clear the plugin cache with optional backup."""
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        
        if not scanner.cache_exists():
            console.print("[yellow]No cache found to clear[/yellow]")
            return
        
        stats = scanner.get_cache_stats()
        plugin_count = stats.get('total_plugins', 0)
        
        if not Confirm.ask(f"Clear cache containing {plugin_count} plugins?"):
            console.print("[yellow]Cache clear cancelled[/yellow]")
            return
        
        if backup:
            backup_path = scanner.backup_cache()
            console.print(f"[green]Cache backed up to {backup_path}[/green]")
        
        scanner.clear_cache()
        console.print("[green]Cache cleared successfully[/green]")
        
    except Exception as e:
        console.print(f"[red]Failed to clear cache: {e}[/red]")
        raise click.Abort()

@cache.command()
@click.pass_context
def repair(ctx: click.Context):
    """Repair corrupted cache and validate integrity."""
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        
        with console.status("Analyzing cache integrity..."):
            issues = scanner.validate_cache()
        
        if not issues:
            console.print("[green]Cache is healthy - no issues found[/green]")
            return
        
        console.print(f"[yellow]Found {len(issues)} cache issues:[/yellow]")
        for issue in issues:
            console.print(f"  • {issue}")
        
        if Confirm.ask("Attempt to repair cache?"):
            with console.status("Repairing cache..."):
                repaired = scanner.repair_cache()
            
            if repaired:
                console.print("[green]Cache repaired successfully[/green]")
            else:
                console.print("[red]Cache repair failed[/red]")
        
    except Exception as e:
        console.print(f"[red]Cache repair failed: {e}[/red]")
        raise click.Abort()

@cache.command()
@click.option('--to', type=click.Choice(['sqlite', 'json']), required=True,
              help='Target cache backend')
@click.option('--backup/--no-backup', default=True,
              help='Backup original cache (default: backup)')
@click.pass_context
def migrate(ctx: click.Context, to: str, backup: bool):
    """Migrate cache between different backend formats."""
    try:
        scanner = PedalboardPluginary(cache_dir=ctx.obj['cache_dir'])
        
        with console.status(f"Migrating cache to {to} format..."):
            success = scanner.migrate_cache(to, backup=backup)
        
        if success:
            console.print(f"[green]Successfully migrated cache to {to} format[/green]")
        else:
            console.print(f"[red]Cache migration to {to} failed[/red]")
        
    except Exception as e:
        console.print(f"[red]Migration failed: {e}[/red]")
        raise click.Abort()

# Utility functions for CLI formatting
def _display_scan_results(plugins: Dict[str, PluginInfo], duration: float, async_mode: bool):
    """Display scan results with Rich formatting."""
    # Summary statistics
    type_counts = {}
    manufacturer_counts = {}
    
    for plugin in plugins.values():
        type_counts[plugin.plugin_type] = type_counts.get(plugin.plugin_type, 0) + 1
        if plugin.manufacturer:
            manufacturer_counts[plugin.manufacturer] = manufacturer_counts.get(plugin.manufacturer, 0) + 1
    
    # Create summary table
    table = Table(title=f"Scan Results Summary ({len(plugins)} plugins found)")
    table.add_column("Metric", style="cyan")
    table.add_column("Value", justify="right", style="magenta")
    
    # Basic stats
    table.add_row("Total Plugins", str(len(plugins)))
    table.add_row("Scan Duration", f"{duration:.1f}s")
    table.add_row("Scan Mode", "Async" if async_mode else "Sync")
    table.add_row("Avg Speed", f"{len(plugins)/duration:.1f} plugins/sec")
    
    table.add_row("", "")  # Separator
    
    # Type breakdown
    for plugin_type, count in sorted(type_counts.items()):
        table.add_row(f"{plugin_type.upper()} Plugins", str(count))
    
    table.add_row("", "")  # Separator
    
    # Top manufacturers
    top_manufacturers = sorted(manufacturer_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    for manufacturer, count in top_manufacturers:
        table.add_row(f"{manufacturer}", str(count))
    
    console.print(table)
    
    # Performance tip
    if not async_mode and len(plugins) > 20:
        console.print("\n[dim]💡 Tip: Use --async for faster scanning of large plugin libraries[/dim]")

def _display_plugin_info(plugin: PluginInfo, show_parameters: bool, test_loading: bool):
    """Display detailed plugin information with Rich formatting."""
    # Main info panel
    info_lines = [
        f"[bold]ID:[/bold] {plugin.id}",
        f"[bold]Name:[/bold] {plugin.name}",
        f"[bold]Type:[/bold] {plugin.plugin_type.upper()}",
        f"[bold]Manufacturer:[/bold] {plugin.manufacturer or 'Unknown'}",
        f"[bold]Path:[/bold] {plugin.path}",
        f"[bold]Parameters:[/bold] {len(plugin.parameters)}"
    ]
    
    panel = Panel(
        "\n".join(info_lines), 
        title=f"Plugin: {plugin.name}", 
        border_style="blue"
    )
    console.print(panel)
    
    # Parameters table
    if show_parameters and plugin.parameters:
        param_table = Table(title="Parameters")
        param_table.add_column("Parameter", style="cyan")
        param_table.add_column("Value", style="magenta")
        param_table.add_column("Type", style="green")
        
        for param in sorted(plugin.parameters.values(), key=lambda p: p.name):
            param_table.add_row(
                param.name,
                str(param.value),
                type(param.value).__name__
            )
        
        console.print(param_table)
    
    # Test loading
    if test_loading:
        with console.status("Testing plugin loading..."):
            try:
                # Test plugin loading
                import pedalboard
                test_plugin = pedalboard.load_plugin(plugin.path)
                success = test_plugin is not None
            except Exception as e:
                success = False
                error_msg = str(e)
        
        if success:
            console.print("[green]✓ Plugin loads successfully[/green]")
        else:
            console.print(f"[red]✗ Plugin failed to load: {error_msg}[/red]")

def _output_plugins(plugins: List[PluginInfo], format: str):
    """Output plugins in specified format."""
    if format == 'table':
        _output_table(plugins)
    elif format == 'json':
        _output_json(plugins)
    elif format == 'yaml':
        _output_yaml(plugins)
    elif format == 'csv':
        _output_csv(plugins)

def _output_table(plugins: List[PluginInfo]):
    """Display plugins in a rich table."""
    table = Table()
    table.add_column("Name", style="cyan", no_wrap=True)
    table.add_column("Type", style="magenta")
    table.add_column("Manufacturer", style="green")
    table.add_column("Params", justify="right", style="yellow")
    table.add_column("Path", style="blue", overflow="ellipsis")
    
    for plugin in plugins:
        table.add_row(
            plugin.name,
            plugin.plugin_type.upper(),
            plugin.manufacturer or "Unknown",
            str(len(plugin.parameters)),
            plugin.path
        )
    
    console.print(table)

def _display_cache_stats(stats: Dict[str, Any]):
    """Display cache statistics with Rich formatting."""
    table = Table(title="Cache Statistics")
    table.add_column("Metric", style="cyan")
    table.add_column("Value", justify="right", style="magenta")
    
    # Basic stats
    table.add_row("Total Plugins", str(stats.get('total_plugins', 0)))
    
    # Database size
    db_size = stats.get('database_size_bytes', 0)
    if db_size > 0:
        if db_size > 1024 * 1024:
            size_str = f"{db_size / (1024 * 1024):.1f} MB"
        elif db_size > 1024:
            size_str = f"{db_size / 1024:.1f} KB"
        else:
            size_str = f"{db_size} bytes"
        table.add_row("Cache Size", size_str)
    
    # Last updated
    last_updated = stats.get('last_updated')
    if last_updated:
        import datetime
        dt = datetime.datetime.fromtimestamp(float(last_updated))
        table.add_row("Last Updated", dt.strftime("%Y-%m-%d %H:%M:%S"))
    
    table.add_row("", "")  # Separator
    
    # By type
    by_type = stats.get('by_type', {})
    for plugin_type, count in sorted(by_type.items()):
        table.add_row(f"{plugin_type.upper()} Plugins", str(count))
    
    # Top manufacturers
    top_manufacturers = stats.get('top_manufacturers', {})
    if top_manufacturers:
        table.add_row("", "")  # Separator
        table.add_row("[bold]Top Manufacturers[/bold]", "")
        for manufacturer, count in list(top_manufacturers.items())[:5]:
            table.add_row(f"  {manufacturer}", str(count))
    
    console.print(table)

# Entry point
def main():
    """Main CLI entry point."""
    cli()

if __name__ == '__main__':
    main()
```

**Implementation Steps**:
1. Create comprehensive Click CLI structure
2. Implement Rich formatting for all output
3. Add advanced search and filtering capabilities
4. Create cache management subcommands
5. Add comprehensive help and examples

**Expected Impact**:
- **User Experience**: Professional CLI with rich formatting and comprehensive help
- **Discoverability**: Advanced search, filtering, and auto-completion
- **Functionality**: Cache management, plugin testing, similar plugin suggestions
- **Productivity**: Fast, intuitive commands with excellent feedback

## Phase 3: Performance Optimization and Testing (Days 11-15)

### 3.1 Async Performance Benchmarking

```python
# tests/performance/test_async_benchmarks.py
import pytest
import asyncio
import time
from pathlib import Path
from unittest.mock import patch, MagicMock
from pedalboard_pluginary import PedalboardScanner

class TestAsyncPerformance:
    """Comprehensive async performance testing."""
    
    @pytest.fixture
    def mock_plugins_dir(self, tmp_path):
        """Create realistic mock plugin directory."""
        plugins_dir = tmp_path / "plugins"
        plugins_dir.mkdir()
        
        # Create mock VST3 files
        vst3_dir = plugins_dir / "vst3"
        vst3_dir.mkdir()
        for i in range(50):
            (vst3_dir / f"TestPlugin{i:02d}.vst3").touch()
        
        # Create mock AU files  
        au_dir = plugins_dir / "au"
        au_dir.mkdir()
        for i in range(30):
            (au_dir / f"TestAU{i:02d}.component").touch()
        
        return plugins_dir
    
    def create_mock_plugin(self, name: str, load_time: float = 0.01):
        """Create mock plugin with realistic load time."""
        mock_plugin = MagicMock()
        mock_plugin.name = name
        mock_plugin.manufacturer = "TestManufacturer"
        mock_plugin.parameters = {f"param_{i}": 0.5 for i in range(10)}
        
        def slow_load(*args, **kwargs):
            time.sleep(load_time)  # Simulate plugin loading time
            return mock_plugin
        
        return slow_load
    
    @pytest.mark.asyncio
    async def test_async_vs_sync_performance(self, mock_plugins_dir, benchmark):
        """Test that async scanning is significantly faster than sync."""
        plugin_count = 80  # Total mock plugins
        load_time_per_plugin = 0.01  # 10ms per plugin
        
        with patch('pedalboard.load_plugin') as mock_load:
            mock_load.side_effect = self.create_mock_plugin("TestPlugin", load_time_per_plugin)
            
            # Benchmark sync scanning
            sync_scanner = PedalboardScanner(
                async_mode=False,
                specific_paths=[str(mock_plugins_dir)]
            )
            
            def sync_scan():
                return sync_scanner.full_scan()
            
            sync_result = benchmark.pedantic(sync_scan, iterations=1, rounds=3)
            sync_time = benchmark.stats.stats.mean
            
            # Benchmark async scanning
            async_scanner = PedalboardScanner(
                async_mode=True,
                max_concurrent=10,
                specific_paths=[str(mock_plugins_dir)]
            )
            
            async def async_scan():
                return await async_scanner.full_scan_async()
            
            # Run async benchmark
            start_time = time.time()
            async_result = await async_scan()
            async_time = time.time() - start_time
            
            # Verify results are equivalent
            assert len(sync_result) == len(async_result)
            assert len(sync_result) == plugin_count
            
            # Verify performance improvement
            speedup = sync_time / async_time
            
            print(f"Sync time: {sync_time:.2f}s")
            print(f"Async time: {async_time:.2f}s") 
            print(f"Speedup: {speedup:.1f}x")
            
            # Assert significant performance improvement
            assert speedup >= 3.0, f"Expected 3x+ speedup, got {speedup:.1f}x"
            
            # Verify we're approaching theoretical maximum
            theoretical_min = (plugin_count * load_time_per_plugin) / 10  # 10 concurrent
            efficiency = theoretical_min / async_time
            assert efficiency >= 0.7, f"Low async efficiency: {efficiency:.2f}"
    
    @pytest.mark.asyncio
    async def test_concurrency_scaling(self, mock_plugins_dir):
        """Test performance scaling with different concurrency levels."""
        concurrency_levels = [1, 5, 10, 20]
        plugin_count = 60
        results = {}
        
        with patch('pedalboard.load_plugin') as mock_load:
            mock_load.side_effect = self.create_mock_plugin("TestPlugin", 0.02)
            
            for concurrency in concurrency_levels:
                scanner = PedalboardScanner(
                    async_mode=True,
                    max_concurrent=concurrency,
                    specific_paths=[str(mock_plugins_dir)]
                )
                
                start_time = time.time()
                plugins = await scanner.full_scan_async()
                scan_time = time.time() - start_time
                
                results[concurrency] = {
                    'time': scan_time,
                    'plugins': len(plugins),
                    'rate': len(plugins) / scan_time
                }
        
        # Verify performance scaling
        assert results[10]['rate'] > results[1]['rate'] * 3
        assert results[20]['rate'] > results[5]['rate'] * 2
        
        # Log results for analysis
        for concurrency, result in results.items():
            print(f"Concurrency {concurrency}: {result['rate']:.1f} plugins/sec")
    
    @pytest.mark.asyncio 
    async def test_memory_usage_async(self, mock_plugins_dir):
        """Test memory usage doesn't grow excessively with async scanning."""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss
        
        # Create large dataset
        with patch('pedalboard.load_plugin') as mock_load:
            mock_load.side_effect = self.create_mock_plugin("TestPlugin", 0.001)
            
            scanner = PedalboardScanner(
                async_mode=True,
                max_concurrent=10,
                specific_paths=[str(mock_plugins_dir)]
            )
            
            # Scan plugins
            plugins = await scanner.full_scan_async()
            
            current_memory = process.memory_info().rss
            memory_growth = current_memory - baseline_memory
            
            # Memory growth should be reasonable
            memory_per_plugin = memory_growth / len(plugins)
            
            print(f"Memory growth: {memory_growth / 1024 / 1024:.1f} MB")
            print(f"Memory per plugin: {memory_per_plugin / 1024:.1f} KB")
            
            # Should use less than 1KB per plugin on average
            assert memory_per_plugin < 1024, f"Excessive memory usage: {memory_per_plugin:.0f} bytes/plugin"
    
    def test_error_handling_performance(self, mock_plugins_dir):
        """Test that error handling doesn't significantly impact performance."""
        with patch('pedalboard.load_plugin') as mock_load:
            # Mix of successful and failing plugins
            def mixed_load_plugin(path):
                if "05" in str(path) or "15" in str(path):  # Some fail
                    raise Exception("Mock plugin error")
                return self.create_mock_plugin("TestPlugin", 0.01)()
            
            mock_load.side_effect = mixed_load_plugin
            
            scanner = PedalboardScanner(
                async_mode=True,
                max_concurrent=10,
                specific_paths=[str(mock_plugins_dir)]
            )
            
            start_time = time.time()
            plugins = asyncio.run(scanner.full_scan_async())
            scan_time = time.time() - start_time
            
            # Should still maintain good performance despite errors
            rate = len(plugins) / scan_time
            assert rate > 20, f"Low error handling performance: {rate:.1f} plugins/sec"
```

### 3.2 SQLite Performance Testing

```python
# tests/performance/test_cache_performance.py
import pytest
import time
from pathlib import Path
from pedalboard_pluginary.cache.sqlite_backend import SQLiteCacheBackend
from pedalboard_pluginary.models import PluginInfo, PluginParameter

class TestCachePerformance:
    """SQLite cache performance testing."""
    
    @pytest.fixture
    def large_plugin_dataset(self):
        """Create large dataset for performance testing."""
        plugins = {}
        
        manufacturers = ["FabFilter", "Waves", "Native Instruments", "Steinberg", "Avid"]
        types = ["vst3", "au"]
        
        for i in range(1000):
            manufacturer = manufacturers[i % len(manufacturers)]
            plugin_type = types[i % len(types)]
            
            plugin = PluginInfo(
                id=f"{plugin_type}/TestPlugin{i:04d}",
                name=f"Test Plugin {i:04d}",
                path=f"/fake/path/TestPlugin{i:04d}.{plugin_type}",
                filename=f"TestPlugin{i:04d}.{plugin_type}",
                plugin_type=plugin_type,
                manufacturer=manufacturer,
                parameters={
                    f"param_{j}": PluginParameter(
                        name=f"Parameter {j}",
                        value=float(j % 100) / 100.0
                    )
                    for j in range(i % 50 + 1)  # Variable parameter count
                }
            )
            plugins[plugin.id] = plugin
        
        return plugins
    
    def test_sqlite_vs_json_write_performance(self, tmp_path, large_plugin_dataset, benchmark):
        """Compare SQLite vs JSON write performance."""
        sqlite_path = tmp_path / "test.db"
        json_path = tmp_path / "test.json"
        
        # Test SQLite write
        sqlite_backend = SQLiteCacheBackend(sqlite_path)
        
        def sqlite_write():
            sqlite_backend.save(large_plugin_dataset)
        
        sqlite_time = benchmark.pedantic(sqlite_write, iterations=1, rounds=3)
        
        # Test JSON write (for comparison)
        from pedalboard_pluginary.serialization import PluginSerializer
        
        def json_write():
            PluginSerializer.save_plugins(large_plugin_dataset, json_path)
        
        start_time = time.time()
        json_write()
        json_time = time.time() - start_time
        
        print(f"SQLite write: {sqlite_time:.3f}s")
        print(f"JSON write: {json_time:.3f}s")
        
        # SQLite should be competitive or faster
        assert sqlite_time < json_time * 2, "SQLite write significantly slower than JSON"
    
    def test_search_performance_scaling(self, tmp_path, large_plugin_dataset):
        """Test search performance with increasing dataset sizes."""
        sqlite_path = tmp_path / "search_test.db"
        backend = SQLiteCacheBackend(sqlite_path)
        
        # Test with different dataset sizes
        sizes = [100, 500, 1000]
        search_times = {}
        
        for size in sizes:
            # Create subset of data
            subset = dict(list(large_plugin_dataset.items())[:size])
            backend.clear()
            backend.save(subset)
            
            # Test search performance
            start_time = time.time()
            results = list(backend.search(query="Test", limit=50))
            search_time = time.time() - start_time
            
            search_times[size] = search_time
            
            print(f"Size {size}: {search_time:.4f}s ({len(results)} results)")
        
        # Search time should scale well (not linearly)
        scaling_factor = search_times[1000] / search_times[100]
        assert scaling_factor < 5, f"Poor search scaling: {scaling_factor:.1f}x"
    
    def test_indexed_vs_unindexed_search(self, tmp_path, large_plugin_dataset):
        """Compare performance with and without indexes."""
        sqlite_path = tmp_path / "index_test.db"
        backend = SQLiteCacheBackend(sqlite_path)
        backend.save(large_plugin_dataset)
        
        # Test with indexes
        start_time = time.time()
        results_with_index = list(backend.search(
            plugin_type="vst3",
            manufacturer="FabFilter",
            limit=100
        ))
        time_with_index = time.time() - start_time
        
        # Drop indexes to test without
        with backend._connect() as conn:
            conn.execute("DROP INDEX IF EXISTS idx_plugins_type")
            conn.execute("DROP INDEX IF EXISTS idx_plugins_manufacturer")
        
        start_time = time.time()
        results_without_index = list(backend.search(
            plugin_type="vst3", 
            manufacturer="FabFilter",
            limit=100
        ))
        time_without_index = time.time() - start_time
        
        # Results should be identical
        assert len(results_with_index) == len(results_without_index)
        
        # Indexed search should be faster
        speedup = time_without_index / time_with_index
        print(f"Index speedup: {speedup:.1f}x")
        assert speedup > 2, f"Insufficient index speedup: {speedup:.1f}x"
    
    def test_concurrent_access_performance(self, tmp_path, large_plugin_dataset):
        """Test performance with concurrent cache access."""
        import threading
        import queue
        
        sqlite_path = tmp_path / "concurrent_test.db"
        backend = SQLiteCacheBackend(sqlite_path)
        backend.save(large_plugin_dataset)
        
        results_queue = queue.Queue()
        
        def concurrent_search(thread_id):
            start_time = time.time()
            results = list(backend.search(
                query=f"Plugin {thread_id % 100:04d}",
                limit=10
            ))
            duration = time.time() - start_time
            results_queue.put((thread_id, duration, len(results)))
        
        # Run concurrent searches
        threads = []
        thread_count = 10
        
        start_time = time.time()
        for i in range(thread_count):
            thread = threading.Thread(target=concurrent_search, args=(i,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        # Collect results
        thread_results = []
        while not results_queue.empty():
            thread_results.append(results_queue.get())
        
        avg_time = sum(r[1] for r in thread_results) / len(thread_results)
        total_results = sum(r[2] for r in thread_results)
        
        print(f"Concurrent access: {thread_count} threads, {avg_time:.4f}s avg, {total_results} total results")
        
        # Should handle concurrent access efficiently
        assert total_time < 2.0, f"Slow concurrent access: {total_time:.2f}s"
        assert len(thread_results) == thread_count, "Some threads failed"
```

## Phase 4: Advanced Features and Production Readiness (Days 16-20)

### 4.1 Smart Change Detection System

```python
# src/pedalboard_pluginary/cache/change_detection.py
import time
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
from ..models import PluginInfo
from ..protocols import CacheBackend, PluginScanner
from ..exceptions import CacheError

logger = logging.getLogger(__name__)

class ChangeDetector:
    """Intelligent change detection for incremental plugin scanning."""
    
    def __init__(self, cache_backend: CacheBackend):
        self.cache = cache_backend
        self._last_scan_time: Optional[float] = None
    
    def detect_changes(
        self, 
        scanners: List[PluginScanner],
        scan_paths: Optional[List[Path]] = None
    ) -> Dict[str, List[Path]]:
        """Detect plugin file changes since last scan.
        
        Returns:
            Dictionary with 'added', 'modified', 'removed' file lists.
        """
        # Discover current plugin files
        current_files = self._discover_current_files(scanners, scan_paths)
        
        # Get cached plugin information
        cached_files = self._get_cached_file_info()
        
        changes = {
            "added": [],
            "modified": [],
            "removed": []
        }
        
        # Check for new and modified files
        for file_path, file_mtime in current_files.items():
            if file_path not in cached_files:
                changes["added"].append(file_path)
                logger.debug(f"New plugin file detected: {file_path}")
            else:
                cached_mtime = cached_files[file_path]
                if file_mtime > cached_mtime + 1:  # 1 second tolerance
                    changes["modified"].append(file_path)
                    logger.debug(f"Modified plugin file detected: {file_path}")
        
        # Check for removed files
        current_paths = set(current_files.keys())
        cached_paths = set(cached_files.keys())
        removed_paths = cached_paths - current_paths
        
        for removed_path in removed_paths:
            changes["removed"].append(removed_path)
            logger.debug(f"Removed plugin file detected: {removed_path}")
        
        return changes
    
    def get_incremental_scan_targets(
        self,
        scanners: List[PluginScanner],
        scan_paths: Optional[List[Path]] = None,
        force_rescan_older_than: Optional[float] = None
    ) -> Tuple[List[Path], Dict[str, List[Path]]]:
        """Get files that need scanning for incremental update.
        
        Args:
            scanners: Available plugin scanners.
            scan_paths: Specific paths to scan (optional).
            force_rescan_older_than: Force rescan of plugins older than this timestamp.
        
        Returns:
            Tuple of (files_to_scan, change_summary).
        """
        changes = self.detect_changes(scanners, scan_paths)
        
        files_to_scan = []
        files_to_scan.extend(changes["added"])
        files_to_scan.extend(changes["modified"])
        
        # Add old plugins for forced rescan
        if force_rescan_older_than:
            old_plugins = self._get_plugins_older_than(force_rescan_older_than)
            files_to_scan.extend(old_plugins)
            changes["force_rescanned"] = old_plugins
        
        # Remove duplicates while preserving order
        seen = set()
        unique_files = []
        for file_path in files_to_scan:
            if file_path not in seen:
                seen.add(file_path)
                unique_files.append(file_path)
        
        return unique_files, changes
    
    def cleanup_removed_plugins(self, removed_files: List[Path]) -> int:
        """Remove plugins for deleted files from cache.
        
        Returns:
            Number of plugins removed from cache.
        """
        removed_count = 0
        
        for file_path in removed_files:
            # Find plugin ID(s) for this file path
            plugin_ids = self._find_plugin_ids_by_path(file_path)
            
            for plugin_id in plugin_ids:
                try:
                    self.cache.delete(plugin_id)
                    removed_count += 1
                    logger.info(f"Removed plugin from cache: {plugin_id}")
                except Exception as e:
                    logger.error(f"Failed to remove plugin {plugin_id}: {e}")
        
        return removed_count
    
    def update_scan_timestamp(self) -> None:
        """Update the last scan timestamp."""
        self._last_scan_time = time.time()
    
    def get_cache_age_stats(self) -> Dict[str, float]:
        """Get statistics about cache age and freshness."""
        try:
            stats = self.cache.get_stats()
            
            # Get file modification times
            if hasattr(self.cache, 'get_changed_plugins'):
                recent_threshold = time.time() - (24 * 60 * 60)  # 24 hours
                recent_plugins = self.cache.get_changed_plugins(recent_threshold)
                
                return {
                    "total_plugins": stats.get("total_plugins", 0),
                    "recently_updated": len(recent_plugins),
                    "cache_freshness": len(recent_plugins) / max(stats.get("total_plugins", 1), 1)
                }
            
            return {"total_plugins": stats.get("total_plugins", 0)}
            
        except Exception as e:
            logger.error(f"Failed to get cache age stats: {e}")
            return {}
    
    def _discover_current_files(
        self, 
        scanners: List[PluginScanner],
        scan_paths: Optional[List[Path]] = None
    ) -> Dict[Path, float]:
        """Discover all current plugin files with modification times."""
        current_files = {}
        
        for scanner in scanners:
            try:
                plugin_files = scanner.find_plugin_files(scan_paths)
                
                for file_path in plugin_files:
                    try:
                        stat = file_path.stat()
                        current_files[file_path] = stat.st_mtime
                    except (OSError, ValueError) as e:
                        logger.warning(f"Cannot stat file {file_path}: {e}")
                        
            except Exception as e:
                logger.error(f"Scanner {scanner.__class__.__name__} failed to find files: {e}")
        
        return current_files
    
    def _get_cached_file_info(self) -> Dict[Path, float]:
        """Get file paths and modification times from cache."""
        cached_files = {}
        
        try:
            # For SQLite backend, we can query file modification times efficiently
            if hasattr(self.cache, '_connect'):
                with self.cache._connect() as conn:
                    rows = conn.execute(
                        "SELECT path, file_mtime FROM plugins WHERE file_mtime > 0"
                    ).fetchall()
                    
                    for path_str, mtime in rows:
                        try:
                            path = Path(path_str)
                            cached_files[path] = float(mtime)
                        except (ValueError, TypeError):
                            continue
            else:
                # Fallback for other cache backends
                logger.warning("Cache backend doesn't support efficient file time queries")
        
        except Exception as e:
            logger.error(f"Failed to get cached file info: {e}")
        
        return cached_files
    
    def _find_plugin_ids_by_path(self, file_path: Path) -> List[str]:
        """Find plugin IDs associated with a file path."""
        plugin_ids = []
        
        try:
            if hasattr(self.cache, '_connect'):
                with self.cache._connect() as conn:
                    rows = conn.execute(
                        "SELECT id FROM plugins WHERE path = ?",
                        (str(file_path),)
                    ).fetchall()
                    
                    plugin_ids = [row[0] for row in rows]
        
        except Exception as e:
            logger.error(f"Failed to find plugin IDs for {file_path}: {e}")
        
        return plugin_ids
    
    def _get_plugins_older_than(self, timestamp: float) -> List[Path]:
        """Get plugin file paths older than given timestamp."""
        old_files = []
        
        try:
            if hasattr(self.cache, '_connect'):
                with self.cache._connect() as conn:
                    rows = conn.execute(
                        "SELECT path FROM plugins WHERE updated_at < ?",
                        (timestamp,)
                    ).fetchall()
                    
                    for row in rows:
                        try:
                            old_files.append(Path(row[0]))
                        except (ValueError, TypeError):
                            continue
        
        except Exception as e:
            logger.error(f"Failed to get old plugins: {e}")
        
        return old_files
```

### 4.2 Configuration Management System

```python
# src/pedalboard_pluginary/config.py
from pathlib import Path
from typing import List, Optional, Dict, Any
import os
import json
import logging
from dataclasses import dataclass, field, asdict
from ..constants import DEFAULT_MAX_CONCURRENT

logger = logging.getLogger(__name__)

@dataclass
class ScanConfig:
    """Configuration for plugin scanning."""
    async_mode: bool = True
    max_concurrent: int = DEFAULT_MAX_CONCURRENT
    timeout: float = 10.0
    retry_attempts: int = 3
    retry_delay: float = 1.0
    
    def validate(self) -> None:
        """Validate configuration values."""
        if self.max_concurrent < 1 or self.max_concurrent > 100:
            raise ValueError(f"max_concurrent must be 1-100, got {self.max_concurrent}")
        
        if self.timeout <= 0 or self.timeout > 300:
            raise ValueError(f"timeout must be 0-300 seconds, got {self.timeout}")

@dataclass  
class CacheConfig:
    """Configuration for caching."""
    backend: str = "sqlite"  # "sqlite" or "json"
    directory: Optional[Path] = None
    auto_migrate: bool = True
    vacuum_threshold: int = 1000  # Vacuum after this many operations
    backup_on_migrate: bool = True
    
    def __post_init__(self):
        if self.directory is None:
            self.directory = Path.home() / ".cache" / "pedalboard-pluginary"
    
    def validate(self) -> None:
        """Validate configuration values."""
        if self.backend not in ["sqlite", "json"]:
            raise ValueError(f"Unknown cache backend: {self.backend}")

@dataclass
class CLIConfig:
    """Configuration for CLI appearance and behavior."""
    output_format: str = "table"  # "table", "json", "yaml", "csv"
    enable_colors: bool = True
    enable_progress: bool = True
    default_limit: int = 50
    page_size: int = 20
    
    def validate(self) -> None:
        """Validate configuration values."""
        valid_formats = ["table", "json", "yaml", "csv"]
        if self.output_format not in valid_formats:
            raise ValueError(f"output_format must be one of {valid_formats}")

@dataclass
class PathConfig:
    """Configuration for plugin search paths."""
    scan_default_locations: bool = True
    additional_scan_paths: List[str] = field(default_factory=list)
    ignore_patterns: List[str] = field(default_factory=list)
    specific_paths: List[str] = field(default_factory=list)
    
    def get_additional_paths(self) -> List[Path]:
        """Get additional scan paths as Path objects."""
        return [Path(p).expanduser() for p in self.additional_scan_paths]
    
    def get_specific_paths(self) -> List[Path]:
        """Get specific paths as Path objects."""
        return [Path(p).expanduser() for p in self.specific_paths]

@dataclass
class LoggingConfig:
    """Configuration for logging."""
    level: str = "INFO"
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_path: Optional[str] = None
    max_file_size: int = 10 * 1024 * 1024  # 10MB
    backup_count: int = 5
    
    def validate(self) -> None:
        """Validate configuration values."""
        valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if self.level.upper() not in valid_levels:
            raise ValueError(f"Invalid log level: {self.level}")

@dataclass
class PluginaryConfig:
    """Main configuration class for Pedalboard Pluginary."""
    scan: ScanConfig = field(default_factory=ScanConfig)
    cache: CacheConfig = field(default_factory=CacheConfig)
    cli: CLIConfig = field(default_factory=CLIConfig)
    paths: PathConfig = field(default_factory=PathConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    
    @classmethod
    def load_from_file(cls, config_path: Path) -> 'PluginaryConfig':
        """Load configuration from file."""
        if not config_path.exists():
            logger.info(f"Config file {config_path} not found, using defaults")
            return cls()
        
        try:
            with open(config_path, 'r') as f:
                data = json.load(f)
            
            # Create nested config objects
            config = cls()
            
            if 'scan' in data:
                config.scan = ScanConfig(**data['scan'])
            
            if 'cache' in data:
                cache_data = data['cache'].copy()
                if 'directory' in cache_data and cache_data['directory']:
                    cache_data['directory'] = Path(cache_data['directory']).expanduser()
                config.cache = CacheConfig(**cache_data)
            
            if 'cli' in data:
                config.cli = CLIConfig(**data['cli'])
            
            if 'paths' in data:
                config.paths = PathConfig(**data['paths'])
            
            if 'logging' in data:
                config.logging = LoggingConfig(**data['logging'])
            
            config.validate()
            return config
            
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            logger.info("Using default configuration")
            return cls()
    
    @classmethod
    def load_from_env(cls) -> 'PluginaryConfig':
        """Load configuration from environment variables."""
        config = cls()
        
        # Scan configuration
        if os.getenv('PLUGINARY_ASYNC_MODE'):
            config.scan.async_mode = os.getenv('PLUGINARY_ASYNC_MODE').lower() == 'true'
        
        if os.getenv('PLUGINARY_MAX_CONCURRENT'):
            config.scan.max_concurrent = int(os.getenv('PLUGINARY_MAX_CONCURRENT'))
        
        if os.getenv('PLUGINARY_TIMEOUT'):
            config.scan.timeout = float(os.getenv('PLUGINARY_TIMEOUT'))
        
        # Cache configuration
        if os.getenv('PLUGINARY_CACHE_BACKEND'):
            config.cache.backend = os.getenv('PLUGINARY_CACHE_BACKEND')
        
        if os.getenv('PLUGINARY_CACHE_DIR'):
            config.cache.directory = Path(os.getenv('PLUGINARY_CACHE_DIR')).expanduser()
        
        # CLI configuration
        if os.getenv('PLUGINARY_OUTPUT_FORMAT'):
            config.cli.output_format = os.getenv('PLUGINARY_OUTPUT_FORMAT')
        
        if os.getenv('PLUGINARY_ENABLE_COLORS'):
            config.cli.enable_colors = os.getenv('PLUGINARY_ENABLE_COLORS').lower() == 'true'
        
        # Path configuration
        if os.getenv('PLUGINARY_ADDITIONAL_PATHS'):
            additional_paths = os.getenv('PLUGINARY_ADDITIONAL_PATHS').split(os.pathsep)
            config.paths.additional_scan_paths = [p for p in additional_paths if p.strip()]
        
        if os.getenv('PLUGINARY_IGNORE_PATTERNS'):
            patterns = os.getenv('PLUGINARY_IGNORE_PATTERNS').split(',')
            config.paths.ignore_patterns = [p.strip() for p in patterns if p.strip()]
        
        # Logging configuration
        if os.getenv('PLUGINARY_LOG_LEVEL'):
            config.logging.level = os.getenv('PLUGINARY_LOG_LEVEL').upper()
        
        if os.getenv('PLUGINARY_LOG_FILE'):
            config.logging.file_path = os.getenv('PLUGINARY_LOG_FILE')
        
        config.validate()
        return config
    
    def save_to_file(self, config_path: Path) -> None:
        """Save configuration to file."""
        # Ensure directory exists
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert to dict for JSON serialization
        data = asdict(self)
        
        # Convert Path objects to strings
        if data['cache']['directory']:
            data['cache']['directory'] = str(data['cache']['directory'])
        
        try:
            with open(config_path, 'w') as f:
                json.dump(data, f, indent=2)
            
            logger.info(f"Configuration saved to {config_path}")
            
        except Exception as e:
            logger.error(f"Failed to save config to {config_path}: {e}")
            raise
    
    def validate(self) -> None:
        """Validate all configuration sections."""
        self.scan.validate()
        self.cache.validate()
        self.cli.validate()
        self.logging.validate()
    
    def get_cache_directory(self) -> Path:
        """Get the resolved cache directory path."""
        if self.cache.directory:
            return self.cache.directory.expanduser().resolve()
        return Path.home() / ".cache" / "pedalboard-pluginary"
    
    def merge_with_defaults(self, overrides: Dict[str, Any]) -> 'PluginaryConfig':
        """Create new config with override values."""
        config_dict = asdict(self)
        
        def deep_update(base: dict, updates: dict) -> dict:
            for key, value in updates.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    base[key] = deep_update(base[key], value)
                else:
                    base[key] = value
            return base
        
        updated_dict = deep_update(config_dict, overrides)
        
        # Convert back to config object
        new_config = PluginaryConfig()
        new_config.scan = ScanConfig(**updated_dict.get('scan', {}))
        new_config.cache = CacheConfig(**updated_dict.get('cache', {}))
        new_config.cli = CLIConfig(**updated_dict.get('cli', {}))
        new_config.paths = PathConfig(**updated_dict.get('paths', {}))
        new_config.logging = LoggingConfig(**updated_dict.get('logging', {}))
        
        new_config.validate()
        return new_config

class ConfigManager:
    """Manages configuration loading and saving."""
    
    DEFAULT_CONFIG_PATHS = [
        Path.home() / ".config" / "pedalboard-pluginary" / "config.json",
        Path.home() / ".pedalboard-pluginary.json",
        Path.cwd() / ".pedalboard-pluginary.json"
    ]
    
    @classmethod
    def load_config(
        self, 
        config_path: Optional[Path] = None,
        use_env: bool = True
    ) -> PluginaryConfig:
        """Load configuration with fallback chain."""
        
        # Start with environment variables if requested
        if use_env:
            config = PluginaryConfig.load_from_env()
        else:
            config = PluginaryConfig()
        
        # Try to load from file
        if config_path:
            # Specific config file provided
            if config_path.exists():
                file_config = PluginaryConfig.load_from_file(config_path)
                return file_config
            else:
                logger.warning(f"Specified config file {config_path} not found")
        else:
            # Try default locations
            for default_path in self.DEFAULT_CONFIG_PATHS:
                if default_path.exists():
                    logger.info(f"Loading config from {default_path}")
                    file_config = PluginaryConfig.load_from_file(default_path)
                    return file_config
        
        logger.info("Using default configuration")
        return config
    
    @classmethod
    def create_default_config(cls, config_path: Path) -> None:
        """Create a default configuration file."""
        config = PluginaryConfig()
        config.save_to_file(config_path)
        logger.info(f"Created default configuration at {config_path}")
```

## Implementation Timeline and Success Metrics

### Week 1 (Days 1-5): SQLite Foundation
**Goals:**
- SQLite cache backend fully functional
- JSON to SQLite migration working
- Performance benchmarks show 10x search improvement

**Success Criteria:**
- ✅ SQLite backend handles 1000+ plugins efficiently  
- ✅ Search performance O(log n) vs O(n) JSON
- ✅ Full-text search working with FTS
- ✅ Automatic migration from JSON preserves all data

### Week 2 (Days 6-10): Modern CLI Experience  
**Goals:**
- Click CLI fully replaces Fire
- Rich formatting and comprehensive help
- Advanced search and filtering working

**Success Criteria:**
- ✅ Professional CLI with comprehensive help system
- ✅ Rich table output with colors and formatting
- ✅ Advanced filtering (type, manufacturer, parameters)
- ✅ Cache management commands functional

### Week 3 (Days 11-15): Performance and Testing
**Goals:**
- Async performance benchmarked and optimized
- Comprehensive test suite for async functionality
- Performance regression testing in place

**Success Criteria:**
- ✅ Async scanning 5x+ faster than sync
- ✅ Memory usage constant with plugin count
- ✅ 90%+ test coverage for core functionality
- ✅ Performance benchmarks prevent regressions

### Week 4 (Days 16-20): Production Readiness
**Goals:**
- Smart change detection working
- Configuration management system
- Production monitoring and error recovery

**Success Criteria:**
- ✅ Incremental updates 95% faster than full scans
- ✅ Configuration system with file and env support
- ✅ Error recovery and health monitoring working
- ✅ Ready for deployment in production environments

## Conclusion

This implementation plan transforms Pedalboard Pluginary from a basic scanning tool into a **production-ready, high-performance plugin management system**. The combination of **SQLite scalability**, **async performance**, and **modern CLI experience** creates a professional-grade tool capable of handling enterprise-scale plugin libraries.

**Key Architectural Advantages:**
- **10x search performance** with SQLite + FTS
- **5-10x scanning performance** with async/await
- **Unlimited scalability** with lazy loading and indexing
- **Professional UX** with Click + Rich formatting
- **Production reliability** with comprehensive error handling

The async foundation is already in place. Adding SQLite caching and Click CLI will complete the transformation into a world-class audio plugin management tool.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[tool.setuptools_scm]
version_scheme = "no-guess-dev"

[project]
name = "pedalboard-pluginary"
version = "0.1.0"
description = "A plugin scanner for Pedalboard"
readme = "README.md"
requires-python = ">=3.9"
license = { text = "Apache-2.0" }
authors = [
    { name = "Adam Twardoch", email = "adam@twardoch.com" }
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Multimedia :: Sound/Audio",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
dependencies = [
    "pedalboard>=0.8.7",
    "fire>=0.5.0",
    "python-benedict>=0.33.0",
    "pyyaml>=6.0.1",
    "typing-extensions>=4.0.0; python_version < '3.11'",
    "tqdm>=4.60.0",
]

[project.urls]
Documentation = "https://github.com/twardoch/pedalboard-pluginary#readme"
Source = "https://github.com/twardoch/pedalboard-pluginary"
Tracker = "https://github.com/twardoch/pedalboard-pluginary/issues"

[project.optional-dependencies]
dev = [
    "pytest>=7.4.4",
    "pytest-cov>=4.1.0",
    "mypy>=1.8.0",
    "flake8>=7.0.0",
    "black>=24.1.1",
    "isort>=5.13.2",
]

[project.scripts]
pbpluginary = "pedalboard_pluginary.__main__:main"

[tool.setuptools]
packages = ["pedalboard_pluginary"]
package-dir = {"" = "src"}

[tool.pytest.ini_options]
addopts = "--cov=pedalboard_pluginary --cov-report=term-missing"
testpaths = ["tests"]

[tool.flake8]
max_line_length = 88
extend_ignore = "E203,W503"
exclude = [
    ".tox",
    "build",
    "dist",
    ".eggs",
    "docs/conf.py",
]

[tool.mypy]
python_version = "3.9"
mypy_path = "src"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["fire", "benedict"]
ignore_missing_imports = true

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3
</file>

<file path="src/pedalboard_pluginary/scanners/vst3_scanner.py">
"""
Handles scanning of VST3 plugins.
"""

import logging
import os
import platform
from pathlib import Path
from typing import Dict, List, Optional

import pedalboard

from ..async_scanner import AsyncScannerMixin
from ..base_scanner import BaseScanner
from ..constants import PLUGIN_TYPE_VST3, VST3_EXTENSION, PLUGIN_LOAD_TIMEOUT
from ..exceptions import PluginLoadError, PluginScanError
from ..models import PluginInfo, PluginParameter
from ..timeout import sync_timeout, TimeoutError
from ..utils import from_pb_param

logger = logging.getLogger(__name__)


class VST3Scanner(BaseScanner, AsyncScannerMixin):
    """Scanner for VST3 plugins."""
    
    @property
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles."""
        return PLUGIN_TYPE_VST3
    
    @property
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        return [VST3_EXTENSION]
    
    def _get_default_vst3_folders(self) -> List[Path]:
        """Get standard VST3 plugin folders for the current OS."""
        os_name = platform.system()
        folders: List[Path] = []
        
        if os_name == "Windows":
            program_files = os.getenv("ProgramFiles", "C:\\Program Files")
            program_files_x86 = os.getenv("ProgramFiles(x86)", "C:\\Program Files (x86)")
            folders = [
                Path(program_files) / "Common Files" / "VST3",
                Path(program_files_x86) / "Common Files" / "VST3",
            ]
        elif os_name == "Darwin":  # macOS
            folders = [
                Path("~/Library/Audio/Plug-Ins/VST3").expanduser(),
                Path("/Library/Audio/Plug-Ins/VST3"),
            ]
        elif os_name == "Linux":
            folders = [
                Path("~/.vst3").expanduser(),
                Path("/usr/lib/vst3"),
                Path("/usr/local/lib/vst3"),
            ]
        
        return [f for f in folders if f.exists()]
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all VST3 plugin files in standard and custom folders.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to VST3 plugin files found.
        """
        if paths:
            # Filter specific paths to only VST3 files
            vst3_paths = [p for p in paths if p.suffix in self.supported_extensions]
            return self._filter_plugin_paths(vst3_paths)
        
        # Search default VST3 folders
        search_folders = self._get_default_vst3_folders()
        
        if not search_folders:
            logger.warning("No VST3 folders to search.")
            return []
        
        logger.info(f"Searching for VST3 plugins in: {search_folders}")
        
        # Find all VST3 files
        discovered_plugins = set()
        for folder in search_folders:
            try:
                for vst3_file in folder.glob("*.vst3"):
                    if vst3_file.is_file():
                        discovered_plugins.add(vst3_file.resolve())
            except Exception as e:
                logger.error(f"Error searching folder {folder}: {e}")
        
        # Apply filtering
        plugin_list = sorted(list(discovered_plugins))
        filtered_list = self._filter_plugin_paths(plugin_list)
        
        logger.info(f"Found {len(filtered_list)} VST3 plugins after filtering.")
        return filtered_list
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan a VST3 plugin and return its information.
        
        Args:
            path: Path to the VST3 plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        if not self.validate_plugin_path(path):
            logger.warning(f"Invalid plugin path: {path}")
            return None
        
        try:
            # Load the plugin to get its parameters with timeout
            logger.debug(f"Loading VST3 plugin: {path}")
            plugin = sync_timeout(pedalboard.load_plugin, PLUGIN_LOAD_TIMEOUT, str(path))
            
            # Extract parameters
            params: Dict[str, PluginParameter] = {}
            if hasattr(plugin, 'parameters'):
                for param_name, param_value in plugin.parameters.items():
                    # Convert the parameter value to our expected type
                    converted_value = from_pb_param(param_value)
                    params[param_name] = PluginParameter(
                        name=param_name,
                        value=converted_value,
                    )
            
            # Try to get manufacturer info if available
            manufacturer = None
            if hasattr(plugin, 'manufacturer'):
                manufacturer = str(plugin.manufacturer)
            
            # Get the plugin's display name if available
            display_name = path.stem
            if hasattr(plugin, 'name'):
                display_name = str(plugin.name)
            
            plugin_info = PluginInfo(
                id=self._create_plugin_id(path),
                name=display_name,
                path=str(path),
                filename=path.name,
                plugin_type=self.plugin_type,
                parameters=params,
                manufacturer=manufacturer,
            )
            
            logger.info(f"Successfully scanned VST3 plugin: {display_name}")
            return plugin_info
            
        except TimeoutError as e:
            logger.warning(f"VST3 plugin {path} timed out during loading: {e}")
            raise PluginLoadError(
                plugin_path=str(path),
                reason=f"Plugin loading timed out after {e.timeout}s"
            )
        except PluginLoadError:
            # Re-raise our custom exceptions
            raise
        except Exception as e:
            logger.error(f"Error scanning VST3 plugin {path}: {e}")
            raise PluginScanError(
                plugin_path=str(path),
                scanner_type=self.plugin_type,
                reason=str(e)
            )
</file>

<file path="src/pedalboard_pluginary/__main__.py">
#!/usr/bin/env python3
# benedict might also lack stubs.
import json
import logging  # For basicConfig
import sys  # For sys.stdout in Display lambda
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import fire
import yaml

# fire library might not have complete type stubs, common to ignore if problematic for mypy.
# Consider adding types-fire if available and it resolves issues.
from benedict import benedict as bdict

from .core import PedalboardPluginary
from .data import (
    PLUGINS_CACHE_FILENAME_BASE,
    get_cache_path,
    load_json_file,
    save_json_file,
)
from .models import PluginInfo
from .types import SerializedPlugin
from .scanner import PedalboardScanner

# Define a more specific type for extra_folders if it's always List[str] after split
ExtraFoldersType = Optional[List[str]]


def setup_logging(verbose_level: int = 0) -> None:
    """Configures basic logging for CLI output."""
    # verbose_level: 0 = WARNING, 1 = INFO, 2 = DEBUG
    log_level = logging.WARNING
    if verbose_level == 1:
        log_level = logging.INFO
    elif verbose_level >= 2:
        log_level = logging.DEBUG

    # Only configure if no handlers are already set (e.g., by tests or other imports)
    # This basicConfig will go to stderr by default for WARNING and above.
    # For INFO, let's direct to stdout for better CLI experience.
    if not logging.getLogger().hasHandlers():
        if log_level <= logging.INFO:
            # For INFO and DEBUG, use a more verbose format and stdout
            logging.basicConfig(
                stream=sys.stdout,
                level=log_level,
                format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            )
        else:
            # For WARNING, ERROR, CRITICAL, use stderr and simpler format
            logging.basicConfig(
                level=log_level, format="%(levelname)s: %(name)s: %(message)s"
            )


def scan_plugins_cli(extra_folders: Optional[str] = None, verbose: int = 0) -> None:
    """Scans all plugins, optionally including extra folders (comma-separated string)."""
    folders_list: List[str] = extra_folders.split(",") if extra_folders else []
    scanner = PedalboardScanner(specific_paths=folders_list)
    scanner.full_scan()  # This updates scanner.plugins
    if scanner.plugins:  # Only save if we found plugins
        cache_file = get_cache_path(PLUGINS_CACHE_FILENAME_BASE)
        save_json_file(scanner.plugins, cache_file)


def update_plugins_cli(extra_folders: Optional[str] = None, verbose: int = 0) -> None:
    """Updates the plugin cache, optionally including extra folders (comma-separated string)."""
    scan_plugins_cli(extra_folders, verbose)


def list_json_cli() -> Dict[str, SerializedPlugin]:
    """Lists all plugins in JSON format."""
    cache_file = get_cache_path(PLUGINS_CACHE_FILENAME_BASE)
    if not cache_file.exists():
        return {}
    data = load_json_file(cache_file)
    return data if isinstance(data, dict) else {}


def list_yaml_cli() -> str:
    """Lists all plugins in YAML format."""
    plugins = list_json_cli()
    return yaml.dump(plugins, sort_keys=False, indent=2)


def main() -> None:
    """Main entry point for the CLI."""
    fire.Fire({
        "scan": scan_plugins_cli,
        "list": list_json_cli,
        "json": list_json_cli,
        "yaml": list_yaml_cli,
        "update": update_plugins_cli,
    })


if __name__ == "__main__":
    main()
</file>

<file path="src/pedalboard_pluginary/data.py">
import json
import os
import platform
import shutil
from importlib import resources
from pathlib import Path
from typing import Any, Dict, List, Set, Union

# JSON-serializable types
JSONValue = Union[str, int, float, bool, None, Dict[str, Any], List[Any]]
JSONDict = Dict[str, JSONValue]

from .utils import ensure_folder

APP_NAME: str = "com.twardoch.pedalboard-pluginary"
PLUGINS_CACHE_FILENAME_BASE: str = "plugins"  # To identify the plugins cache file


def get_cache_path(cache_name: str, extension: str = "json") -> Path:
    """Get the path to a cache file."""
    os_name = platform.system()
    if os_name == "Windows":
        app_data_env = os.getenv("APPDATA")
        if app_data_env is None:
            app_data_dir = (
                Path(os.path.expanduser("~")) / "AppData" / "Roaming" / APP_NAME
            )
        else:
            app_data_dir = Path(app_data_env) / APP_NAME
    elif os_name == "Darwin":  # macOS
        app_data_dir = Path.home() / "Library" / "Application Support" / APP_NAME
    else:  # Linux and other Unix-like systems
        xdg_cache_home_env = os.getenv("XDG_CACHE_HOME")
        if xdg_cache_home_env:
            app_data_dir = Path(xdg_cache_home_env) / APP_NAME
        else:
            app_data_dir = Path.home() / ".cache" / APP_NAME

    app_data_dir.mkdir(parents=True, exist_ok=True)  # Ensure base app dir exists
    return app_data_dir / f"{cache_name}.{extension}"


def get_sqlite_cache_path(cache_name: str) -> Path:
    """Get the path to a SQLite cache database."""
    return get_cache_path(cache_name, "db")


def load_json_file(file_path: Path) -> Any:
    """Load JSON data from a file."""
    if not file_path.exists():
        return {}

    with open(file_path, "r", encoding="utf-8") as file:
        try:
            raw_data = json.load(file)
        except json.JSONDecodeError:
            return {}  # Return empty dict if JSON is corrupted

    return raw_data


def save_json_file(data: Any, file_path: Path) -> None:
    """Save JSON data to a file."""
    ensure_folder(file_path.parent)
    with open(file_path, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=4)


def load_ignores(ignores_path: Path) -> Set[str]:
    """Load ignores data (list of strings) from the file."""
    content = load_json_file(ignores_path)
    if isinstance(content, list):  # Expects a list of strings
        return set(item for item in content if isinstance(item, str))
    return set()


def save_ignores(ignores: Set[str], ignores_path: Path) -> None:
    """Save ignores data to the file."""
    save_json_file(sorted(list(ignores)), ignores_path)


def copy_default_ignores(destination_path: Path) -> None:
    """Copy the default ignores file to the destination if it does not exist."""
    try:
        import importlib.resources

        default_ignores_src_path = importlib.resources.files(
            "pedalboard_pluginary.resources"
        ).joinpath("default_ignores.json")

        if not destination_path.exists():
            ensure_folder(destination_path.parent)
            with importlib.resources.as_file(
                default_ignores_src_path
            ) as src_file_on_fs:
                if src_file_on_fs.exists():
                    shutil.copy(src_file_on_fs, destination_path)
                else:
                    save_json_file([], destination_path)
    except (ImportError, FileNotFoundError, TypeError) as e:
        print(
            f"Warning: Could not copy default ignores using importlib.resources: {e}. Creating empty ignores file."
        )
        if not destination_path.exists():
            ensure_folder(destination_path.parent)
            save_json_file([], destination_path)
</file>

<file path="README.md">
# Pedalboard Pluginary

[![Codecov](https://codecov.io/gh/twardoch/pedalboard-pluginary/branch/main/graph/badge.svg?token=YOUR_CODECOV_TOKEN_HERE)](https://codecov.io/gh/twardoch/pedalboard-pluginary)
<!-- Replace YOUR_CODECOV_TOKEN_HERE with the actual token from Codecov if needed, or remove the token part if your repo is public and Codecov supports tokenless uploads for it.
The URL should also be verified once the repo is active on Codecov. -->

_Pedalboard Pluginary_ is an independent Python-based package and command-line tool that scans and lists VST-3 plugins on macOS and Windows, and Audio Unit (AU) plugins on macOS. It’s intended as a companion for the _[Pedalboard](https://github.com/spotify/pedalboard)_ Python library by Spotify, but it’s not affiliated with _Pedalboard_ or Spotify.

## Features

With _Pedalboard Pluginary_, you can scan and list VST-3 and AU audio plugins installed on your machine, including their default parameters. 

- It automatically scans and catalogs VST-3 and AU plugins installed on your system.
- Provides a command-line interface (CLI) for quick access to your plugin library.
- Saves the plugin information in a JSON file. This file has the information about the plugin parameters and their default values. 
- Works on Windows and macOS (Windows is currently untested).
- It bundles an `ignores.json` file, which “blacklists” some plugins that are known to cause issues with Pedalboard. It will not scan these, and will not include them in the cache. If you find that some plugins are not working with Pedalboard, you can add them to your `ignores.json` file. See “Contributing” section below.

## Future plans

I plan to extend the package with another functionality, “jobs”, which will allow to load a stack of plugins with their parameter values from a dictionary or JSON file, and run them in a batch using Pedalboard. 

## Installation

To install _Pedalboard Pluginary_, run:

```bash
python3 -m pip install --upgrade pedalboard-pluginary
```

For the current development version:

```bash
python3 -m pip install --upgrade git+https://github.com/twardoch/pedalboard-pluginary
```

## Command-line usage

After installation, you can use `pbpluginary` from the command line.

### Commands:

- `pbpluginary list` displays the plugin information stored in the cache, as a JSON. If no cache exists, it will scan your system and create the cache.
- `pbpluginary scan` scans all available plugins, and caches the information. Run this if you’ve installed or upgraded some VST-3 or AU plugins.

## Python usage

You can use _Pedalboard Pluginary_ as a library in your Python scripts. Here's a quick example:

```python
from pedalboard_pluginary import PedalboardPluginary

pluginary = PedalboardPluginary()
print(pluginary.list_plugins())
```

This snippet will list all plugins that have been scanned and cached, as a JSON.

## Changes

- **v1.1.0**: Added `update` CLI command which only scans plugins that aren’t cached yet. Not perfect. Added `json` and `yaml` CLI commands. Additional refactorings. 
- **v1.0.0**: Initial release with basic scanning and listing of both VST-3 and AU plugins, and command-line interface for easy interaction.

## License

- **Pedalboard Pluginary** is written by Adam Twardoch, with assistance from GPT-4.
- Copyright (c) 2023 Adam Twardoch.
- Licensed under the [Apache-2.0 license](https://raw.githubusercontent.com/twardoch/pedalboard-pluginary/main/LICENSE.txt).
- _Pedalboard Pluginary_ is not affiliated with [Pedalboard](https://github.com/spotify/pedalboard) or Spotify.

## Contributing

- If you encounter any issues or have suggestions, feel free to open an [issue](https://github.com/twardoch/pedalboard-pluginary/issues) on GitHub. 
- If you find that some plugins are not working with Pedalboard, open an issue that lists the key, which is the plugin type and the base filename, like `"aufx/CoreAudio"` or `"vst3/RX 10 Connect"`. You can also modify the [`default_ignores.json`](https://raw.githubusercontent.com/twardoch/pedalboard-pluginary/main/src/pedalboard_pluginary/resources/default_ignores.json) file, and submit a pull request.
- If you want to contribute code, please open a pull request.
</file>

<file path="src/pedalboard_pluginary/scanners/au_scanner.py">
"""
Handles scanning of Audio Unit (AU) plugins on macOS.
"""

import logging
import platform
import subprocess
from pathlib import Path
from typing import Dict, List, Optional

import pedalboard

from ..async_scanner import AsyncScannerMixin
from ..base_scanner import BaseScanner
from ..constants import AU_EXTENSION, PLATFORM_MACOS, PLUGIN_TYPE_AU, PLUGIN_LOAD_TIMEOUT
from ..exceptions import PlatformError, PluginLoadError, PluginScanError
from ..models import PluginInfo, PluginParameter
from ..timeout import sync_timeout, TimeoutError
from ..utils import from_pb_param

logger = logging.getLogger(__name__)


class AUScanner(BaseScanner, AsyncScannerMixin):
    """Scanner for Audio Unit plugins."""
    
    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
    ):
        """Initialize the AU scanner with optional ignore paths and specific paths."""
        super().__init__(ignore_paths, specific_paths)
        self._is_macos = platform.system() == PLATFORM_MACOS
        if not self._is_macos:
            logger.info("AU scanning is only available on macOS.")
    
    @property
    def plugin_type(self) -> str:
        """Return the plugin type this scanner handles."""
        return PLUGIN_TYPE_AU
    
    @property
    def supported_extensions(self) -> List[str]:
        """Return list of file extensions this scanner supports."""
        return [AU_EXTENSION]
    
    def _get_au_plugin_locations(self) -> List[Path]:
        """Get standard AU plugin locations on macOS."""
        return [
            Path("/Library/Audio/Plug-Ins/Components"),
            Path("~/Library/Audio/Plug-Ins/Components").expanduser(),
            Path("/System/Library/Components"),
        ]
    
    def _list_aufx_plugins_raw(self) -> List[str]:
        """List all Audio Unit effects plugins using auval."""
        if not self._is_macos:
            return []
        
        try:
            result = subprocess.run(
                ["auval", "-a"], 
                capture_output=True, 
                text=True, 
                check=True,
                timeout=30
            )
            return result.stdout.splitlines()
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.warning(f"Failed to run auval command: {e}")
            return []
    
    def _parse_aufx_path_from_auval(self, plugin_str: str) -> Optional[Path]:
        """Parse the AU plugin path from auval output."""
        parts = plugin_str.strip().split()
        if len(parts) >= 3 and parts[0] == "aufx":
            bundle_id = parts[2]
            
            # Search in standard locations
            for location in self._get_au_plugin_locations():
                if location.exists():
                    # First try exact match
                    component_path = location / f"{bundle_id}.component"
                    if component_path.exists():
                        return component_path
                    
                    # Then search for partial match
                    for component in location.glob("*.component"):
                        if bundle_id in str(component):
                            return component
        
        return None
    
    def find_plugin_files(self, paths: Optional[List[Path]] = None) -> List[Path]:
        """Find all AU plugin files.
        
        Args:
            paths: Optional list of specific paths to check.
            
        Returns:
            List of paths to AU plugin files found.
        """
        if not self._is_macos:
            return []
        
        if paths:
            # Filter specific paths to only AU component files
            au_paths = [p for p in paths if p.suffix in self.supported_extensions]
            return self._filter_plugin_paths(au_paths)
        
        # Use auval to discover plugins
        discovered_plugins = []
        auval_output = self._list_aufx_plugins_raw()
        
        for line in auval_output:
            if line.strip().startswith("aufx"):
                plugin_path = self._parse_aufx_path_from_auval(line)
                if plugin_path:
                    discovered_plugins.append(plugin_path)
        
        # Also scan standard directories for any missed plugins
        for location in self._get_au_plugin_locations():
            if location.exists():
                try:
                    for component in location.glob("*.component"):
                        if component not in discovered_plugins:
                            discovered_plugins.append(component)
                except Exception as e:
                    logger.error(f"Error scanning directory {location}: {e}")
        
        # Apply filtering
        filtered_list = self._filter_plugin_paths(discovered_plugins)
        logger.info(f"Found {len(filtered_list)} AU plugins after filtering.")
        return filtered_list
    
    def scan_plugin(self, path: Path) -> Optional[PluginInfo]:
        """Scan an AU plugin and return its information.
        
        Args:
            path: Path to the AU plugin file.
            
        Returns:
            PluginInfo object if successful, None if scanning failed.
        """
        if not self.validate_plugin_path(path):
            logger.warning(f"Invalid plugin path: {path}")
            return None
        
        try:
            # Try to load the plugin using pedalboard with timeout
            logger.debug(f"Loading AU plugin: {path}")
            plugin = sync_timeout(pedalboard.load_plugin, PLUGIN_LOAD_TIMEOUT, str(path))
            
            # Extract parameters
            params: Dict[str, PluginParameter] = {}
            if hasattr(plugin, 'parameters'):
                for param_name, param_value in plugin.parameters.items():
                    # Convert the parameter value to our expected type
                    converted_value = from_pb_param(param_value)
                    params[param_name] = PluginParameter(
                        name=param_name,
                        value=converted_value,
                    )
            
            # Try to get manufacturer info
            manufacturer = None
            if hasattr(plugin, 'manufacturer'):
                manufacturer = str(plugin.manufacturer)
            
            # Get the plugin's display name
            display_name = path.stem
            if hasattr(plugin, 'name'):
                display_name = str(plugin.name)
            
            plugin_info = PluginInfo(
                id=self._create_plugin_id(path),
                name=display_name,
                path=str(path),
                filename=path.name,
                plugin_type=self.plugin_type,
                parameters=params,
                manufacturer=manufacturer,
            )
            
            logger.info(f"Successfully scanned AU plugin: {display_name}")
            return plugin_info
            
        except TimeoutError as e:
            logger.warning(f"AU plugin {path} timed out during loading: {e}")
            # Fall back to basic info extraction from auval
            return self._scan_with_auval(path)
        except PluginLoadError:
            # Re-raise our custom exceptions
            raise
        except Exception as e:
            logger.error(f"Failed to scan AU plugin {path} with pedalboard: {e}")
            # Fall back to basic info extraction from auval
            return self._scan_with_auval(path)
    
    def _scan_with_auval(self, path: Path) -> Optional[PluginInfo]:
        """Scan plugin using auval as fallback method."""
        try:
            result = subprocess.run(
                ["auval", "-v", str(path)],
                capture_output=True,
                text=True,
                timeout=10,
            )
            
            # Parse basic info from auval output
            name = path.stem
            manufacturer = None
            
            for line in result.stdout.splitlines():
                if "NAME:" in line:
                    name = line.split("NAME:", 1)[1].strip()
                elif "MANUFACTURER:" in line:
                    manufacturer = line.split("MANUFACTURER:", 1)[1].strip()
            
            if name:
                plugin_info = PluginInfo(
                    id=self._create_plugin_id(path),
                    name=name,
                    path=str(path),
                    filename=path.name,
                    plugin_type=self.plugin_type,
                    manufacturer=manufacturer,
                    parameters={},  # No parameters from auval
                )
                logger.info(f"Scanned AU plugin with auval: {name}")
                return plugin_info
                
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.error(f"Failed to scan {path} with auval: {e}")
        
        return None
</file>

<file path="TODO.md">
# Pedalboard Pluginary - Implementation Roadmap

## Phase 1: Async Performance Revolution (Week 1-2)

### Async Scanner Architecture
- [x] Create AsyncScannerMixin with concurrent plugin loading
- [x] Implement AsyncVST3Scanner and AsyncAUScanner classes
- [x] Add configurable concurrency limits in constants
- [x] Update PedalboardScanner to support async mode
- [ ] Benchmark performance improvements (target: 5-10x speedup)

### SQLite Cache Backend
- [ ] Create cache package with SQLiteCacheBackend
- [ ] Implement indexed search with full-text capabilities
- [ ] Add lazy loading for large datasets
- [ ] Create migration from JSON to SQLite
- [ ] Add cache statistics and management commands

### Smart Cache Invalidation
- [ ] Create ChangeDetector for file-based change detection
- [ ] Implement incremental scan functionality
- [ ] Add modification time tracking
- [ ] Create smart update logic for changed/added/removed plugins

## Phase 2: Modern CLI Revolution (Week 3)

### Migrate to Click + Rich
- [ ] Replace Fire with Click for command structure
- [ ] Add Rich for tables, progress bars, and formatting
- [ ] Implement comprehensive help system
- [ ] Add plugin search and filtering commands
- [ ] Create cache management subcommands

### Advanced Search and Filtering
- [ ] Create PluginSearchEngine with multiple filter options
- [ ] Implement fuzzy search using similarity matching
- [ ] Add suggestion system for similar plugins
- [ ] Create parameter-based filtering
- [ ] Add sorting and pagination support

## Phase 3: Production Hardening (Week 4)

### Comprehensive Testing Strategy
- [ ] Create comprehensive integration test suite
- [ ] Add performance benchmarks and regression tests
- [ ] Test error scenarios and edge cases
- [ ] Add cross-platform compatibility tests
- [ ] Implement continuous integration with multiple Python versions

### Error Recovery and Resilience
- [ ] Create ResilienceManager for error recovery
- [ ] Implement plugin blacklisting for problematic plugins
- [ ] Add safe cache operation wrappers
- [ ] Create cache repair and validation functionality
- [ ] Add health monitoring and status reporting

## Phase 4: Advanced Features (Week 5-6)

### Configuration Management System
- [ ] Create Pydantic-based settings with environment variable support
- [ ] Add configuration file support (.env, config files)
- [ ] Implement configuration validation and defaults
- [ ] Create CLI commands for configuration management

### Plugin Categorization System
- [ ] Create PluginCategory enum and categorization rules
- [ ] Implement intelligent categorization based on names and parameters
- [ ] Add category-based filtering and search
- [ ] Create category statistics and reporting

### Export and Integration Features
- [ ] Implement CSV export functionality
- [ ] Add JSON export with comprehensive metadata
- [ ] Create plugin preset system
- [ ] Add bulk import/export capabilities
- [ ] Create DAW integration helpers

## Quality Gates

### Phase 1 Completion Criteria
- [ ] Async scanning 5x faster than sync
- [ ] SQLite cache working for 1000+ plugins
- [ ] Memory usage < 50MB baseline
- [ ] Zero mypy errors maintained

### Phase 2 Completion Criteria
- [ ] New CLI fully replaces Fire-based interface
- [ ] Rich output formatting working
- [ ] All commands have comprehensive help
- [ ] Search and filtering functional

### Phase 3 Completion Criteria
- [ ] >90% test coverage achieved
- [ ] CI passing on all platforms (Windows, macOS, Linux)
- [ ] Performance benchmarks in place
- [ ] Error recovery working reliably

### Phase 4 Completion Criteria
- [ ] Plugin categorization working accurately
- [ ] Export formats functional
- [ ] Search capabilities fully implemented
- [ ] Configuration system operational

## Dependencies to Add

### Core Dependencies
- [ ] click>=8.0.0 (CLI framework)
- [ ] rich>=13.0.0 (output formatting)
- [ ] pydantic>=2.0.0 (configuration management)

### Development Dependencies
- [ ] pytest-benchmark (performance testing)
- [ ] pytest-asyncio (async testing)
- [ ] psutil (memory testing)

## Success Metrics

### Performance Targets
- [ ] 10-20 plugins/second scan speed (vs 1-2 current)
- [ ] O(log n) search performance (vs O(n) current)
- [ ] Constant memory usage with lazy loading

### Reliability Targets
- [ ] Graceful error handling and recovery
- [ ] 99%+ plugin compatibility
- [ ] Zero data loss on crashes

### Usability Targets
- [ ] Intuitive CLI with comprehensive help
- [ ] Rich formatting and progress reporting
- [ ] Fast search and filtering capabilities

## Current Status

### ✅ Completed (Phase 0)
- Type-safe architecture with comprehensive TypedDict and protocols
- Modular scanner design with extensible BaseScanner
- Robust error handling with custom exception hierarchy
- Unified serialization layer with validation
- Zero mypy errors in strict mode
- Timeout protection for plugin loading

### 🚧 In Progress
- Planning and design for async implementation
- Architecture review for performance improvements

### 📋 Next Priority
1. Implement async scanner architecture (highest impact)
2. Create SQLite cache backend (scalability)
3. Migrate to Click CLI framework (user experience)
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added (December 2024)
- Created PLAN.md for implementation roadmap
- Created TODO.md for task tracking
- Created CHANGELOG.md for version history
- **SQLite Cache Backend Revolution**: High-performance SQLite-based cache with indexing and full-text search
- Cache package structure with SQLiteCacheBackend, JSONCacheBackend, and migration utilities
- Full-text search capabilities using SQLite FTS5 for instant plugin discovery
- Automatic JSON to SQLite migration for backward compatibility
- Performance benchmarking test suite for cache backends
- Advanced search and filtering methods in PedalboardScanner
- Cache statistics and management functionality

### Changed
- Refactored scanner architecture to use modular scanner classes
- Improved type annotations throughout the codebase
- **Cache Architecture Modernization**: PedalboardScanner now uses pluggable cache backends
- Updated data.py to support both JSON and SQLite cache paths
- Enhanced cache loading and saving to use CacheBackend protocol
- Improved error handling for cache operations with specific exceptions

### Fixed
- Fixed duplicate imports in scanner.py
- Fixed duplicate full_scan method definitions
- Fixed missing attributes in PedalboardScanner class (ignores, ignores_path)
- Fixed incorrect method calls to scanner instances
- Fixed parameter order in save_json_file calls
- Fixed VST3Scanner inheritance issue (removed BaseScanner dependency)
- Fixed missing scan_plugin method implementations in scanner classes
- Implemented proper plugin parameter extraction using pedalboard API
- Added progress bars using tqdm for plugin scanning
- Enhanced AU scanner with fallback to auval for metadata extraction
- Improved VST3 scanner with manufacturer and display name extraction

### Removed
- Removed obsolete scan_aufx_plugins and scan_vst3_plugins methods
- Removed redundant BaseScanner class definition in scanner.py
- Removed unnecessary type aliases in scanner modules

### Enhanced
- Rewrote VST3Scanner to properly load plugins and extract parameters
- Rewrote AUScanner to properly load plugins with fallback to auval
- Added proper plugin metadata extraction (manufacturer, display name)
- Improved plugin path discovery for both VST3 and AU formats
- Created scanner abstraction layer with BaseScanner class
- Added Protocol definitions for scanner interfaces
- Implemented type safety improvements with types.py
- Refactored scanners to use common base class functionality
- Created unified serialization layer with PluginSerializer
- Added cache versioning and metadata support
- Improved type safety with TypedDict definitions
- Centralized all JSON operations in serialization module
- Created custom exception hierarchy for better error handling
- Added constants module for configuration values
- Implemented progress reporting abstraction with multiple backends
- Added retry decorator for transient failures
- Enhanced error handling throughout the codebase
- Added py.typed marker for type checking support
- Fixed most mypy type errors
- Added typing-extensions dependency for Python 3.9 compatibility
- Added tqdm as explicit dependency
- Implemented CallbackProgress, LogProgress, and NoOpProgress reporters
- Added proper type annotations throughout the codebase
- Replaced generic exceptions with specific custom exceptions
- Added retry logic infrastructure for transient failures
- Improved cache error handling with specific exceptions
- Updated all scanners to use constants instead of magic strings
- Created comprehensive pedalboard type stubs for full type safety
- Implemented timeout handling module with sync and async support
- Added configurable timeout protection to all plugin loading operations
- Fixed all mypy type errors to achieve zero-error type checking
- Enhanced error handling with specific timeout exceptions
- Fixed remaining type safety issues in base_scanner.py and __main__.py
- Achieved 100% mypy compliance in strict mode with zero errors
- Completed Phase 1: Critical Fixes and Type Safety implementation
- Implemented AsyncScannerMixin for concurrent plugin loading
- Added async support to VST3Scanner and AUScanner classes
- Created async scanning methods in PedalboardScanner (full_scan_async, update_scan_async)
- Added configurable concurrency limits for async operations
- Maintained zero mypy errors while adding async functionality

## [1.1.0] - Previous Release

### Added
- Added `update` CLI command which only scans plugins that aren't cached yet
- Added `json` and `yaml` CLI commands

### Changed
- Additional refactorings

## [1.0.0] - Initial Release

### Added
- Initial release with basic scanning and listing of both VST-3 and AU plugins
- Command-line interface for easy interaction
- Support for macOS and Windows (Windows untested)
- Plugin parameter extraction with default values
- JSON cache file for plugin information
- Blacklist functionality for problematic plugins
</file>

<file path="src/pedalboard_pluginary/scanner.py">
import asyncio
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, cast

from .scanners.au_scanner import AUScanner
from .scanners.vst3_scanner import VST3Scanner
from .async_scanner import AsyncScannerMixin

import pedalboard

from .constants import DEFAULT_MAX_CONCURRENT
from .data import (
    copy_default_ignores,
    get_cache_path,
    get_sqlite_cache_path,
    load_ignores,
    save_json_file,
)
from .cache import SQLiteCacheBackend, JSONCacheBackend, migrate_json_to_sqlite
from .exceptions import CacheCorruptedError, PluginScanError
from .models import PluginInfo, PluginParameter
from .progress import TqdmProgress
from .protocols import ProgressReporter
from .serialization import PluginSerializer
from .utils import ensure_folder, from_pb_param

logger: logging.Logger = logging.getLogger(__name__)


class PedalboardScanner:
    """Main scanner class that coordinates scanning of all plugin types."""

    def __init__(
        self,
        ignore_paths: Optional[List[str]] = None,
        specific_paths: Optional[List[str]] = None,
        progress_reporter: Optional[ProgressReporter] = None,
        async_mode: bool = False,
        max_concurrent: int = DEFAULT_MAX_CONCURRENT,
        use_sqlite: bool = True,
    ):
        """Initialize the scanner with optional ignore paths and specific paths.
        
        Args:
            ignore_paths: List of regex patterns for paths to ignore.
            specific_paths: List of specific paths to scan.
            progress_reporter: Optional progress reporter instance.
            async_mode: Whether to use async scanning for better performance.
            max_concurrent: Maximum number of concurrent scans (async mode only).
            use_sqlite: Whether to use SQLite cache backend (default: True).
        """
        self.ignore_paths = ignore_paths or []
        self.specific_paths = specific_paths or []
        self.plugins: Dict[str, PluginInfo] = {}
        self.progress_reporter = progress_reporter or TqdmProgress()
        self.async_mode = async_mode
        self.max_concurrent = max_concurrent
        self.use_sqlite = use_sqlite
        
        # Initialize cache paths
        self.ignores_path = get_cache_path("ignores")
        
        # Initialize cache backend
        if use_sqlite:
            self.sqlite_path = get_sqlite_cache_path("plugins")
            self.json_path = get_cache_path("plugins")  # For migration
            self.cache_backend = SQLiteCacheBackend(self.sqlite_path)
            
            # Migrate from JSON if SQLite doesn't exist but JSON does
            if not self.sqlite_path.exists() and self.json_path.exists():
                try:
                    migrated_count = migrate_json_to_sqlite(self.json_path, self.sqlite_path)
                    logger.info(f"Migrated {migrated_count} plugins from JSON to SQLite cache")
                except Exception as e:
                    logger.warning(f"Migration failed, starting with empty SQLite cache: {e}")
        else:
            self.json_path = get_cache_path("plugins")
            self.cache_backend = JSONCacheBackend(self.json_path)
        
        # Initialize ignores
        copy_default_ignores(self.ignores_path)
        self.ignores = load_ignores(self.ignores_path)
        
        # Initialize scanners
        self.scanners = [
            AUScanner(
                ignore_paths=self.ignore_paths, specific_paths=self.specific_paths
            ),
            VST3Scanner(
                ignore_paths=self.ignore_paths, specific_paths=self.specific_paths
            ),
        ]
        
        # Load existing plugin data if available
        self.load_data()

    def load_data(self) -> None:
        """Load existing plugin data from cache."""
        try:
            self.plugins = self.cache_backend.load()
        except Exception as e:
            logger.warning(f"Cache corrupted, will perform full scan: {e}")
            self.plugins = {}

    def save_data(self) -> None:
        """Save plugin data to cache."""
        self.cache_backend.save(self.plugins)
        
        # Save updated ignores
        save_json_file(list(self.ignores), self.ignores_path)

    def full_scan(self) -> Dict[str, PluginInfo]:
        """Perform a full scan of all plugin types."""
        self.plugins = {}
        total_files = 0
        
        # First, count all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend([(scanner, pf) for pf in plugin_files])
            total_files += len(plugin_files)
        
        # Scan all plugins with progress reporting
        self.progress_reporter.start(total_files, "Scanning plugins")
        
        for scanner, plugin_file in all_plugin_files:
            plugin_key = f"{scanner.__class__.__name__.replace('Scanner', '').lower()}:{plugin_file}"
            
            # Skip ignored plugins
            if plugin_key in self.ignores:
                logger.info(f"Skipping ignored plugin: {plugin_file}")
                self.progress_reporter.update(1, f"Skipped: {plugin_file.name}")
                continue
            
            try:
                plugin_info = scanner.scan_plugin(plugin_file)
                if plugin_info:
                    self.plugins[plugin_info.id] = plugin_info
                    logger.info(f"Scanned plugin: {plugin_file}")
                    self.progress_reporter.update(1, f"Scanned: {plugin_info.name}")
                else:
                    self.progress_reporter.update(1)
            except PluginScanError as e:
                logger.error(f"Failed to scan plugin {plugin_file}: {e}")
                self.ignores.add(plugin_key)
                self.progress_reporter.update(1, f"Failed: {plugin_file.name}")
            except Exception as e:
                logger.error(f"Unexpected error scanning {plugin_file}: {e}")
                self.ignores.add(plugin_key)
                self.progress_reporter.update(1, f"Error: {plugin_file.name}")
        
        self.progress_reporter.finish(f"Scanned {len(self.plugins)} plugins")
        
        # Save the results
        self.save_data()
        return self.plugins

    def update_scan(self) -> Dict[str, PluginInfo]:
        """Update the scan with new plugins while preserving existing data."""
        # Keep track of existing plugins
        existing_plugins = set(self.plugins.keys())
        new_plugins = {}
        
        # Find all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend([(scanner, pf) for pf in plugin_files])
        
        # Only scan plugins that aren't already in the cache
        plugins_to_scan = []
        for scanner, plugin_file in all_plugin_files:
            plugin_type = scanner.__class__.__name__.replace('Scanner', '').lower()
            plugin_key = f"{plugin_type}:{plugin_file}"
            
            if plugin_key not in existing_plugins and plugin_key not in self.ignores:
                plugins_to_scan.append((scanner, plugin_file, plugin_key))
        
        # Scan new plugins with progress reporting
        if plugins_to_scan:
            self.progress_reporter.start(len(plugins_to_scan), "Scanning new plugins")
            
            for scanner, plugin_file, plugin_key in plugins_to_scan:
                try:
                    plugin_info = scanner.scan_plugin(plugin_file)
                    if plugin_info:
                        self.plugins[plugin_info.id] = plugin_info
                        new_plugins[plugin_info.id] = plugin_info
                        logger.info(f"Scanned new plugin: {plugin_file}")
                        self.progress_reporter.update(1, f"Scanned: {plugin_info.name}")
                    else:
                        self.progress_reporter.update(1)
                except PluginScanError as e:
                    logger.error(f"Failed to scan plugin {plugin_file}: {e}")
                    self.ignores.add(plugin_key)
                    self.progress_reporter.update(1, f"Failed: {plugin_file.name}")
                except Exception as e:
                    logger.error(f"Unexpected error scanning {plugin_file}: {e}")
                    self.ignores.add(plugin_key)
                    self.progress_reporter.update(1, f"Error: {plugin_file.name}")
            
            self.progress_reporter.finish(f"Found {len(new_plugins)} new plugins")
            
            # Save updated data
            self.save_data()
        
        return new_plugins

    async def full_scan_async(self) -> Dict[str, PluginInfo]:
        """Perform a full async scan of all plugin types."""
        if not self.async_mode:
            raise ValueError("Async mode not enabled. Set async_mode=True during initialization.")
        
        self.plugins = {}
        
        # Collect all plugin files from all scanners
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend(plugin_files)
        
        # Filter out ignored plugins
        files_to_scan = []
        for plugin_file in all_plugin_files:
            # Find the appropriate scanner for this file
            found_scanner = self._find_scanner_for_file(plugin_file)
            if found_scanner:
                plugin_key = f"{found_scanner.__class__.__name__.replace('Scanner', '').lower()}:{plugin_file}"
                if plugin_key not in self.ignores:
                    files_to_scan.append(plugin_file)
        
        # Scan plugins asynchronously
        if files_to_scan:
            # Use the first scanner that supports async (they all do now)
            async_scanner = cast(AsyncScannerMixin, self.scanners[0])  # VST3Scanner and AUScanner both have AsyncScannerMixin
            
            scanned_plugins = []
            async for plugin in async_scanner.scan_plugins_batch(
                files_to_scan, 
                max_concurrent=self.max_concurrent,
                progress_reporter=self.progress_reporter
            ):
                scanned_plugins.append(plugin)
                self.plugins[plugin.id] = plugin
        
        # Save the results
        self.save_data()
        return self.plugins
    
    async def update_scan_async(self) -> Dict[str, PluginInfo]:
        """Update the scan asynchronously with new plugins while preserving existing data."""
        if not self.async_mode:
            raise ValueError("Async mode not enabled. Set async_mode=True during initialization.")
        
        # Keep track of existing plugins
        existing_plugins = set(self.plugins.keys())
        new_plugins = {}
        
        # Find all plugin files
        all_plugin_files = []
        for scanner in self.scanners:
            plugin_files = scanner.find_plugin_files()
            all_plugin_files.extend(plugin_files)
        
        # Only scan plugins that aren't already in the cache
        files_to_scan = []
        for plugin_file in all_plugin_files:
            found_scanner = self._find_scanner_for_file(plugin_file)
            if found_scanner:
                plugin_type = found_scanner.__class__.__name__.replace('Scanner', '').lower()
                plugin_key = f"{plugin_type}:{plugin_file}"
                
                if plugin_key not in existing_plugins and plugin_key not in self.ignores:
                    files_to_scan.append(plugin_file)
        
        # Scan new plugins asynchronously
        if files_to_scan:
            async_scanner = cast(AsyncScannerMixin, self.scanners[0])
            
            async for plugin in async_scanner.scan_plugins_batch(
                files_to_scan,
                max_concurrent=self.max_concurrent,
                progress_reporter=self.progress_reporter
            ):
                self.plugins[plugin.id] = plugin
                new_plugins[plugin.id] = plugin
            
            # Save updated data
            self.save_data()
        
        return new_plugins
    
    def _find_scanner_for_file(self, plugin_file: Path) -> Optional[Union[AUScanner, VST3Scanner]]:
        """Find the appropriate scanner for a given plugin file."""
        for scanner in self.scanners:
            if scanner.validate_plugin_path(plugin_file):
                return scanner  # type: ignore[return-value]
        return None

    def get_json(self) -> str:
        """Return the plugins data as a JSON string."""
        # Use the serializer to convert plugins to dict format
        plugins_dict = {}
        for key, plugin_info in self.plugins.items():
            plugins_dict[key] = PluginSerializer.plugin_to_dict(plugin_info)
        
        return json.dumps(plugins_dict, indent=2)
    
    def search_plugins(self, query: str, limit: int = 50) -> List[PluginInfo]:
        """Search plugins using full-text search (SQLite only).
        
        Args:
            query: Search query string.
            limit: Maximum number of results to return.
            
        Returns:
            List of matching PluginInfo objects.
        """
        if isinstance(self.cache_backend, SQLiteCacheBackend):
            return self.cache_backend.search(query, limit)
        else:
            # Fallback for JSON backend - simple name/manufacturer filtering
            results = []
            query_lower = query.lower()
            for plugin in self.plugins.values():
                if (query_lower in plugin.name.lower() or 
                    (plugin.manufacturer and query_lower in plugin.manufacturer.lower())):
                    results.append(plugin)
                if len(results) >= limit:
                    break
            return results
    
    def filter_by_type(self, plugin_type: str) -> List[PluginInfo]:
        """Filter plugins by type.
        
        Args:
            plugin_type: Type of plugins to filter (e.g., 'vst3', 'au').
            
        Returns:
            List of matching PluginInfo objects.
        """
        if isinstance(self.cache_backend, SQLiteCacheBackend):
            return self.cache_backend.filter_by_type(plugin_type)
        else:
            # Fallback for JSON backend
            return [plugin for plugin in self.plugins.values() 
                   if plugin.plugin_type.lower() == plugin_type.lower()]
    
    def get_cache_stats(self) -> Dict[str, int]:
        """Get cache statistics.
        
        Returns:
            Dictionary with cache statistics.
        """
        if isinstance(self.cache_backend, SQLiteCacheBackend):
            return self.cache_backend.get_stats()
        else:
            # Basic stats for JSON backend
            stats = {'total_plugins': len(self.plugins)}
            type_counts: Dict[str, int] = {}
            for plugin in self.plugins.values():
                plugin_type = plugin.plugin_type
                type_counts[plugin_type] = type_counts.get(plugin_type, 0) + 1
                
            for plugin_type, count in type_counts.items():
                stats[f"{plugin_type}_plugins"] = count
                
            return stats
</file>

</files>
